\documentclass[12pt,english,prl,superscriptaddress,nobibnotes,nofootinbib]{revtex4-2}
\usepackage[latin9]{inputenc}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{1}
\usepackage{babel}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[pdftex,unicode=true,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},pdfborderstyle={},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfauthor={Jacob A. Barandes},
 pdfsubject={Physics},
 pdfkeywords={physics}}

%\usepackage{amsmath,amsthm,amscd,amssymb}
\usepackage[noBBpl,sc]{mathpazo}
\usepackage[papersize={6.8in, 10.0in}, left=.5in, right=.5in, top=.6in, bottom=.9in]{geometry}
\linespread{1.1}
\sloppy
\raggedbottom
\pagestyle{plain}

% these include amsmath and that can cause trouble in older docs.
\input{/Users/psu/arxiv-psu/helpers/cmrsum}
\input{/Users/psu/arxiv-psu/helpers/fix-underbrace.tex}

\usepackage[small]{titlesec}
\titlelabel{\thetitle.\quad}

\usepackage{microtype}

% hyperref last because otherwise some things go wrong.
\usepackage{hyperref}
\hypersetup{colorlinks=true
,breaklinks=true
,urlcolor=blue
,anchorcolor=blue
,citecolor=blue
,filecolor=blue
,linkcolor=blue
,menucolor=blue
,linktocpage=true}
\hypersetup{
bookmarksopen=true,
bookmarksnumbered=true,
bookmarksopenlevel=10
}

% make sure there is enough TOC for reasonable pdf bookmarks.
\setcounter{tocdepth}{3}

%\usepackage[dotinlabels]{titletoc}
%\titlelabel{{\thetitle}.\quad}
%\input{../helpers/psu-plain-titles.tex}
%\input{../helpers/psu-sc-headers.tex}
\input{../helpers/fix-revtex-12.tex}
%\DeclareSymbolFont{CMlargesymbols}{OMX}{cmex}{m}{n}
%\DeclareMathSymbol{\sum}{\mathop}{CMlargesymbols}{"50}
%\pdfbookmark[1]{Introduction}{Introduction}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%%%%%%%%%%%%%%%%%%%%
% Notes

% Be sure to examine the corresponding local LyX layout (texclass) file, where some additional code is defined.

%%%%%%%%%%%%%%%%%%%%
% Packages

% We include the cancel package, which allows us to put a slash through symbols in mathematical expressions.

% We include the chappg package, which numbers pages by chapter according to [chapter]-[page].
% \usepackage{chappg}
%Unfortunately, this package doesn't handle Parts very well: They get a page number [chapter]-[page] set by the previous chapter. So I add an explicit \pagenumber[#]{bychapter} on the line before each Part, and then a \pagenumber{bychapter} on the line afterward.
%Note also that the chappg package only works for the book document class, and will generate lots of errors for the article document class.

% We include the draftwatermark package, which prints a watermark (text or image) on every page; the precise details are written in LaTeX code before the Title environment.
% \usepackage{draftwatermark}

%%%%%%%%%%%%%%%%%%%%%
% Formatting Improvements

% Corrects the extra spacing around left/right delimiters
\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}

%%%%%%%%%%%%%%%%%%%%
% Page Formatting

% Note that the book document class automatically ensures that new chapters always begin on odd pages, provided that one checks the box next to "Two-sided document" under the "Page Layout" option.

% We reset the footnote counter after every new page.
% \@addtoreset{footnote}{page}

% We use symbols (asterisk, dagger, etc.) for all footnotes instead of labeling them with incremental numbers.
%\renewcommand{\thefootnote}{\fnsymbol{footnote}}

%We change the default background colors for examples and exercises. The \definecolor command has the form \definecolor{colorname}{gray|rgb|cmyk}{...} where ... is a comma-separated list of values between 0 and 1.
%\definecolor{examplebgcolor}{gray}{0.97}
%\definecolor{exercisebgcolor}{gray}{0.97}

%%%%%%%%%%%%%%%%%%%%
% Shortcuts

% We create a shortcut command "\ancillary" that prints a footnote denoting ancillary sections, examples, and exercises, using "\footnotetext" rather than "\footnote" so that we don't actually put a footnote symbol in the main body of the text.
%\newcommand{\ancillary} {
%  \footnotetext{* Marked sections, examples, and exercises lie outside the main development of these notes and can therefore be skipped on a first reading.}
%}

% Define a copyright notice to appear at the footer of desired pages, using \footnotetext rather than \footnote so that no symbol appears
%\newcommand\copyrightnotice{%
%  \footnotetext{\copyright\,Jacob Barandes 2023. All rights reserved. Email: jacob\_barandes@g.harvard.edu}%
%}

%%%%%%%%%%%%%%%%%%%%
% New Operators

% Each DeclareMathOperator{\code}{operator} command assigns a user-defined TeX code "\code" to a new math operator "operator." The asterisk in the command just ensures that subscripts appear underneath rather than to the lower-right of the new operator.

\def\smalloverbrace#1{\mathop{\vbox{\m@th\ialign{##\crcr%
      \noalign{\kern3\p@}%
      \tiny\downbracefill\crcr\noalign{\kern3\p@\nointerlineskip}%
      $\hfil\displaystyle{#1}\hfil$\crcr}}}\limits}

\def\smallunderbrace#1{\mathop{\vtop{\m@th\ialign{##\crcr
   $\hfil\displaystyle{#1}\hfil$\crcr
   \noalign{\kern3\p@\nointerlineskip}%
   \tiny\upbracefill\crcr\noalign{\kern3\p@}}}}\limits}

%%%%%%%%%%%%%%%%%%%%
% New Symbols

% Define my own blackboard font based on the bbold package, so that
% it doesn't overwrite the built-in \mathbb

\DeclareMathAlphabet{\mymathbb}{U}{bbold}{m}{n}

\makeatother

\begin{document}
\title{The Stochastic-Quantum Correspondence}
\author{Jacob A. Barandes}
\email{jacob\_barandes@harvard.edu}

\affiliation{Jefferson Physical Laboratory, Harvard University, Cambridge, MA 02138}
\date{7 Sep 2023 }
\begin{abstract}
This paper introduces an exact correspondence between a general class
of stochastic systems and quantum theory. This correspondence provides
a new framework for using Hilbert-space methods to formulate highly
generic, non-Markovian types of stochastic dynamics, with potential
applications throughout the sciences. This paper also uses the correspondence
in the other direction to reconstruct quantum theory from physical
models that consist of trajectories in configuration spaces undergoing
stochastic dynamics. The correspondence thereby yields a new formulation
of quantum theory, alongside the Hilbert-space, path-integral formulations,
and quasiprobability formulations. In addition, this reconstruction
approach opens up new ways of understanding quantum phenomena like
interference, decoherence, entanglement, noncommutative observables,
and wave-function collapse.
\end{abstract}
\maketitle

\noindent \begin{center}
\global\long\def\quote#1{``#1"}%
\global\long\def\apostrophe{\textrm{'}}%
\global\long\def\slot{\phantom{x}}%
\global\long\def\eval#1{\left.#1\right\vert }%
\global\long\def\keyeq#1{\boxed{#1}}%
\global\long\def\importanteq#1{\boxed{\boxed{#1}}}%
\global\long\def\given{\vert}%
\global\long\def\mapping#1#2#3{#1:#2\to#3}%
\global\long\def\composition{\circ}%
\global\long\def\set#1{\left\{  #1\right\}  }%
\global\long\def\setindexed#1#2{\left\{  #1\right\}  _{#2}}%

\global\long\def\setbuild#1#2{\left\{  \left.\!#1\,\right|\,#2\right\}  }%
\global\long\def\complem{\mathrm{c}}%

\global\long\def\union{\cup}%
\global\long\def\intersection{\cap}%
\global\long\def\cartesianprod{\times}%
\global\long\def\disjointunion{\sqcup}%

\global\long\def\isomorphic{\cong}%

\global\long\def\setsize#1{\left|#1\right|}%
\global\long\def\defeq{\equiv}%
\global\long\def\conj{\ast}%
\global\long\def\overconj#1{\overline{#1}}%
\global\long\def\re{\mathrm{Re\,}}%
\global\long\def\im{\mathrm{Im\,}}%

\global\long\def\transp{\mathrm{T}}%
\global\long\def\tr{\mathrm{tr}}%
\global\long\def\adj{\dagger}%
\global\long\def\diag#1{\mathrm{diag}\left(#1\right)}%
\global\long\def\dotprod{\cdot}%
\global\long\def\crossprod{\times}%
\global\long\def\Probability#1{\mathrm{Prob}\left(#1\right)}%
\global\long\def\Amplitude#1{\mathrm{Amp}\left(#1\right)}%
\global\long\def\cov{\mathrm{cov}}%
\global\long\def\corr{\mathrm{corr}}%

\global\long\def\absval#1{\left\vert #1\right\vert }%
\global\long\def\expectval#1{\left\langle #1\right\rangle }%
\global\long\def\op#1{\hat{#1}}%

\global\long\def\bra#1{\left\langle #1\right|}%
\global\long\def\ket#1{\left|#1\right\rangle }%
\global\long\def\braket#1#2{\left\langle \left.\!#1\right|#2\right\rangle }%

\global\long\def\parens#1{(#1)}%
\global\long\def\bigparens#1{\big(#1\big)}%
\global\long\def\Bigparens#1{\Big(#1\Big)}%
\global\long\def\biggparens#1{\bigg(#1\bigg)}%
\global\long\def\Biggparens#1{\Bigg(#1\Bigg)}%
\global\long\def\bracks#1{[#1]}%
\global\long\def\bigbracks#1{\big[#1\big]}%
\global\long\def\Bigbracks#1{\Big[#1\Big]}%
\global\long\def\biggbracks#1{\bigg[#1\bigg]}%
\global\long\def\Biggbracks#1{\Bigg[#1\Bigg]}%
\global\long\def\curlies#1{\{#1\}}%
\global\long\def\bigcurlies#1{\big\{#1\big\}}%
\global\long\def\Bigcurlies#1{\Big\{#1\Big\}}%
\global\long\def\biggcurlies#1{\bigg\{#1\bigg\}}%
\global\long\def\Biggcurlies#1{\Bigg\{#1\Bigg\}}%
\global\long\def\verts#1{\vert#1\vert}%
\global\long\def\bigverts#1{\big\vert#1\big\vert}%
\global\long\def\Bigverts#1{\Big\vert#1\Big\vert}%
\global\long\def\biggverts#1{\bigg\vert#1\bigg\vert}%
\global\long\def\Biggverts#1{\Bigg\vert#1\Bigg\vert}%
\global\long\def\Verts#1{\Vert#1\Vert}%
\global\long\def\bigVerts#1{\big\Vert#1\big\Vert}%
\global\long\def\BigVerts#1{\Big\Vert#1\Big\Vert}%
\global\long\def\biggVerts#1{\bigg\Vert#1\bigg\Vert}%
\global\long\def\BiggVerts#1{\Bigg\Vert#1\Bigg\Vert}%
\global\long\def\ket#1{\vert#1\rangle}%
\global\long\def\bigket#1{\big\vert#1\big\rangle}%
\global\long\def\Bigket#1{\Big\vert#1\Big\rangle}%
\global\long\def\biggket#1{\bigg\vert#1\bigg\rangle}%
\global\long\def\Biggket#1{\Bigg\vert#1\Bigg\rangle}%
\global\long\def\bra#1{\langle#1\vert}%
\global\long\def\bigbra#1{\big\langle#1\big\vert}%
\global\long\def\Bigbra#1{\Big\langle#1\Big\vert}%
\global\long\def\biggbra#1{\bigg\langle#1\bigg\vert}%
\global\long\def\Biggbra#1{\Bigg\langle#1\Bigg\vert}%
\global\long\def\braket#1#2{\langle#1\vert#2\rangle}%
\global\long\def\bigbraket#1#2{\big\langle#1\big\vert#2\big\rangle}%
\global\long\def\Bigbraket#1#2{\Big\langle#1\Big\vert#2\Big\rangle}%
\global\long\def\biggbraket#1#2{\bigg\langle#1\bigg\vert#2\bigg\rangle}%
\global\long\def\Biggbraket#1#2{\Bigg\langle#1\Bigg\vert#2\Bigg\rangle}%
\global\long\def\angs#1{\langle#1\rangle}%
\global\long\def\bigangs#1{\big\langle#1\big\rangle}%
\global\long\def\Bigangs#1{\Big\langle#1\Big\rangle}%
\global\long\def\biggangs#1{\bigg\langle#1\bigg\rangle}%
\global\long\def\Biggangs#1{\Bigg\langle#1\Bigg\rangle}%

\global\long\def\vec#1{\mathbf{#1}}%
\global\long\def\vecgreek#1{\boldsymbol{#1}}%
\global\long\def\idmatrix{\mymathbb{1}}%
\global\long\def\projector{P}%
\global\long\def\permutationmatrix{\Sigma}%
\global\long\def\densitymatrix{\rho}%
\global\long\def\krausmatrix{K}%
\global\long\def\stochasticmatrix{\Gamma}%
\global\long\def\lindbladmatrix{L}%
\global\long\def\dynop{\Theta}%
\global\long\def\timeevop{U}%
\global\long\def\hadamardprod{\odot}%
\global\long\def\tensorprod{\otimes}%

\global\long\def\inprod#1#2{\left\langle #1,#2\right\rangle }%
\global\long\def\normket#1{\left\Vert #1\right\Vert }%
\global\long\def\hilbspace{\mathcal{H}}%
\global\long\def\samplespace{\Omega}%
\global\long\def\configspace{\mathcal{C}}%
\global\long\def\phasespace{\mathcal{P}}%
\global\long\def\spectrum{\sigma}%
\global\long\def\restrict#1#2{\left.#1\right\vert _{#2}}%
\global\long\def\from{\leftarrow}%
\global\long\def\statemap{\omega}%
\global\long\def\degangle#1{#1^{\circ}}%
\global\long\def\trivialvector{\tilde{v}}%
\global\long\def\eqsbrace#1{\left.#1\qquad\right\}  }%
\par\end{center}

\newpage

\section{Introduction\label{sec:Introduction}}

The theory of stochastic processes describes the phenomenological
behavior of systems with definite configurations that evolve probabilistically
in time. Quantum theory is a comprehensive mathematical apparatus
for making measurement predictions when taking into account the microscopic
constituents of various kinds of physical systems, from subatomic
particles to superconductors. At an empirical level, both theories
involve probabilities, and at the level of formalism, both employ
vectors and matrices.

There have been a number of previous attempts in the research literature
to identify a fundamental relationship connecting stochastic-process
theory and quantum theory~\citep{SudarshanMatthewsRau:1961sdqms,SkorobogatovSvertilov:1998qmcbfaansp,Gillespie:2000nsp,GillespieAlltopMartin:2001mqmpaacsp,Aaronson:2005qcahv,Strocchi:2008aittmsoqm,Frasca:2012qmitsroasp}.
The most well-known of these approaches are due to Bopp~\citep{Bopp:1947qsuk,Bopp:1952efdqbsdk,Bopp:1953sudgdqde},
F{\'e}nyes~\citep{Fenyes:1952ewbuidq}, and Nelson~\citep{Nelson:1966dtobm,Nelson:1985qf}.
Altogether different are stochastic-collapse models~\citep{GhirardiRiminiWeber:1986udmms,BassiGhirardi:2003drm},
in which a quantum system's wave function or density matrix is assumed
to experience stochastic fluctuations through time.

The present paper, which is not continuous with those earlier developments,
introduces an exact correspondence between a highly general class
of stochastic systems and quantum theory. This \emph{stochastic-quantum correspondence}
takes the form of a simple \textquoteleft dictionary\textquoteright{}
expressing any time-dependent stochastic matrix in terms of a suitable
combination of Hilbert-space ingredients.

From a practical standpoint, the stochastic-quantum correspondence
provides a systematic framework for constructing highly generic forms
of stochastic dynamics, much as the Lagrangian or Hamiltonian formulations
of classical mechanics provide systematic frameworks for constructing
deterministic dynamics. Potential applications range from turbulence
to finance, to name just two examples. Importantly, the stochastic-quantum
correspondence does not require assuming that the stochastic dynamics
in question can be modeled as a Markov chain, nor does it require
making a number of other frequently invoked approximations. 

Taking a more foundational perspective, this paper also uses the stochastic-quantum
correspondence to show that physical models based on configuration
spaces combined with stochastic dynamics can replicate all the empirical
predictions of textbook quantum theory\textemdash including interference,
decoherence, entanglement, noncommutative observables, and wave-function
collapse\textemdash without relying on the austere and metaphysically
opaque Dirac-von Neumann axioms~\citep{Dirac:1930pofm,vonNeumann:1932mgdq}.
In this alternative approach, a given system moves stochastically
along a physical trajectory in a classical-looking configuration space,
 and the mathematical objects of the Hilbert-space formulation play
a functional role akin to gauge-dependent variables.

At the very least, this approach therefore yields a new formulation
of quantum theory, one that is based on a picture of stochastic systems
evolving in configuration spaces. This new formulation therefore joins
a list of ways to formulate quantum systems that include the traditional
Hilbert-space formulation~\citep{Dirac:1930pofm,vonNeumann:1932mgdq},
the path-integral formulation~\citep{Dirac:1933tliqm,Feynman:1942tpolaiqm,Feynman:1948statnrqm},
and the quasi-probability formulation~\citep{Wigner:1932otqcfte,Moyal:1949qmaast}.
As noted in \citep{Feynman:1948statnrqm}, ``there is a pleasure
in recognizing old things from a new point of view,'' and ``there
is always the hope that the new point of view will inspire an idea
for the modification of present theories, a modification necessary
to encompass present experiments.''

In addition to establishing these new results, this paper identifies
several forms of gauge invariance that have not been described in
the research literature thus far, analyzes the measurement process
in detail, and describes the implications of the stochastic-quantum
correspondence for dynamical symmetries and for formal enlargements
or dilations of a system's Hilbert space. Taking advantage of having
a concrete model of stochastic physical variables in hand, this paper
also revisits and clarifies a number of important questions related
to the status of nonlocality in quantum theory.

Considering the mathematical simplicity of this stochastic-quantum
correspondence, it is surprising that it has apparently not shown
up in the research literature before. To the author's knowledge, the
only previous example that bears a suggestive resemblance to the approach
taken in this paper, at least at the level of some of its equations,
is the unpublished draft~\citep{Wetterich:2009qrocse}.\footnote{The author thanks Logan McCarty for finding this reference.}
Although that reference argues that some stochastic processes can
be modeled using a formalism similar to that of quantum theory, it
does not establish that the resulting Hilbert-space representation
is fully general. Nor does it attempt to show that the correspondence
is bidirectional, so that quantum systems can be modeled by stochastic
processes in configuration spaces.

Section~\ref{sec:Stochastic-Processes} will start with the definition
of a \emph{generalized stochastic system}, and then introduce the
key distinction between \emph{divisible} and \emph{indivisible}
dynamics. Section~\ref{sec:The-Hilbert-Space-Formulation} will construct
the Hilbert-space representation for a given generalized stochastic
system, formulate the dictionary that will ultimately connect generalized
stochastic systems with quantum systems, and identify an important
new class of gauge transformations that have not yet been described
in the research literature. Section~\ref{sec:The-Stochastic-Quantum-Correspondence}
will establish the stochastic-quantum correspondence, which will involve
reviewing the definition of \emph{unistochastic matrices}, introducing
the concept of a \emph{division event}, and showing how some of the
most characteristic features of any quantum system\textemdash like
interference, entanglement, and decoherence\textemdash can be understood
from the perspective of the underlying generalized stochastic system.
Section~\ref{sec:Measurements} will provide a detailed treatment
of the measurement process, which will entail introducing the notion
of an \emph{emergeable}, and then turn to a larger discussion of
the measurement problem and the uncertainty principle. Section~\ref{sec:Further-Implications}
will describe further implications of the stochastic-quantum correspondence
for quantum theory, focusing on symmetries, Hilbert-space dilations,
and nonlocality. Section~\ref{sec:Discussion-and-Future-Work} will
conclude the paper with a brief discussion, as well as with several
open questions to be addressed in future work.

\section{Stochastic Processes\label{sec:Stochastic-Processes}}

\subsection{Generalized Stochastic Systems\label{subsec:Generalized-Stochastic-Systems}}

The most general kind of stochastic process requires only a sample
space, an initial probability distribution, and a time-dependent random
variable. (For pedagogical treatments of the theory of stochastic
processes, see~\citep{Rosenblatt:1962rp,Parzen:1962sp,Doob:1990sp,Ross:1995sp}.)
Stochastic processes defined in this way lack an ingredient that plays
the role of a \emph{dynamical law}. This paper will be concerned
with a slightly more narrowly defined construction that includes the
notion of a dynamical law, and that can still describe a wide variety
of physical systems.

A \emph{generalized stochastic system} will be defined to consist
of a \emph{configuration space} $\configspace$ together with a dynamical
law in the form of a \emph{stochastic map} $\stochasticmatrix\left(t\right)$
that acts linearly on probability distributions over $\configspace$
at an initial time $0$ to yield corresponding probability distributions
over $\configspace$ at other times $t\ne0$. For the purposes of
this paper, the set of allowed times will be assumed to be isomorphic
to a subset of the real line $\mathbb{R}$ containing $0$, up to
a choice of measurement units.

The formalism for a generalized stochastic system is easiest to express
in the case in which $\configspace$ has a finite number $N$ of configurations
labeled by positive integers $1,\dots,N$: 
\begin{equation}
\configspace\defeq\set{1,\dots,N}.\label{eq:DefConfigSpace}
\end{equation}
 In that case, the system's \emph{standalone probabilities} at the
initial time $0$ can be denoted by 
\begin{equation}
p_{j}\left(0\right)\quad\left[\textrm{for }j=1,\dots,N\right],\label{eq:InitialStandaloneProbabilities}
\end{equation}
 the system's standalone probabilities at $t\ne0$ can be denoted
by 
\begin{equation}
p_{i}\left(t\right)\quad\left[\textrm{for }i=1,\dots,N\right],\label{eq:FinalStandaloneProbabilities}
\end{equation}
 and the stochastic map consists of \emph{conditional probabilities}
\begin{equation}
\stochasticmatrix_{ij}\left(t\right)\defeq p\left(i,t\given j,0\right)\quad\left[\textrm{for }i,j=1,\dots,N\right],\label{eq:DefStochasticMatrixAsConditionals}
\end{equation}
 where $p\left(i,t\given j,0\right)$ denotes the conditional probability
for the system to be in its $i$th configuration at the time $t$,
given that the system is in its $j$th configuration at the initial
time $0$. (Note that no assumption is made here about whether $t>0$
or $t<0$.) Being probabilities, these quantities satisfy the usual
non-negativity and normalization conditions 
\begin{equation}
p_{j}\left(0\right),p_{i}\left(t\right)\geq0,\quad\sum_{j=1}^{N}p_{j}\left(0\right)=\sum_{i=1}^{N}p_{i}\left(t\right)=1,\label{eq:ConditionsProbabilities}
\end{equation}
 and 
\begin{equation}
\stochasticmatrix_{ij}\left(t\right)\geq0,\qquad\sum_{i=1}^{N}\stochasticmatrix_{ij}\left(t\right)=1.\label{eq:StochasticMatrixConditions}
\end{equation}
 Then from \emph{Bayesian marginalization}, one has the \emph{linear}
relationship 
\begin{equation}
p_{i}\left(t\right)=\sum_{j=1}^{N}\stochasticmatrix_{ij}\left(t\right)p_{j}\left(0\right),\label{eq:FinalStandaloneProbabilitiesFromMarginalizationStochasticMatrix}
\end{equation}
 where the initial standalone probabilities $p_{j}\left(0\right)$
are assumed to be arbitrary and can therefore be freely adjusted without
altering the conditional probabilities $\stochasticmatrix_{ij}\left(t\right)$.

Letting $p\left(0\right)$ denote an $N\times1$ \emph{probability vector}
whose entries are given by the standalone probabilities $p_{j}\left(0\right)$,
letting $p\left(t\right)$ denote the analogous $N\times1$ probability
vector with entries given by $p_{i}\left(t\right)$, and letting $\stochasticmatrix\left(t\right)$
denote the $N\times N$ time-dependent \emph{transition matrix} consisting
of the conditional probabilities $\stochasticmatrix_{ij}\left(t\right)$,
one can naturally recast the linear Bayesian marginalization relationship
\eqref{eq:FinalStandaloneProbabilitiesFromMarginalizationStochasticMatrix}
in matrix form as 
\begin{equation}
p\left(t\right)=\stochasticmatrix\left(t\right)p\left(0\right).\label{eq:MatrixFormStochasticMap}
\end{equation}
 The conditions \eqref{eq:StochasticMatrixConditions} on the transition
matrix $\stochasticmatrix\left(t\right)$ identify it, mathematically
speaking, as a \emph{(column) stochastic matrix}. On physical grounds,
$\stochasticmatrix\left(t\right)$ will be assumed to satisfy the
continuity condition that in the limit $t\to0$, it approaches its
initial value $\stochasticmatrix\left(0\right)$, which will be taken
to be the $N\times N$ identity matrix $\idmatrix$: 
\begin{equation}
\lim_{t\to0}\stochasticmatrix\left(t\right)=\stochasticmatrix\left(0\right)\defeq\idmatrix\defeq\diag{1,\dots,1}.\label{eq:ContinuityConditionStochasticMatrixAtTimeZero}
\end{equation}

Next, consider a \emph{random variable} $A\left(t\right)$ with (not
necessarily distinct) real-valued \emph{magnitudes} $a_{1}\left(t\right),\dots,a_{N}\left(t\right)$
determined by the system's configuration $i=1,\dots,N$, and possibly
also depending explicitly on the time $t$. Then the \emph{expectation value}
$\expectval{A\left(t\right)}$ is defined as the statistical average
or mean of the magnitudes of $A\left(t\right)$ over the system's
standalone probability distribution at $t$: 
\begin{equation}
\expectval{A\left(t\right)}\defeq\sum_{i=1}^{N}a_{i}\left(t\right)p_{i}\left(t\right)=\sum_{i=1}^{N}\sum_{j=1}^{N}a_{i}\left(t\right)\stochasticmatrix_{ij}\left(t\right)p_{j}\left(0\right).\label{eq:DefRandomVariableExpectationValue}
\end{equation}
 One can go on to define the standard deviation and various statistical
moments of $A\left(t\right)$ by appropriate generalizations of this
basic definition.

All these formulas can be extended to systems with continuous configuration
spaces. For a system with a continuous configuration space $\configspace$,
one uses standalone probability densities $p\left(y,0\right)$ at
the initial time $0$ and $p\left(x,t\right)$ at $t\ne0$, where
$x$ and $y$ each symbolically denotes a set of real-valued coordinates.
The Bayesian marginalization relationship \eqref{eq:FinalStandaloneProbabilitiesFromMarginalizationStochasticMatrix}
then becomes 
\begin{equation}
p\left(x,t\right)=\int_{\configspace}d\mu\left(y\right)\,\stochasticmatrix\left(x,y,t\right)p\left(y,0\right),\label{eq:FinalStandaloneProbabilitiesFromMarginalizationStochasticMatrixContinuousIntegral}
\end{equation}
 where $d\mu\left(y\right)$ is a suitable integral measure over $\configspace$
and where the conditional probability density $\stochasticmatrix\left(x,y,t\right)$
naturally serves as an integral kernel. A random variable $A\left(t\right)$
then has magnitudes $a\left(x,t\right)$ labeled by $x$ and $t$,
and its expectation value \eqref{eq:DefRandomVariableExpectationValue}
becomes 
\begin{equation}
\eqsbrace{\begin{aligned} & \expectval{A\left(t\right)}\defeq\int_{\configspace}d\mu\left(x\right)\,a\left(x,t\right)p\left(x,t\right)\\
 & \quad=\int_{\configspace}d\mu\left(x\right)\int_{\configspace}d\mu\left(y\right)\,a\left(x,t\right)\stochasticmatrix\left(x,y,t\right)p\left(y,0\right).
\end{aligned}
\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace}\label{eq:DefRandomVariableExpectationValueContinuousIntegral}
\end{equation}
For ease of exposition, the discrete case will be assumed going forward.

Equations like \eqref{eq:FinalStandaloneProbabilitiesFromMarginalizationStochasticMatrix},
\eqref{eq:ContinuityConditionStochasticMatrixAtTimeZero}, and \eqref{eq:FinalStandaloneProbabilitiesFromMarginalizationStochasticMatrixContinuousIntegral}
may appear to single out the initial time $0$ as a special time.
Subsection~\ref{subsec:Division-Events-and-the-Markov-Approximation},
however, will show that for systems in sufficiently strong contact
with a repeatedly eavesdropping environment, the initial time $0$
need not actually be a unique time, but will typically be only one
of many times that play a similar role.

\subsection{Traditional Approximations\label{subsec:Traditional-Approximations}}

In textbook treatments of stochastic processes, one often introduces
various approximations or simplifications in defining a system's time-dependent
transition matrix $\stochasticmatrix\left(t\right)$ to make it easier
to construct and describe. A typical such approximation is to assume
that the system is a \emph{discrete-time Markov chain}, meaning that
for some small but finite time scale $\delta t$, one can express
the time-dependent transition matrix $\stochasticmatrix\left(n\,\delta t\right)$
at any integer number $n\geq1$ of steps of duration $\delta t$ as
$n$ powers of a constant stochastic matrix $\stochasticmatrix$:
\begin{equation}
\stochasticmatrix\left(n\,\delta t\right)=\stochasticmatrix^{n}.\label{eq:DefDiscreteMarkovChain}
\end{equation}

Somewhat more generally, a convenient simplification is to assume
that for any two times $t$ and $t^{\prime}$ satisfying $t>t^{\prime}>0$,
one has the composition law 
\begin{equation}
\stochasticmatrix\left(t\right)=\stochasticmatrix\left(t\from t^{\prime}\right)\stochasticmatrix\left(t^{\prime}\right).\label{eq:DivisibilityCondition}
\end{equation}
 This simplification is known as \emph{divisibility}~\citep{MilzModi:2021qspaqnp},
a term that seems to have first appeared in the research literature
in a 2008 paper~\citep{WolfCirac:2008dqc} by Wolf and Cirac in the
context of quantum channels.\footnote{Note that this notion of divisibility is unrelated to the much older
concept of \emph{infinite divisibility}, which refers to a probability
distribution that can be expressed as the probability distribution
of a sum of any integer number of independent and identically distributed
random variables.} Here $\stochasticmatrix\left(t\from t^{\prime}\right)$ is likewise
required to satisfy the mathematical requirements of being a stochastic
matrix, in the sense that its entries are all non-negative and its
columns each sum to $1$, as in \eqref{eq:StochasticMatrixConditions}.

An even more special simplification is to take the transition matrix
$\stochasticmatrix\left(t\right)$ to be a time-dependent \emph{permutation matrix},
meaning a matrix whose columns are a permutation of the columns of
the $N\times N$ identity matrix $\idmatrix$. In that case, $\stochasticmatrix\left(t\right)$
contains only $1$s and $0$s, so it does not contain nontrivial probabilities
at all, and the system transitions deterministically from one configuration
to another in its configuration space $\configspace$. In a suitable
continuum limit $N\to\infty$, the time evolution reduces to smooth,
deterministic dynamics.

Absent these sorts of approximations or simplifications, one is confronted
with the task of constructing an $N\times N$ time-dependent, generically
non-Markovian, \emph{indivisible} transition matrix $\stochasticmatrix\left(t\right)$
for a given configuration space $\configspace$, ideally in a systematic
way. For small configuration spaces, it is easy to devise smoothly
time-dependent, non-Markovian, indivisible examples, like the $2\times2$
transition matrix 
\begin{equation}
\stochasticmatrix\left(t\right)\defeq\begin{pmatrix}e^{-t^{2}/\tau^{2}} & 1-e^{-t^{2}/\tau^{2}}\\
1-e^{-t^{2}/\tau^{2}} & e^{-t^{2}/\tau^{2}}
\end{pmatrix},\label{eq:DefSimple2x2ExponentialStochasticMatrix}
\end{equation}
 where $\tau$ is a constant with units of time, or 
\begin{equation}
\stochasticmatrix\left(t\right)\defeq\begin{pmatrix}\cos^{2}\omega t & \sin^{2}\omega t\\
\sin^{2}\omega t & \cos^{2}\omega t
\end{pmatrix},\label{eq:DefSimple2x2SinusoidalStochasticMatrix}
\end{equation}
 where $\omega$ is a constant with units of inverse-time. However,
it may not seem obvious how to construct smoothly time-dependent transition
matrices $\stochasticmatrix\left(t\right)$ systematically beyond
the $2\times2$ case, especially in the case of large ($N\gg1$) configuration
spaces.

A sufficiently general approach for accomplishing this task could
have numerous practical applications in many scientific and technical
fields. This paper will develop one such approach, and use it to provide
a self-contained theoretical justification for why the Markov and
divisibility approximations work so well in many real-world cases.

\section{The Hilbert-Space Formulation\label{sec:The-Hilbert-Space-Formulation}}

\subsection{The Schur-Hadamard Factorization\label{subsec:The-Schur-Hadamard-Factorization}}

One of the goals of this paper will be to introduce a new and highly
general framework for formulating time-dependent transition matrices
$\stochasticmatrix\left(t\right)$, conceptually akin to the Lagrangian
or Hamiltonian frameworks for formulating deterministic dynamics for
mechanical systems.

The starting place will be to \textquoteleft solve\textquoteright{}
the non-negativity condition $\stochasticmatrix_{ij}\left(t\right)\geq0$
on the individual entries of the transition matrix $\stochasticmatrix\left(t\right)$
by expressing them in the following way: 
\begin{equation}
\stochasticmatrix_{ij}\left(t\right)=\verts{\dynop_{ij}\left(t\right)}^{2}.\label{eq:StochasticMatrixEntryFromAbsValSquare}
\end{equation}
 This equation is not a postulate\textemdash it is a mathematical
identity.

The $N\times N$ matrix $\dynop\left(t\right)$ introduced in \eqref{eq:StochasticMatrixEntryFromAbsValSquare}
is guaranteed to exist, although it is not unique. Its entries $\dynop_{ij}\left(t\right)$
could be taken to be the real square roots of the corresponding quantities
$\stochasticmatrix_{ij}\left(t\right)$, but they could also include
complex numbers, quaternions, or even the elements of a more general
normed algebra (although associativity is a very helpful property
to require). To keep things simple, and with the eventual goal of
reproducing the usual formalism of quantum theory, this paper will
choose $\dynop_{ij}\left(t\right)$ to involve only at most the complex
numbers.

On account of the normalization condition on the transition matrix
$\stochasticmatrix\left(t\right)$ specified in \eqref{eq:StochasticMatrixConditions},
note that the matrix $\dynop\left(t\right)$ must satisfy the summation
condition 
\begin{equation}
\sum_{i=1}^{N}\verts{\dynop_{ij}\left(t\right)}^{2}=1.\label{eq:SumTimeEvOpAbsValSqEq1}
\end{equation}
 For now, no further conditions, such as unitarity, will be imposed
on $\dynop\left(t\right)$, whose significance will soon become more
clear.

There are several helpful ways to re-express the identity \eqref{eq:StochasticMatrixEntryFromAbsValSquare}.
To begin, one can introduce the \emph{Schur-Hadamard product} $\hadamardprod$,
which is defined for arbitrary $N\times N$ matrices $X$ and $Y$
as entry-wise multiplication~\citep{Schur:1911bztdbbmuvv,Halmos:1958fvs,Horn:1990thp}:
\begin{equation}
\left(X\hadamardprod Y\right)_{ij}\defeq X_{ij}Y_{ij}.\label{eq:DefSchurHadamardProduct}
\end{equation}
 One can then regard \eqref{eq:StochasticMatrixEntryFromAbsValSquare}
as expressing the transition matrix $\stochasticmatrix\left(t\right)$
as a \emph{Schur-Hadamard factorization} of the complex-conjugated
matrix $\overconj{\dynop\left(t\right)}$ with $\dynop\left(t\right)$
itself: 
\begin{equation}
\stochasticmatrix\left(t\right)=\overconj{\dynop\left(t\right)}\hadamardprod\dynop\left(t\right).\label{eq:StochasticMatrixSchurHadamardFactorization}
\end{equation}

Schur-Hadamard products are not widely used in linear algebra, in
part because they are basis-dependent. For the purposes of analyzing
a given generalized stochastic system, however, this basis-dependence
is unimportant, because the system's configuration space $\configspace$
naturally singles out a specific basis, to be defined momentarily.

\subsection{The Dictionary\label{subsec:The-Dictionary}}

As an alternative approach that will turn out to have significant
ramifications, one starts by defining an $N$-member collection of
$N\times N$ constant, diagonal projection matrices $\projector_{1},\dots,\projector_{N}$,
which will be called \emph{configuration projectors}. For each $i=1,\dots,N$,
the configuration projector $\projector_{i}$ consists of a single
$1$ in its $i$th row, $i$th column, and $0$s in all its other
entries. That is, $\projector_{i}$ is defined as 
\begin{equation}
\projector_{i}\defeq\mathrm{diag}\parens{0,\dots,0,\underset{\mathclap{\substack{\uparrow\\
i\textrm{th entry}
}
}}{1},0,\dots,0},\label{eq:DefConfigurationProjectors}
\end{equation}
 with individual entries 
\begin{equation}
\projector_{i,jk}=\delta_{ij}\delta_{ik},\label{eq:DefConfigurationProjectorsEntries}
\end{equation}
 where $\delta_{ij}$ is the usual Kronecker delta: 
\begin{equation}
\delta_{ij}\defeq\begin{cases}
1 & \textrm{for }i=j,\\
0 & \textrm{for }i\ne j.
\end{cases}\label{eq:DefKroneckerDelta}
\end{equation}
 It follows immediately that the configuration projectors satisfy
the conditions of \emph{mutual exclusivity}, 
\begin{equation}
\projector_{i}\projector_{j}=\delta_{ij}\projector_{i},\label{eq:ConfigurationProjectorsMutuallyExclusive}
\end{equation}
 and \emph{completeness}, 
\begin{equation}
\sum_{i=1}^{N}\projector_{i}=\idmatrix,\label{eq:ConfigurationProjectorsComplete}
\end{equation}
 where again $\idmatrix$ is the $N\times N$ identity matrix. These
two conditions are the defining features of a \emph{projection-valued measure (PVM)}~\citep{Mackey:1952irolcgi,Mackey:1957qmahs},
so the configuration projectors $\projector_{1},\dots,\projector_{N}$
naturally constitute a PVM.

Letting $\tr\left(\cdots\right)$ denote the usual matrix trace, one
can then recast the mathematical identity \eqref{eq:StochasticMatrixEntryFromAbsValSquare}
relating the entries of $\stochasticmatrix\left(t\right)$ with the
entries of $\dynop\left(t\right)$ as 
\begin{equation}
\keyeq{\stochasticmatrix_{ij}\left(t\right)=\tr\parens{\dynop^{\adj}\left(t\right)\projector_{i}\dynop\left(t\right)\projector_{j}}.}\label{eq:DefDictionary}
\end{equation}
 This equation is a new result. It will turn out to serve as an important
\emph{dictionary} that translates between the formalism of generalized
stochastic processes, as symbolized by $\stochasticmatrix_{ij}\left(t\right)$
on the left-hand side, and an expansive set of mathematical tools
for constructing stochastic dynamics, as embodied by the right-hand
side.\footnote{Similar-looking formulas appear incidentally in equations (3)\textendash (6)
of~\citep{AuffevesGrangier:2017rtqffpra} as an intermediate step
in proving a lemma that the authors use for conceptually different
purposes.}

\subsection{The Hilbert Space\label{subsec:The-Hilbert-Space}}

To understand what these mathematical tools are, the next step will
be to introduce a set of $N\times1$ column vectors $e_{1},\dots,e_{N}$,
where $e_{i}$ has a $1$ in its $i$th component and $0$s in all
its other components: 
\begin{equation}
e_{1}\defeq\left(\begin{smallmatrix}1\\
0\\
\vdots\\
0\\
0
\end{smallmatrix}\right),\quad\dots,\quad e_{N}\defeq\left(\begin{smallmatrix}0\\
0\\
\vdots\\
0\\
1
\end{smallmatrix}\right).\label{eq:DefConfigurationBasis}
\end{equation}
 That is, $e_{i}$ has components 
\begin{equation}
e_{i,j}=\delta_{ij}.\label{eq:DefConfigurationBasisEntries}
\end{equation}
 It follows that the column vectors $e_{1},\dots,e_{N}$ form an orthonormal
basis for the vector space of all $N\times1$ column vectors, and
$e_{1},\dots,e_{N}$ will be called the system's \emph{configuration basis}.
In particular, 
\begin{equation}
e_{i}^{\adj}e_{j}=\delta_{ij},\quad e_{i}e_{i}^{\adj}=\projector_{i},\label{eq:ConfigurationBasisOrthonormalComplete}
\end{equation}
 where $\projector_{i}$ is the $i$th configuration projector, as
defined in \eqref{eq:DefConfigurationProjectors}.

Hence, the right-hand side of the dictionary \eqref{eq:DefDictionary}
is a trace over a \emph{Hilbert space}, meaning a complete inner-product
space over the complex numbers. More explicitly, the dictionary picks
out a Hilbert space $\hilbspace$ that is isomorphic to the vector
space $\mathbb{C}^{N}$ of $N\times1$ column vectors with complex-valued
entries, under the inner product $v^{\adj}w$: 
\begin{equation}
\hilbspace\cong\mathbb{C}^{N}.\label{eq:HilbertSpace}
\end{equation}
 The dictionary therefore provides a Hilbert-space formulation for
constructing highly generic forms of stochastic dynamics.

Substituting the right-hand side of the dictionary \eqref{eq:DefDictionary}
into the linear Bayesian marginalization relationship \eqref{eq:FinalStandaloneProbabilitiesFromMarginalizationStochasticMatrix}
between the system's standalone probabilities $p_{j}\left(0\right)$
at the initial time $0$ and the standalone probabilities $p_{i}\left(t\right)$
at $t\ne0$, one finds that 
\begin{equation}
p_{i}\left(t\right)=\tr\parens{\projector_{i}\densitymatrix\left(t\right)},\label{eq:FinalStandaloneProbabilitiesFromTraceDensityMatrix}
\end{equation}
 where $\densitymatrix\left(t\right)$ is an $N\times N$ time-dependent,
self-adjoint, unit-trace, generically non-diagonal matrix defined
as  
\begin{equation}
\eqsbrace{\begin{aligned}\densitymatrix\left(t\right) & \defeq\dynop\left(t\right)\left[\sum_{j=1}^{N}p_{j}\left(0\right)\projector_{j}\right]\dynop^{\adj}\left(t\right)\\
 & =\dynop\left(t\right)\diag{\dots,p_{j}\left(0\right),\dots}\dynop^{\adj}\left(t\right),\\
 & \qquad\qquad\textrm{with}\quad\densitymatrix^{\adj}\left(t\right)=\densitymatrix\left(t\right)\\
 & \qquad\qquad\textrm{and}\quad\tr\parens{\densitymatrix\left(t\right)}=1.
\end{aligned}
}\label{eq:DefTimeDependentDensityMatrix}
\end{equation}
 Crucially, notice how the linearity of the Bayesian marginalization
relationship \eqref{eq:FinalStandaloneProbabilitiesFromMarginalizationStochasticMatrix}
is ultimately responsible for the linearity of the relationship between
the matrix $\densitymatrix\left(t\right)$ and its value $\densitymatrix\left(0\right)$
at the initial time $0$, as expressed in \eqref{eq:DefTimeDependentDensityMatrix}.

Similarly, by substituting the formula \eqref{eq:FinalStandaloneProbabilitiesFromTraceDensityMatrix}
for $p_{i}\left(t\right)$ into the definition \eqref{eq:DefRandomVariableExpectationValue}
of the expectation value of a random variable $A\left(t\right)$,
one obtains 
\begin{equation}
\expectval{A\left(t\right)}=\tr\parens{A\left(t\right)\densitymatrix\left(t\right)}.\label{eq:ExpectationValueRandomVariableFromTrace}
\end{equation}
 Here $A\left(t\right)$ is now understood to be the $N\times N$
time-dependent, diagonal matrix whose entries are the random variable's
individual magnitudes $a_{1}\left(t\right),\dots,a_{N}\left(t\right)$:
\begin{equation}
A\left(t\right)\defeq\sum_{i=1}^{N}a_{i}\left(t\right)\projector_{i}=\diag{\dots,a_{i}\left(t\right),\dots}.\label{eq:DefDiagonalRandomVariableMatrix}
\end{equation}

In the special case in which the system's standalone probability distribution
at the initial time $0$ is pure, meaning that one of the system's
configurations $j$ is occupied with probability $1$, the system's
probability vector at the initial time $0$ is equal to the $j$th
vector $e_{j}$ in the configuration basis \eqref{eq:DefConfigurationBasisEntries}:
\begin{equation}
p\left(0\right)=e_{j}=\left(\begin{smallmatrix}0\\
\vdots\\
0\\
1\\
0\\
\vdots\\
0
\end{smallmatrix}\right)\negthickspace\negthickspace\negthickspace\negthickspace\leftarrow j\textrm{th entry}\quad\left[\textrm{pure}\right].\label{eq:PureInitialProbabilityVector}
\end{equation}
 One can then define an $N\times1$ column vector 
\begin{equation}
\Psi\left(t\right)\defeq\dynop\left(t\right)e_{j},\label{eq:DefStateVector}
\end{equation}
 which is ultimately just the $j$th column of $\dynop\left(t\right)$.
Due to the summation condition \eqref{eq:SumTimeEvOpAbsValSqEq1}
on $\dynop\left(t\right)$, this column vector $\Psi\left(t\right)$
automatically has unit unit norm, in the sense that 
\begin{equation}
\Psi^{\adj}\left(t\right)\Psi\left(t\right)=1.\label{eq:StateVectorUnitNorm}
\end{equation}
 Moreover, the $i$th component $\Psi_{i}\left(t\right)$ of $\Psi\left(t\right)$
is equal to the specific complex-valued matrix entry $\dynop_{ij}\left(t\right)$:
\begin{equation}
\Psi_{i}\left(t\right)=\dynop_{ij}\left(t\right).\label{eq:WaveFunctionFromTimeEvOpEntry}
\end{equation}
 This component $\Psi_{i}\left(t\right)$ is a purely law-like quantity,
in the sense of being just another name for a part of $\dynop\left(t\right)$,
which is itself just a way of encoding the system's dynamical law,
as embodied by the system's transition matrix $\stochasticmatrix\left(t\right)$.

It follows from a short calculation that when the purity condition
\eqref{eq:PureInitialProbabilityVector} holds at the initial time
$0$, the self-adjoint matrix $\densitymatrix\left(t\right)$ defined
in \eqref{eq:DefTimeDependentDensityMatrix} is rank-one and has factorization
\begin{equation}
\densitymatrix\left(t\right)=\Psi\left(t\right)\Psi^{\adj}\left(t\right)\quad\left[\textrm{pure}\right].\label{eq:RankOneDensityMatrixFactorizedStateVector}
\end{equation}
 The probability formula \eqref{eq:FinalStandaloneProbabilitiesFromTraceDensityMatrix}
then simplifies to 
\begin{equation}
p_{i}\left(t\right)=\verts{\Psi_{i}\left(t\right)}^{2},\label{eq:ConfigurationBornRuleFromStateVector}
\end{equation}
 and the formula \eqref{eq:ExpectationValueRandomVariableFromTrace}
for the expectation value of a random variable $A\left(t\right)$
becomes 
\begin{equation}
\expectval{A\left(t\right)}=\Psi^{\adj}\left(t\right)A\left(t\right)\Psi\left(t\right).\label{eq:ExpectationValueRandomVariableFromStateVector}
\end{equation}

Looking at all these results, one notices a striking resemblance to
mathematical objects and formulas that are familiar from textbook
quantum theory. Specifically, one sees that $\dynop\left(t\right)$
plays the role of a \emph{time-evolution operator}, $\densitymatrix\left(t\right)$
is a \emph{density matrix,} $\Psi\left(t\right)$ is a \emph{state vector}
or \emph{wave function}, and $A\left(t\right)$ represents an \emph{observable}.\footnote{Note that for the purposes of this paper, the terms \textquoteleft operator\textquoteright{}
and \textquoteleft matrix\textquoteright{} will be used interchangeably,
as will the terms \textquoteleft state vector\textquoteright{} and
\textquoteleft wave function.\textquoteright{}} The probability formulas \eqref{eq:FinalStandaloneProbabilitiesFromTraceDensityMatrix}
and \eqref{eq:ConfigurationBornRuleFromStateVector} have the same
form as the \emph{Born rule}, and \eqref{eq:ExpectationValueRandomVariableFromTrace}
and \eqref{eq:ExpectationValueRandomVariableFromStateVector} have
the same form as the standard expressions for quantum expectation
values. (For pedagogical treatments of quantum theory, see~\citep{GriffithsSchroeter:2018iqm,Townsend:2012maqm,Shankar:1994pqm,SakuraiNapolitano:2010mqm,SchumacherWestmoreland:2010qpsi}.)

These formulas are all expressed in what would conventionally be called
the \emph{Schr{\" o}dinger picture}. It will also end up being useful
to introduce the \emph{Heisenberg picture}, which is defined according
to 
\begin{equation}
\eqsbrace{\begin{aligned} & \densitymatrix^{H}\defeq\densitymatrix\left(0\right),\quad\Psi^{H}\defeq\Psi\left(0\right),\\
 & \quad A^{H}\left(t\right)\defeq\dynop^{\adj}\left(t\right)A\left(t\right)\dynop\left(t\right),
\end{aligned}
}\label{eq:DefHeisenbergPicture}
\end{equation}
 where $A^{H}\left(t\right)$ now includes both a possible \emph{explicit}
dependence on time through its magnitudes $a_{i}\left(t\right)$ as
well as an \emph{implicit} dependence on time through the time-evolution
operator $\dynop\left(t\right)$. In the Heisenberg picture, the probability
formula \eqref{eq:FinalStandaloneProbabilitiesFromTraceDensityMatrix}
becomes\footnote{Note that for a generic time-evolution operator $\dynop\left(t\right)$,
the Heisenberg-picture version $\projector_{i}^{H}\left(t\right)\defeq\dynop^{\adj}\left(t\right)\projector_{i}\dynop\left(t\right)$
of a projector $\projector_{i}$ will not likewise be a projector.} 
\begin{equation}
p_{i}\left(t\right)=\tr\parens{\projector^{H}\left(t\right)\densitymatrix^{H}},\label{eq:FinalStandaloneProbabilitiesFromTraceDensityMatrixHeisenbergPicture}
\end{equation}
 and the formula \eqref{eq:ExpectationValueRandomVariableFromTrace}
for expectation values becomes 
\begin{equation}
\expectval{A\left(t\right)}=\tr\parens{A^{H}\left(t\right)\densitymatrix^{H}}.\label{eq:ExpectationValueRandomVariableFromTraceHeisenbergPicture}
\end{equation}

Despite the similarity to expressions found in quantum theory, as
well as the appearance of non-diagonal matrices, it is important to
keep in mind that the system under investigation here is always fundamentally
in a specific configuration $i=1,\dots,N$ in its configuration space
$\configspace$ at any given time, and that the system's dynamics
is completely captured by the transition matrix $\stochasticmatrix\left(t\right)$,
whose entries are conditional probabilities $p\left(i,t\given j,0\right)$,
in accordance with \eqref{eq:DefStochasticMatrixAsConditionals}.
The mathematical objects $\dynop\left(t\right)$, $\densitymatrix\left(t\right)$,
$\Psi\left(t\right)$, and $A\left(t\right)$, despite being extremely
useful, are difficult to assign direct physical meanings, in part
because they are not uniquely defined by $\configspace$ or $\stochasticmatrix\left(t\right)$.

\subsection{Gauge Transformations\label{subsec:Gauge-Transformations}}

To make this non-uniqueness more manifest, it will be helpful to introduce
an analogy with the \emph{Maxwell theory of classical electromagnetism}.
(For pedagogical treatments of classical electromagnetism, see~\citep{Griffiths:2017ie,Zangwill:2012me,Jackson:1998ce}.)

In classical electromagnetism, the electric and magnetic fields are
physically meaningful quantities, but it is often very convenient
to work instead in terms of scalar and vector potentials, which are
not uniquely defined. All choices for the potentials that yield the
same electric and magnetic fields are said to be related by \emph{gauge transformations}.
Any one such choice for the potentials is called a \emph{gauge choice},
and the scalar and vector potentials themselves are called \emph{gauge potentials}
or \emph{gauge variables}.

Making a suitable gauge choice can greatly simplify many calculations,
such as using Lorenz gauge to compute the electric and magnetic fields
for delayed boundary conditions. Ultimately, however, no gauge choice
is fundamentally more physically correct than any other gauge choice,
and all calculations of physical predictions in classical electromagnetism
must conclude with expressions that are written in terms of gauge-invariant
quantities.

To set up the claimed analogy with electromagnetic gauge transformations,
one starts by observing from the basic relationship $\stochasticmatrix_{ij}\left(t\right)=\verts{\dynop_{ij}\left(t\right)}^{2}$
in \eqref{eq:StochasticMatrixEntryFromAbsValSquare} that the Schur-Hadamard
product \eqref{eq:DefSchurHadamardProduct} of the time-evolution
operator $\dynop\left(t\right)$ and a matrix of time-dependent phases
$\exp\parens{i\theta_{ij}\parens t}$ is a transformation of $\dynop\left(t\right)$
with no physical effects, and therefore corresponds to a genuine form
of gauge invariance: 
\begin{equation}
\dynop\left(t\right)\mapsto\dynop\left(t\right)\hadamardprod\begin{pmatrix}e^{i\theta_{11}\left(t\right)} & e^{i\theta_{12}\left(t\right)}\\
e^{i\theta_{21}\left(t\right)} & \ddots\\
 &  & e^{i\theta_{NN}\left(t\right)}
\end{pmatrix}.\label{eq:DefLocalInTimeSchurHadamardGaugeTransformation}
\end{equation}
 This gauge transformation can be written equivalently at the level
of individual matrix entries as 
\begin{equation}
\dynop_{ij}\left(t\right)\mapsto\dynop_{ij}\left(t\right)e^{i\theta_{ij}\left(t\right)}.\label{eq:DefLocalInTimeSchurHadamardGaugeTransformationEntries}
\end{equation}

To the author's knowledge, this kind of gauge invariance, which will
be called a \emph{Schur-Hadamard gauge transformation}, has not yet
been described in the research literature, and is therefore a new
result. It will turn out to play a key role in the analysis of dynamical
symmetries that will be presented in Subsection~\ref{subsec:Symmetries},
and will be extended in an interesting way in the context of Hilbert-space
dilations in Subsection~\ref{subsec:Dilations}.

The Hilbert-space formulation has another form of gauge invariance,
which appears to have first been written down in~\citep{Brown:1999aooiqm}
in the context of transformations of the Schr{\" o}dinger equation between
inertial and non-inertial reference frames. Letting $V\left(t\right)$
be a time-dependent unitary matrix, the following transformation is
also a gauge invariance of the Hilbert-space formulation, leaving
all probabilities $p_{i}\left(t\right)$, expectation values $\expectval{A\left(t\right)}$,
and the transition matrix $\stochasticmatrix\left(t\right)$ as a
whole unchanged:\footnote{One should be mindful of the appearance of the initial time $0$ in
$V^{\adj}\left(0\right)$ in the transformation rule for $\dynop\left(t\right)$.} 
\begin{equation}
\eqsbrace{\begin{aligned}\densitymatrix\left(t\right) & \mapsto\densitymatrix_{V}\left(t\right)\defeq V\left(t\right)\densitymatrix\left(t\right)V^{\adj}\left(t\right),\\
\Psi\left(t\right) & \mapsto\Psi_{V}\left(t\right)\defeq V\left(t\right)\Psi\left(t\right),\\
A\left(t\right) & \mapsto A_{V}\left(t\right)\defeq V\left(t\right)A\left(t\right)V^{\adj}\left(t\right),\\
\dynop\left(t\right) & \mapsto\dynop_{V}\left(t\right)\defeq V\left(t\right)\dynop\left(t\right)V^{\adj}\left(0\right).
\end{aligned}
}\label{eq:DefLocalInTimeUnitaryTransformation}
\end{equation}

If the unitary matrix $V\left(t\right)$ is taken to be time-\emph{independent},
then the gauge transformation \eqref{eq:DefLocalInTimeUnitaryTransformation}
is merely a change of orthonormal basis. However, if $V\left(t\right)$
depends nontrivially on time, and if one regards the system's Hilbert
space at each moment in time as a \emph{fiber} over a one-dimensional
\emph{base manifold} parameterized by the time coordinate $t$, then
$V\left(t\right)$ represents a local-in-time, unitary transformation
of each individual Hilbert-space fiber. In particular, any given time-dependent
state vector $\Psi\left(t\right)$, regarded as a trajectory through
the Hilbert space $\hilbspace$, can be mapped to any other trajectory
by a suitable choice of time-dependent unitary matrix $V\left(t\right)$,
so trajectories in $\hilbspace$ do not describe gauge-invariant facts.

\section{The Stochastic-Quantum Correspondence\label{sec:The-Stochastic-Quantum-Correspondence}}

\subsection{Kraus Decompositions\label{subsec:Kraus-Decompositions}}

In the most general case, a time-evolution operator $\dynop\left(t\right)$
may not satisfy any nontrivial constraints apart from the summation
condition \eqref{eq:SumTimeEvOpAbsValSqEq1}. It will turn out to
be helpful to find alternative ways of representing the $N\times N$
matrix $\dynop\left(t\right)$ in terms of more tightly constrained
mathematical objects.

For $\beta=1,\dots,N$, let $\krausmatrix_{\beta}\left(t\right)$
be the $N\times N$ matrix defined to share its $\beta$th column
with $\dynop\left(t\right)$, but with $0$s in all its other entries:
\begin{equation}
\krausmatrix_{\beta}\left(t\right)\defeq\begin{pmatrix}0 & \cdots & 0 & \dynop_{1\beta}\left(t\right) & 0 & \cdots & 0\\
\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
0 & \cdots & 0 & \dynop_{N\beta}\left(t\right) & 0 & \cdots & 0
\end{pmatrix}\quad\left[\beta=1,\dots,N\right].\label{eq:DefKrausOperator}
\end{equation}
 The entries of $\krausmatrix_{\beta}\left(t\right)$ are given explicitly
by 
\begin{equation}
\krausmatrix_{\beta,ij}\left(t\right)=\delta_{\beta j}\dynop_{ij}\left(t\right).\label{eq:DefKrausOperatorEntries}
\end{equation}
 Then the summation condition \eqref{eq:SumTimeEvOpAbsValSqEq1} on
$\dynop\left(t\right)$ becomes the statement that the matrices $\krausmatrix_{1}\left(t\right),\dots,\krausmatrix_{N}\left(t\right)$
satisfy the \emph{Kraus identity}: 
\begin{equation}
\sum_{\beta=1}^{N}\krausmatrix_{\beta}^{\adj}\left(t\right)\krausmatrix_{\beta}\left(t\right)=\idmatrix.\label{eq:DefKrausIdentity}
\end{equation}
 These matrices are therefore called \emph{Kraus operators}~\citep{Kraus:1971gscqt}.
One can then write the dictionary \eqref{eq:DefDictionary} in an
alternative form called a \emph{Kraus decomposition}:\footnote{Conditional probabilities similar in form to \eqref{eq:DictionaryFromKrausDecomposition}
were studied in~\citep{BarandesKagan:2023qcpanmoqi}.} 
\begin{equation}
\stochasticmatrix_{ij}\left(t\right)=\sum_{\beta=1}^{N}\tr\parens{\krausmatrix_{\beta}^{\adj}\left(t\right)\projector_{i}\krausmatrix_{\beta}\left(t\right)\projector_{j}}.\label{eq:DictionaryFromKrausDecomposition}
\end{equation}

Like all the other mathematical objects in the Hilbert-space formulation,
the Kraus operators $\krausmatrix_{1}\left(t\right),\dots,\krausmatrix_{N}\left(t\right)$
are not unique. Notice also that any number of $N\times N$ matrices
satisfying the Kraus identity \eqref{eq:DefKrausIdentity} are guaranteed
to yield a valid transition matrix $\stochasticmatrix\left(t\right)$
via the Kraus decomposition \eqref{eq:DictionaryFromKrausDecomposition}.
Said in another way, the preceding argument establishes the existence
but not the uniqueness of Kraus operators for any given time-evolution
operator $\dynop\left(t\right)$.

Kraus operators and Kraus decompositions play an important role in
quantum information theory. In particular, they provide (non-unique)
expressions for generalizations of unitary time evolution known as
\emph{quantum channels}, or \emph{completely positive trace-preserving (CPTP) maps}.

\subsection{Unistochastic Matrices\label{subsec:Unistochastic-Matrices}}

In general, Kraus operators need not have the specific form \eqref{eq:DefKrausOperator},
and there need not be $N$ of them. In the most minimal case in which
a system's transition matrix $\stochasticmatrix\left(t\right)$ is
determined by just a single Kraus operator $\krausmatrix_{1}\left(t\right)$,
that Kraus operator will be denoted instead by $\timeevop\left(t\right)$.
In that case, the general Schur-Hadamard factorization \eqref{eq:StochasticMatrixSchurHadamardFactorization}
specializes to 
\begin{equation}
\stochasticmatrix\left(t\right)=\overconj{\timeevop\left(t\right)}\hadamardprod\timeevop\left(t\right).\label{eq:UnistochasticMatrixFromSchurHadamardFactorization}
\end{equation}
 That is, 
\begin{equation}
\stochasticmatrix_{ij}\left(t\right)=\verts{\timeevop_{ij}\left(t\right)}^{2}.\label{eq:UnistochasticMatrixFromAbsValSqTimeEvOp}
\end{equation}
 Equivalently, in dictionary form \eqref{eq:DefDictionary}, one has
\begin{equation}
\stochasticmatrix_{ij}\left(t\right)=\tr\parens{\timeevop^{\adj}\left(t\right)\projector_{i}\timeevop\left(t\right)\projector_{j}}.\label{eq:UnistochasticMatrixFromDictionary}
\end{equation}
 The Kraus identity \eqref{eq:DefKrausIdentity}, meanwhile, reduces
to the statement that $\timeevop\left(t\right)$ is unitary, 
\begin{equation}
\timeevop^{\adj}\left(t\right)=\timeevop^{-1}\left(t\right),\label{eq:DefUnitaryTimeEvOp}
\end{equation}
 and the system's transition matrix $\stochasticmatrix\left(t\right)$
is then said to be a \emph{unistochastic matrix}. That is, a unistochastic
matrix is a square matrix whose individual entries are the modulus-squares
of the corresponding entries of a unitary matrix.

Note that a unitary time-evolution operator $\timeevop\left(t\right)$
will not generically remain unitary under arbitrary Schur-Hadamard
gauge transformations \eqref{eq:DefLocalInTimeSchurHadamardGaugeTransformation}.
Hence, writing a unistochastic transition matrix $\stochasticmatrix\left(t\right)$
in terms of a unitary time-evolution operator $\timeevop\left(t\right)$
corresponds to making a gauge choice\textemdash or, somewhat more
precisely, to partially fixing the gauge freedom \eqref{eq:DefLocalInTimeSchurHadamardGaugeTransformation}.

Unistochastic matrices were first introduced in 1954 by Horn~\citep{Horn:1954dsmatdoarm},
who originally called them \textquoteleft ortho-stochastic\textquoteright{}
matrices. The modern term \textquoteleft unistochastic matrix\textquoteright{}
was introduced by Thompson in 1989~\citep{Thompson:1989uln,NylenTamUhlig:1993oteopsonhasm}.
The term \emph{orthostochastic matrix} now refers to a square matrix
whose entries are the modulus-squares of the corresponding entries
of a \emph{real orthogonal} matrix. 

Every orthostochastic matrix is unistochastic. Importantly, however,
the reverse statement is not generally true, meaning that the complex
numbers generically play a necessary role in formulating a unistochastic
transition matrix $\stochasticmatrix\left(t\right)$ in terms of a
unitary time-evolution operator $\timeevop\left(t\right)$. Even when
the complex numbers are not strictly necessary for writing down a
unitary time-evolution operator $\timeevop\left(t\right)$, such as
if the time-evolution operator can be taken to be real and orthogonal,
it is still very convenient to employ the complex numbers for a given
Hilbert-space representation, so that one can take advantage of the
many highly useful constructs that show up in standard treatments
of quantum theory.

It follows immediately from the dictionary formula \eqref{eq:UnistochasticMatrixFromDictionary}
that every unistochastic transition matrix is \emph{doubly stochastic},
or \emph{bistochastic}, which means that summing over any of its
rows \emph{or} any of its columns always yields $1$: 
\begin{equation}
\sum_{i=1}^{N}\stochasticmatrix_{ij}\left(t\right)=\sum_{j=1}^{N}\stochasticmatrix_{ij}\left(t\right)=1.\label{eq:DoublyStochasticCondition}
\end{equation}


\subsection{Unistochastic Systems\label{subsec:Unistochastic-Systems}}

A generalized stochastic system whose transition matrix $\stochasticmatrix\left(t\right)$
is a unistochastic matrix will be called a \emph{unistochastic system}.

To provide a simple example, note that every permutation matrix is,
in particular, a unitary matrix. Moreover, because the entries of
a permutation matrix are all $1$s and $0$s, they are individually
invariant when one computes their modulus-squares, so every permutation
matrix is \emph{also} a unistochastic matrix. It follows that a discrete,
deterministic system whose dynamics is defined by a permutation matrix
is a special case of a unistochastic system. 

Remarkably, as will be shown in Subsection~\ref{subsec:Dilations}\emph{,
every} stochastic map can be expressed in terms of a unitary time-evolution
operator on a suitably enlarged or \emph{dilated} Hilbert space.
As a consequence, \emph{every} generalized stochastic system can be
regarded as a subsystem of a unistochastic system. This statement
can be formalized as a theorem, called the \emph{stochastic-quantum theorem}~\citep{Barandes:2023tsqt},
and implies that the study of generalized stochastic systems can essentially
be reduced to the study of unistochastic systems. Hence, assuming
a unistochastic system is not as special a condition as it might initially
seem.

Assuming a unistochastic system based on a unitary time-evolution
operator $\timeevop\left(t\right)$ that is a differentiable function
of the time $t$, one can define a corresponding self-adjoint generator
$H\left(t\right)$, called the system's \emph{Hamiltonian}, according
to 
\begin{equation}
H\left(t\right)\defeq i\hbar\frac{\partial\timeevop\left(t\right)}{\partial t}\timeevop^{\adj}\left(t\right)=H^{\adj}\left(t\right).\label{eq:DefHamiltonian}
\end{equation}
 Here the factor of $i$ ensures that the $N\times N$ matrix $H\left(t\right)$
is self-adjoint, and, for present purposes, the \emph{reduced Planck constant}
$\hbar$ is a fixed quantity introduced for purposes of measurement
units. Ultimately, the specific numerical value of $\hbar$ in any
given set of units must be determined empirically by comparison with
experiments.

In terms of the Hamiltonian, the system's density matrix $\densitymatrix\left(t\right)$
then evolves in time according to the \emph{von Neumann equation},
\begin{equation}
i\hbar\frac{\partial\densitymatrix\left(t\right)}{\partial t}=\bracks{H\left(t\right),\densitymatrix\left(t\right)},\label{eq:VonNeumannEq}
\end{equation}
 its state vector $\Psi\left(t\right)$ (if it exists) evolves according
to the \emph{Schr{\" o}dinger equation}, 
\begin{equation}
i\hbar\frac{\partial\Psi\left(t\right)}{\partial t}=H\left(t\right)\Psi\left(t\right),\label{eq:SchrodingerEq}
\end{equation}
 its Heisenberg-picture random variables $A^{H}\left(t\right)$ evolve
according to the \emph{Heisenberg equation of motion}, 
\begin{equation}
\frac{dA^{H}\left(t\right)}{dt}=\frac{i}{\hbar}\bracks{H^{H}\left(t\right),A^{H}\left(t\right)}+\left(\frac{\partial A\left(t\right)}{\partial t}\right)^{H},\label{eq:HeisenbergEquationOfMotion}
\end{equation}
 and its expectation values $\expectval{A\left(t\right)}$ evolve
according to the \emph{Ehrenfest equation}, 
\begin{equation}
\frac{d\angs{A\left(t\right)}}{dt}=\frac{i}{\hbar}\tr\parens{\bracks{H\left(t\right),A\left(t\right)}\densitymatrix\left(t\right)}+\expectval{\frac{\partial A\left(t\right)}{\partial t}}.\label{eq:EhrenfestEquation}
\end{equation}
 The matrix $H^{H}\left(t\right)$ appearing in the Heisenberg equation
of motion \eqref{eq:HeisenbergEquationOfMotion} is the Hamiltonian
expressed in the Heisenberg picture \eqref{eq:DefHeisenbergPicture}.
Note also that the brackets $\bracks{X,Y}$ that naturally show up
in these equations are genuine \emph{commutators} $XY-YX$, not Poisson
brackets, and involve products of non-diagonal matrices that do not
generally commute with each other under multiplication.

The emergence of these famous equations from a physical model based
on a stochastically evolving trajectory in a configuration space $\configspace$
is a surprising new result.

Intriguingly, if the system's time-evolution operator $\dynop\left(t\right)=\timeevop\left(t\right)$
is indeed unitary, then under the unitary gauge transformation defined
by \eqref{eq:DefLocalInTimeUnitaryTransformation}, the Hamiltonian
transforms precisely as a \emph{non-Abelian gauge potential}: 
\begin{equation}
\eqsbrace{\begin{aligned} & H\left(t\right)\mapsto H_{V}\left(t\right)\\
 & \qquad=V\left(t\right)H\left(t\right)V^{\adj}\left(t\right)-i\hbar V\left(t\right)\frac{\partial V^{\adj}\left(t\right)}{\partial t}.
\end{aligned}
\negmedspace\negmedspace\negmedspace\negmedspace\negmedspace\negmedspace}\label{eq:HamiltonianNonAbelianTransformation}
\end{equation}
 This transformation behavior makes clear that a Hamiltonian is not
a gauge-invariant observable, even though it may happen to coincide
with various observables according to particular gauge choices. (For
pedagogical treatments of non-Abelian gauge theories, see~\citep{PeskinSchroeder:1995iqft,Weinberg:1996tqtfii}.)

Furthermore, one can write the Schr{\" o}dinger equation \eqref{eq:SchrodingerEq}
as 
\begin{equation}
\mathcal{D}\left(t\right)\Psi\left(t\right)=0.\label{eq:GaugeCovariantSchrodiginerEq}
\end{equation}
 Here $\mathcal{D}\left(t\right)$ is a \emph{gauge-covariant derivative}
defined according to 
\begin{equation}
\mathcal{D}\left(t\right)\defeq\idmatrix\frac{\partial}{\partial t}+\frac{i}{\hbar}H\left(t\right),\label{eq:DefGaugeCovDeriv}
\end{equation}
 which maintains its form under the unitary gauge transformations
\eqref{eq:DefLocalInTimeUnitaryTransformation}, in the sense that
\begin{equation}
V\left(t\right)\left[\idmatrix\frac{\partial}{\partial t}+\frac{i}{\hbar}H\left(t\right)\right]\left(\cdots\right)=\left[\idmatrix\frac{\partial}{\partial t}+\frac{i}{\hbar}H_{V}\left(t\right)\right]\left[V\left(t\right)\left(\cdots\right)\right].\label{eq:GaugeCovDerivTransf}
\end{equation}

These formulas make manifest that the Hilbert-space formulation of
a generalized stochastic system is ultimately a collection of gauge-dependent
quantities, or gauge variables. Hence, although a Hilbert-space formulation
may be extremely useful for constructing stochastic dynamics or for
carrying out calculations, one might rightly be suspicious about trying
to assign direct physical meanings to its mathematical ingredients.

Notice that if one picks the gauge-transformation matrix $V\left(t\right)$
to be the adjoint of the unistochastic system's time-evolution operator
$\timeevop\left(t\right)$, 
\begin{equation}
V\left(t\right)\defeq\timeevop^{\adj}\left(t\right),\label{eq:DefLocalInTimeUnitaryTransformationForHeisenbergPicture}
\end{equation}
 then the Hamiltonian precisely vanishes: 
\begin{equation}
H_{V}\left(t\right)=0.\label{eq:DefLocalInTimeUnitaryTransformationForHeisenbergPictureHamiltonianZero}
\end{equation}
 This choice of gauge is nothing other than the definition \eqref{eq:DefHeisenbergPicture}
of the Heisenberg picture. Unitary gauge transformations \eqref{eq:DefLocalInTimeUnitaryTransformation}
can therefore be viewed as generalized changes of time-evolution picture.\footnote{The fact that one can set $H_{V}\left(t\right)=0$ for all $t$ is
a manifestation of the fact that the \emph{fiber bundle} in this
case, consisting of copies of the system's Hilbert space fibered over
a one-dimensional base manifold parameterized by the time $t$, has
vanishing curvature.}

\subsection{Interference\label{subsec:Interference}}

The appearance of the Schr{\" o}dinger equation \eqref{eq:SchrodingerEq}
is an important signal that the dictionary \eqref{eq:DefDictionary}
is more than just a tool for using Hilbert-space methods to craft
highly general forms of stochastic dynamics. It also suggests that
generalized stochastic systems might have the resources to replicate
the features of quantum theory more broadly.

As another hint pointing in this direction, one starts by noting that
an arbitrary time-dependent transition matrix $\stochasticmatrix\left(t\right)$
is generically \emph{indivisible}, in the sense that it does not
satisfy the divisibility property \eqref{eq:DivisibilityCondition}
at arbitrary times. To see what goes wrong with divisibility, suppose
that at some time $t^{\prime}$, the transition matrix $\stochasticmatrix\left(t^{\prime}\right)$
has a matrix inverse $\stochasticmatrix^{-1}\left(t^{\prime}\right)$,
and define a new $N\times N$ matrix $\tilde{\stochasticmatrix}\left(t\from t^{\prime}\right)$
according to 
\begin{equation}
\tilde{\stochasticmatrix}\left(t\from t^{\prime}\right)\defeq\stochasticmatrix\left(t\right)\stochasticmatrix^{-1}\left(t^{\prime}\right).\label{eq:DefHypotheticalRelativeStochasticMatrix}
\end{equation}
 As an immediate consequence, one then has 
\begin{equation}
\stochasticmatrix\left(t\right)=\tilde{\stochasticmatrix}\left(t\from t^{\prime}\right)\stochasticmatrix\left(t^{\prime}\right),\label{eq:HypotheticalDivisionEvent}
\end{equation}
 which resembles the divisibility property \eqref{eq:DivisibilityCondition}.
However, it follows from an elementary theorem of linear algebra that
the inverse of a stochastic matrix can only be stochastic if both
matrices are permutation matrices, and therefore do not involve nontrivial
probabilities.\footnote{Proof: Let $X$ and $Y$ be $N\times N$ matrices with only non-negative
entries and with $Y=X^{-1}$, so that $XY=\idmatrix$. Then, in particular,
the first row of $X$ must be orthogonal to the second-through-$N$th
columns of $Y$. Because $Y$ is invertible, the columns of $Y$ must
all be linearly independent, so the first row of $X$ must be orthogonal
to the $\left(N-1\right)$-dimensional subspace spanned by the second-through-$N$th
columns of $Y$. Because the entries of $X$ and $Y$ are all non-negative
by assumption, the only way that this orthogonality condition can
hold is if precisely one of the entries in the first row of $X$ is
nonzero, with a $0$ in the corresponding entry in each of the second-through-$N$th
columns of $Y$. Repeating this argument for the other rows of $X$,
one sees that $X$ can only have a single nonzero entry in each row.
If $X$ is a stochastic matrix, then each of these nonzero entries
must be the number $1$, so $X$ must be a permutation matrix. Because
the inverse of a permutation matrix is again a permutation matrix,
it follows that $Y$ must likewise be a permutation matrix.~QED} Hence, the matrix $\tilde{\stochasticmatrix}\left(t\from t^{\prime}\right)$
defined in \eqref{eq:DefHypotheticalRelativeStochasticMatrix} is
not generically stochastic, so \eqref{eq:HypotheticalDivisionEvent}
does not express a genuine form of divisibility.

There is an alternative\textemdash and far-reaching\textemdash way
to understand the generic indivisibility of a time-dependent transition
matrix $\stochasticmatrix\left(t\right)$. To this end, suppose that
$\stochasticmatrix\left(t\right)$ happens to be unistochastic, and
let $\timeevop\left(t\right)$ be a unitary time-evolution operator
for $\stochasticmatrix\left(t\right)$. Then for any two times $t$
and $t^{\prime}$, one can define a \emph{relative} time-evolution
operator 
\begin{equation}
\timeevop\left(t\from t^{\prime}\right)\defeq\timeevop\left(t\right)\timeevop^{\adj}\left(t^{\prime}\right),\label{eq:DefRelativeTimeEvOp}
\end{equation}
 which yields the composition law 
\begin{equation}
\timeevop\left(t\right)=\timeevop\left(t\from t^{\prime}\right)\timeevop\left(t^{\prime}\right).\label{eq:UnitaryCompositionLaw}
\end{equation}
 At the level of the unistochastic transition matrix $\stochasticmatrix\left(t\right)$,
one has from the Schur-Hadamard factorization \eqref{eq:UnistochasticMatrixFromSchurHadamardFactorization}
that 
\begin{equation}
\eqsbrace{\begin{aligned} & \stochasticmatrix\left(t\right)=\overconj{\timeevop\left(t\right)}\hadamardprod\timeevop\left(t\right)\\
 & =\overconj{\left[\timeevop\left(t\from t^{\prime}\right)\timeevop\left(t^{\prime}\right)\right]}\hadamardprod\left[\timeevop\left(t\from t^{\prime}\right)\timeevop\left(t^{\prime}\right)\right],
\end{aligned}
\negthickspace\negthickspace\negthickspace}\label{eq:UnistochasticMatrixSchurHadamardForInterference}
\end{equation}
which cannot generally be expressed in the form $\stochasticmatrix\left(t\from t^{\prime}\right)\stochasticmatrix\left(t^{\prime}\right)$
for any stochastic matrix $\stochasticmatrix\left(t\from t^{\prime}\right)$,
due to the presence of cross terms.

Indeed, examining individual matrix entries, one finds more explicitly
that 
\begin{equation}
\eqsbrace{\begin{aligned} & \stochasticmatrix_{ij}\left(t\right)=\sum_{k=1}^{N}\verts{\timeevop_{ik}\left(t\from t^{\prime}\right)}^{2}\verts{\timeevop_{kj}\left(t^{\prime}\right)}^{2}\\
 & +\sum_{k\ne l}\overconj{\timeevop_{ik}\left(t\from t^{\prime}\right)\timeevop_{kj}\left(t^{\prime}\right)}\timeevop_{il}\left(t\from t^{\prime}\right)\timeevop_{lj}\left(t^{\prime}\right).
\end{aligned}
\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace}\label{eq:UnistochasticInterferenceCalculation}
\end{equation}
 With $\stochasticmatrix_{kj}\left(t^{\prime}\right)$ defined according
to \eqref{eq:UnistochasticMatrixFromAbsValSqTimeEvOp} as usual, 
\begin{equation}
\stochasticmatrix_{kj}\left(t^{\prime}\right)=\verts{\timeevop_{kj}\left(t^{\prime}\right)}^{2},\label{eq:UnistochasticInterferenceIntermedTimeAbsValSq}
\end{equation}
 and defining 
\begin{equation}
\stochasticmatrix_{ik}\left(t\from t^{\prime}\right)\defeq\verts{\timeevop_{ik}\left(t\from t^{\prime}\right)}^{2},\label{eq:RelativeUnistochasticInterferenceIntermedTimeAbsValSq}
\end{equation}
 which is manifestly unistochastic, one sees that the discrepancy
between $\stochasticmatrix\left(t\right)$ and its would-be division
$\stochasticmatrix\left(t\from t^{\prime}\right)\stochasticmatrix\left(t^{\prime}\right)$
is given by 
\begin{equation}
\eqsbrace{\begin{aligned} & \stochasticmatrix_{ij}\left(t\right)-\left[\stochasticmatrix\left(t\from t^{\prime}\right)\stochasticmatrix\left(t^{\prime}\right)\right]_{ij}\\
 & =\sum_{k\ne l}\overconj{\timeevop_{ik}\left(t\from t^{\prime}\right)\Psi_{k}\left(t^{\prime}\right)}\timeevop_{il}\left(t\from t^{\prime}\right)\Psi_{l}\left(t^{\prime}\right),
\end{aligned}
\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace}\label{eq:UnistochasticInterference}
\end{equation}
 where $\Psi\left(t^{\prime}\right)\defeq\dynop\left(t^{\prime}\right)e_{j}$
is the system's state vector at the time $t^{\prime}$, in keeping
with the general definition of state vectors in \eqref{eq:DefStateVector}.

Remarkably, the right-hand side of \eqref{eq:UnistochasticInterference}
gives the general mathematical formula for quantum interference, despite
the absence of manifestly quantum-theoretic assumptions. One sees
from this analysis that interference is a direct consequence of stochastic
dynamics not generally being divisible. More precisely, interference
is nothing more than a generic discrepancy between \emph{actual} indivisible
stochastic dynamics and \emph{hypothetically} divisible stochastic
dynamics. 

In particular, quantum-mechanical interference does not imply that
matter has a physically wavelike nature, contrary to frequent claims
in textbook treatments like~\citep{FeynmanLeightonSands:1965tflopv3}.
Indeed, from the perspective of the present discussion, the notion
that quantum-mechanical interference ever necessitated assigning matter
a physically wavelike quality was merely an unfortunate accident of
history, arising from the fact that many early empirical examples
of interference in quantum systems happened to resemble the behavior
of interfering waves propagating in three-dimensional physical space.

These historical examples were clearly special cases. \emph{Multiparticle}
systems have Schr{\" o}dinger waves that propagate through high-dimensional
configuration spaces, as was noted by Schr{\" o}dinger himself in his early
work on wave mechanics~\citep{Schrodinger:1926autotmoaam}. For more
abstract systems, like \emph{qubits}, there fail to exist continuous
configuration spaces for Schr{\" o}dinger waves altogether.

This new way of thinking about quantum-mechanical interference has
implications for the interpretation of the famous \emph{double-slit experiment}.
Recall that in the double-slit experiment, an emitter sends particles
one at a time toward a wall with two slits in it, and a detection
screen on the other side of the wall records the particle's eventual
landing site. In the usual \textquoteleft classical\textquoteright{}
description of the experiment, one asks first which slit the particle
enters, and then, \emph{conditioning} \emph{on the answer}, one then
\emph{restarts} the dynamics with that slit as the new initial condition.
Over many repetitions of the experiment, the detection screen records
a blend consisting of the statistical distribution of landing sites
from particles passing through the \emph{upper} slit with the statistical
distribution of landing sites from particles passing through the \emph{lower}
slit. In the case of quantum-mechanical particles like electrons,
however, one instead finds that the landing sites form a wavelike
interference pattern, and the conclusion is supposedly that each particle
is really a Schr{\" o}dinger wave of some kind, or that the particle fails
to go through one slit or the other.\footnote{The exposition in~\citep{FeynmanLeightonSands:1965tflopv3} ends
up at precisely such a conclusion: ``It is \emph{not} true that the
electrons go \emph{either} through hole 1 or hole 2.'' {[}Emphasis
in the original.{]} This conclusion, however, does not logically follow
from the empirical appearance of interference effects, but also implicitly
depends on the hidden assumption that the behavior of an electron
in a double-slit experiment can be described by divisible dynamics.}

According to the approach laid out in this paper, the particle really
does go through a specific slit in each run of the experiment. The
final interference pattern on the detection screen is due to the generic
indivisibility of time evolution for quantum systems. One cannot divide
up the particle's evolution into, firstly, its transit from the emitter
to the slits, and then secondly, conditioned on which slit the particle
enters, the particle's transit from the slits to the detection screen.
The interference that shows up in the double-slit experiment may be
surprising, but that is only because indivisible stochastic dynamics
can be highly nonintuitive. In the historical absence of a sufficiently
comprehensive framework for describing indivisible stochastic dynamics,
it was difficult to recognize just how nonintuitive such dynamics
could be, or what sorts of empirically appearances it could produce.

In response to this last point, one might suggest that Schr{\" o}dinger
waves nonetheless offer a superior means of explaining why the double-slit
experiment yields the results that it does. Unfortunately, such hopes
are dashed as soon as one considers sending \emph{two} particles toward
the slits on each run of the experiment. A two-particle system's Schr{\" o}dinger
wave evolves in a \emph{six-dimensional} configuration space, which
is arguably not more intuitive than indivisible stochastic dynamics.

Of course, if one regards the quantum-mechanical particles that make
up matter as arising more fundamentally from underlying \emph{quantum fields},
then the wavelike properties of those quantum fields ensure that particles
of matter have wavelike properties as well, and therefore exhibit
\emph{wave-particle duality}. That said, there is nothing about the
analysis of the double-slit experiment alone that calls for positing
quantum fields. The necessity of quantum field theory comes from other
theoretical and empirical considerations. (For a modern motivation,
see \citep{Weinberg:1996tqtfi}.) One should also keep in mind that
quantum fields are conceptually distinct from Schr{\" o}dinger wave functions.

\subsection{Implications of Interference\label{subsec:Implications-of-Interference}}

The fact that interference shows up in a sufficiently generic stochastic
model means that relative phase factors in state vectors have clear
empirical signatures, even in the absence of the usual axioms of textbook
quantum theory. These empirical manifestations of relative phases
are strong evidence that it should be possible to carry out measurements
on a much wider set of observables than those that are represented
by diagonal matrices \eqref{eq:DefDiagonalRandomVariableMatrix} in
a generalized stochastic system's configuration basis. Indeed, Subsection~\ref{subsec:The-Measurement-Process}
will show that non-diagonal, self-adjoint matrices will turn out to
be candidate observables as well.

Thinking more broadly, this overall analysis means that if one is
given an indivisible generalized stochastic system, then there will
generically be a quantitative discrepancy between the system's actual
behavior\textemdash as predicted theoretically or measured empirically\textemdash and
predictions made for the system based on the nearest divisible or
Markovian approximation to the system's stochastic dynamics. Again,
this discrepancy is precisely interference.

One way to understand this discrepancy is to note that under a divisibility
approximation, one can assign definite probabilities to each of the
system's possible trajectories by iteratively applying transition
matrices, according to the composition law $\stochasticmatrix\left(t\right)=\stochasticmatrix\left(t\from t^{\prime}\right)\stochasticmatrix\left(t^{\prime}\right)$
from \eqref{eq:DivisibilityCondition}. Because iteratively applying
transition matrices is not possible for a generalized stochastic system
with indivisible dynamics, such systems do not generically have well-defined
probabilities for entire possible trajectories.

In the Hilbert-space formulation of a generalized stochastic system,
one can assign complex-valued quantities called \emph{amplitudes}
to the system's possible trajectories, using the fact that unitary
time-evolution operators can be composed or applied iteratively, as
in $\timeevop\left(t\right)=\timeevop\left(t\from t^{\prime}\right)\timeevop\left(t^{\prime}\right)$
from \eqref{eq:UnitaryCompositionLaw}. These amplitudes form the
conceptual basis for the \emph{path-integral formulation} of quantum
theory~\citep{Dirac:1933tliqm,Feynman:1942tpolaiqm,Feynman:1948statnrqm}.
From the standpoint of the stochastic-quantum correspondence, which
gives an alternative formulation of quantum theory, the fact that
these amplitudes \textquoteleft interfere\textquoteright{} with each
other does not mean that they all physically occur in some sort of
literal superposition, or that the system simultaneously takes all
such paths in reality, but is merely an artifact of the indivisible
dynamics of the underlying generalized stochastic system.

Collectively, the foregoing observations lead to the concrete prediction
that interference should arise in a much broader class of contexts
than just for quantum systems. Indeed, given any probabilistically
evolving system with indivisible or non-Markovian dynamics, one can
now interpret any discrepancies between the behavior of such a system
and the behavior of its nearest divisible or Markovian approximation
as manifestations of interference. One could therefore imagine experimentally
measuring interference effects for essentially any system that can
be modeled using indivisible or non-Markovian stochastic dynamics.

\subsection{Division Events and the Markov Approximation\label{subsec:Division-Events-and-the-Markov-Approximation}}

Why do discrete-time Markov chains \eqref{eq:DefDiscreteMarkovChain}
provide such a good approximation to so many stochastic processes
in the real world? One intuitively reasonable explanation is that
when a system is not isolated from a noisy and intrusive environment,
delicate correlations from one time to another \textquoteleft wash
out\textquoteright{} over short time scales as those correlations
leak out into the environment.

Deriving this intuitive picture from first principles in a more precise
way might appear to be a difficult task. Indeed, such a derivation
would seem to require finding a more general framework for describing
a non-Markovian process, and then showing that such a process becomes
approximately Markovian in the appropriate physical circumstances.
Fortunately, this paper provides just such a framework.

To set things up, one starts by introducing a composite system $\mathcal{S}\mathcal{E}$
consisting of a \emph{subject system} $\mathcal{S}$ together with
an \emph{environment} $\mathcal{E}$. The configurations of the subject
system's configuration space $\configspace_{\mathcal{S}}$ will be
labeled by $i=1,\dots,N$, and the configurations of the environment's
configuration space $\configspace_{\mathcal{E}}$ will be labeled
by $e=1,\dots,M$, where $M\geq N$. The configuration space of the
composite system is then the Cartesian product\footnote{The right-hand side of \eqref{eq:CompositeSystemSubjectEnvironmentConfigSpaceFromCartesianProduct}
is indeed a Cartesian product, not a tensor product, because this
equation is solely a statement about the composite system's configuration
space, not its dynamics or its Hilbert-space representation.} 
\begin{equation}
\configspace_{\mathcal{S}\mathcal{E}}=\configspace_{\mathcal{S}}\cartesianprod\configspace_{\mathcal{E}},\label{eq:CompositeSystemSubjectEnvironmentConfigSpaceFromCartesianProduct}
\end{equation}
 meaning that each element of $\configspace_{\mathcal{S}\mathcal{E}}$
is a simple ordered pair of the form $\left(i,e\right)$. One then
singles out $N$ configurations of the environment by labeling them
as $e\left(1\right),\dots,e\left(N\right)$.

For the dynamics, suppose for simplicity that the composite system
evolves according to an overall unistochastic transition matrix 
\begin{equation}
\stochasticmatrix^{\mathcal{S}\mathcal{E}}\left(t\right)=\overconj{\timeevop^{\mathcal{S}\mathcal{E}}\left(t\right)}\hadamardprod\timeevop^{\mathcal{S}\mathcal{E}}\left(t\right),\label{eq:CompositeSubjectEnvironmentUnistochasticMatrixFromSchurHadamardFactorization}
\end{equation}
 or, in terms of individual entries, 
\begin{equation}
\stochasticmatrix_{ie,i_{0}e_{0}}^{\mathcal{S}\mathcal{E}}\left(t\right)=\verts{\timeevop_{ie,i_{0}e_{0}}^{\mathcal{S}\mathcal{E}}\left(t\right)}^{2}.\label{eq:CompositeSubjectEnvironmentUnistochasticMatrixFromAbsValSquareUnitary}
\end{equation}
 Furthermore, suppose that the subject system and the environment
interact up to a time $t^{\prime}>0$ in such a way that they end
up with joint probabilities of the form 
\begin{equation}
p_{i^{\prime}e^{\prime}}^{\mathcal{S}\mathcal{E}}\left(t^{\prime}\right)=p_{i^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)\delta_{e^{\prime}e\parens{i^{\prime}}},\label{eq:CompositeSubjectEnvironmentIntermedJointProbabilitiesCorrelated}
\end{equation}
 which describe an idealized \emph{statistical correlation} between
the configuration $i^{\prime}$ of the subject system at $t^{\prime}$
and the corresponding configuration $e\parens{i^{\prime}}$ of the
environment. 

If there is to be any possibility of the two subsystems evolving independently
for times $t>t^{\prime}$ after the interaction has concluded, then
it should be possible to factorize the composite system's relative
time-evolution operator $\timeevop^{\mathcal{S}\mathcal{E}}\left(t\from t^{\prime}\right)$
between the two subsystems for $t>t^{\prime}$ as the following tensor
product:\footnote{Note the natural appearance of a tensor product in \eqref{eq:CompositeSubjectEnvironmentTimeEvOpFactorizesAfterInteraction},
which is a statement about the composite system's dynamics in the
system's Hilbert-space representation.} 
\begin{equation}
\eqsbrace{\begin{aligned}\timeevop^{\mathcal{S}\mathcal{E}}\left(t\from t^{\prime}\right) & =\timeevop^{\mathcal{S}}\left(t\from t^{\prime}\right)\tensorprod\timeevop^{\mathcal{E}}\left(t\from t^{\prime}\right)\\
 & \qquad\qquad\textrm{for }t>t^{\prime}.
\end{aligned}
\negthickspace\negthickspace\negthickspace}\label{eq:CompositeSubjectEnvironmentTimeEvOpFactorizesAfterInteraction}
\end{equation}
 In terms of individual entries, one has 
\begin{equation}
\eqsbrace{\begin{aligned}\timeevop_{ie,i^{\prime}e^{\prime}}^{\mathcal{S}\mathcal{E}}\left(t\from t^{\prime}\right) & =\timeevop_{ii^{\prime}}^{\mathcal{S}}\left(t\from t^{\prime}\right)\timeevop_{ee^{\prime}}^{\mathcal{E}}\left(t\from t^{\prime}\right)\\
 & \qquad\qquad\textrm{for }t>t^{\prime},
\end{aligned}
\negthickspace\negthickspace\negthickspace}\label{eq:CompositeSubjectEnvironmentTimeEvOpFactorizesAfterInteraction-1}
\end{equation}
 meaning that each entry $\timeevop_{ie,i^{\prime}e^{\prime}}^{\mathcal{S}\mathcal{E}}\left(t\from t^{\prime}\right)$
of the composite system's relative time-evolution operator is the
product of corresponding entries $\timeevop_{ii^{\prime}}^{\mathcal{S}}\left(t\from t^{\prime}\right)$
and $\timeevop_{ee^{\prime}}^{\mathcal{E}}\left(t\from t^{\prime}\right)$
of the relative time-evolution operators for the two subsystems individually.

In light of the Born rule \eqref{eq:ConfigurationBornRuleFromStateVector},
the joint probabilities \eqref{eq:CompositeSubjectEnvironmentIntermedJointProbabilitiesCorrelated}
correspond to a wave function\footnote{If necessary, one can easily write down idealized examples of unitary
time-evolution operators for the composite system that produce the
wave function \eqref{eq:CompositeSubjectEnvironmentIntermedWaveFunctionCorrelated}.
For instance, one could use $\timeevop^{\mathcal{S}\mathcal{E}}\left(t^{\prime}\right)\defeq\sum_{i^{\prime}}\projector_{i^{\prime}}^{\mathcal{S}}\tensorprod R_{e\parens{i^{\prime}}}^{\mathcal{E}}$,
where $\projector_{i^{\prime}}^{\mathcal{S}}$ is the $i^{\prime}$th
configuration projector \eqref{eq:DefConfigurationProjectors} for
the subject system, and where $R_{e\parens{i^{\prime}}}^{\mathcal{E}}$
is a unitary transformation that takes the environment's initial configuration
to the configuration $e\parens{i^{\prime}}$.} 

\begin{equation}
\Psi_{i^{\prime}e^{\prime}}^{\mathcal{S}\mathcal{E}}\left(t^{\prime}\right)=\Psi_{i^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)\delta_{e^{\prime}e\parens{i^{\prime}}}.\label{eq:CompositeSubjectEnvironmentIntermedWaveFunctionCorrelated}
\end{equation}
 The composite system's wave function at later times $t>t^{\prime}$
after the interaction is therefore given in terms of the relative
time-evolution operator \eqref{eq:CompositeSubjectEnvironmentTimeEvOpFactorizesAfterInteraction-1}
according to 

\begin{equation}
\eqsbrace{\begin{aligned} & \Psi_{ie}^{\mathcal{S}\mathcal{E}}\left(t\right)=\sum_{i^{\prime},e^{\prime}}\timeevop_{ie,i^{\prime}e^{\prime}}^{\mathcal{S}\mathcal{E}}\left(t\from t^{\prime}\right)\Psi_{i^{\prime}e^{\prime}}^{\mathcal{S}\mathcal{E}}\left(t^{\prime}\right)\\
 & =\sum_{i^{\prime}}\timeevop_{ii^{\prime}}^{\mathcal{S}}\left(t\from t^{\prime}\right)\Psi_{i^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)\timeevop_{ee\parens{i^{\prime}}}^{\mathcal{E}}\left(t\from t^{\prime}\right).
\end{aligned}
\negthickspace\negthickspace\negthickspace}\label{eq:CompositeSubjectEnvironmentFinalWaveFunctionCalculation}
\end{equation}
 From the Born rule \eqref{eq:ConfigurationBornRuleFromStateVector},
one sees that the joint probabilities for $t>t^{\prime}$ are given
by 
\begin{equation}
p_{ie}^{\mathcal{S}\mathcal{E}}\left(t\right)=\absval{\Psi_{ie}^{\mathcal{S}\mathcal{E}}\left(t\right)}^{2}.\label{eq:CompositeSubjectEnvironmentFinalJointProbabilitiesFromAbsValSq}
\end{equation}
Marginalizing over the configuration $e$ of the environment and invoking
the unitarity of the environment's relative time-evolution operator
$\timeevop^{\mathcal{E}}\left(t\from t^{\prime}\right)$, one obtains
the standalone probabilities $p_{i}^{\mathcal{S}}\left(t\right)$
for the subject system alone for $t>t^{\prime}$: 
\begin{equation}
\eqsbrace{\begin{aligned} & p_{i}^{\mathcal{S}}\left(t\right)=\sum_{e}p_{ie}^{\mathcal{S}\mathcal{E}}\left(t\right)\\
 & =\sum_{i_{1}^{\prime},i_{2}^{\prime}}\overconj{\timeevop_{ii_{1}^{\prime}}^{\mathcal{S}}\left(t\from t^{\prime}\right)\Psi_{i_{1}^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)}\timeevop_{ii_{2}^{\prime}}^{\mathcal{S}}\left(t\from t^{\prime}\right)\Psi_{i_{2}^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)\\
 & \qquad\times\sum_{e}\overconj{\timeevop_{ee\parens{i_{1}^{\prime}}}^{\mathcal{E}}\left(t\from t^{\prime}\right)}\timeevop_{ee\parens{i_{2}^{\prime}}}^{\mathcal{E}}\left(t\from t^{\prime}\right)\\
 & =\sum_{i^{\prime}}\verts{\timeevop_{ii^{\prime}}^{\mathcal{S}}\left(t\from t^{\prime}\right)}^{2}\verts{\Psi_{i^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)}^{2}.
\end{aligned}
\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace}\label{eq:SubjectFinalStandaloneProbabilitiesCalculation}
\end{equation}
 

Taking the limit $t\to t^{\prime}$ in \eqref{eq:SubjectFinalStandaloneProbabilitiesCalculation}
and referring back to the Born rule \eqref{eq:ConfigurationBornRuleFromStateVector}
again, one sees that the subject system's standalone probabilities
at the time $t^{\prime}$ are 
\begin{equation}
p_{i^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)=\verts{\Psi_{i^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)}^{2}.\label{eq:SubjectIntermedStandaloneProbabilitiesFromBornRule}
\end{equation}
 One also sees from \eqref{eq:SubjectFinalStandaloneProbabilitiesCalculation}
that, as in \eqref{eq:RelativeUnistochasticInterferenceIntermedTimeAbsValSq},
one can identify 
\begin{equation}
\stochasticmatrix_{ii^{\prime}}^{\mathcal{S}}\left(t\from t^{\prime}\right)\defeq\verts{\timeevop_{ii^{\prime}}^{\mathcal{S}}\left(t\from t^{\prime}\right)}^{2}.\label{eq:SubjectRelativeUnistochasticMatrix}
\end{equation}
 Hence, \eqref{eq:SubjectFinalStandaloneProbabilitiesCalculation}
simplifies to a genuinely linear relationship that precisely mirrors
the Bayesian marginalization formula \eqref{eq:FinalStandaloneProbabilitiesFromMarginalizationStochasticMatrix},
with $t^{\prime}$ now effectively serving as a new \textquoteleft initial
time\textquoteright : 
\begin{equation}
p_{i}^{\mathcal{S}}\left(t\right)=\sum_{i^{\prime}}\stochasticmatrix_{ii^{\prime}}^{\mathcal{S}}\left(t\from t^{\prime}\right)p_{i^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right).\label{eq:SubjectFinalStandaloneProbabilitiesFromMarginalizationRelativeStochasticMatrix}
\end{equation}

Applying the original Bayesian marginalization formula \eqref{eq:FinalStandaloneProbabilitiesFromMarginalizationStochasticMatrix}
from the actual initial time $0$ to the time $t^{\prime}$, one also
has the equation 
\begin{equation}
p_{i^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)=\sum_{j}\stochasticmatrix_{i^{\prime}j}^{\mathcal{S}}\left(t^{\prime}\right)p_{j}^{\mathcal{S}}\left(0\right).\label{eq:SubjectIntermedStandaloneProbabilitiesFromMarginalizationStochasticMatrix}
\end{equation}
 Combining \eqref{eq:SubjectFinalStandaloneProbabilitiesFromMarginalizationRelativeStochasticMatrix}
with \eqref{eq:SubjectIntermedStandaloneProbabilitiesFromMarginalizationStochasticMatrix}
immediately yields 
\begin{equation}
p_{i}^{\mathcal{S}}\left(t\right)=\sum_{j}\stochasticmatrix_{ij}^{\mathcal{S}}\left(t\right)p_{j}^{\mathcal{S}}\left(0\right),\label{eq:SubjectFinalStandaloneProbabilitiesFromMarginalizationStochasticMatrix}
\end{equation}
 where $\stochasticmatrix^{\mathcal{S}}\left(t\right)$ is a manifestly
\emph{divisible} transition matrix: 
\begin{equation}
\stochasticmatrix^{\mathcal{S}}\left(t\right)\defeq\stochasticmatrix^{\mathcal{S}}\left(t\from t^{\prime}\right)\stochasticmatrix^{\mathcal{S}}\left(t^{\prime}\right).\label{eq:SubjectDivisibleStochasticMatrixDivisionEvent}
\end{equation}
 Thus, the interaction between the subject system $\mathcal{S}$ and
the environment $\mathcal{E}$ up to the time $t^{\prime}$ has led
to a transition matrix $\stochasticmatrix^{\mathcal{S}}\left(t\right)$
for the subject system that is momentarily divisible at $t^{\prime}$.

It is natural to refer to $t^{\prime}$ as a \emph{division event}.
An important corollary is that the initial time $0$ is not a unique
or special time, but is instead only one of many division events inevitably
experienced by a system in sufficiently strong contact with a repeatedly
eavesdropping environment\textemdash in the sense that the interactions
with the environment lead to correlations that look approximately
like those in \eqref{eq:CompositeSubjectEnvironmentIntermedJointProbabilitiesCorrelated}.
Division events will play a crucial role in the rest of this paper.

Suppose that these kinds of division events can be approximated as
occurring regularly over a characteristic time scale $\delta t$.
Suppose, moreover, that the unistochastic dynamics is homogeneous
in time, in the sense that $\timeevop^{\mathcal{S}}\left(t+\delta t\from t\right)=\timeevop^{\mathcal{S}}\left(\delta t\right)$
for all times $t$. Then the subject system's transition matrix after
any integer number $n\geq1$ of time steps $\delta t$ is given by
\begin{equation}
\stochasticmatrix^{\mathcal{S}}\left(n\,\delta t\right)=\left(\stochasticmatrix^{\mathcal{S}}\right)^{n},\label{eq:SubjectDiscreteMarkovChain}
\end{equation}
 where 
\begin{equation}
\stochasticmatrix_{ij}^{\mathcal{S}}\defeq\verts{\timeevop_{ij}^{\mathcal{S}}\left(\delta t\right)}^{2}.\label{eq:SubjectDiscreteMarkovChainUnistochasticMatrix}
\end{equation}
 The stochastic dynamics therefore takes the form of a discrete-time
Markov chain \eqref{eq:DefDiscreteMarkovChain}. This analysis therefore
provides a theoretical explanation for the ubiquity of Markovian stochastic
dynamics in so many real-world cases, and represents another new result.

\subsection{Decoherence\label{subsec:Decoherence}}

Had the environment not interacted with the subject system, then the
subject system's density matrix $\densitymatrix^{\mathcal{S}}\left(t^{\prime}\right)$
at the time $t^{\prime}$ would have generically been non-diagonal,
in accordance with the general definition \eqref{eq:DefTimeDependentDensityMatrix}:
\begin{equation}
\eqsbrace{\begin{aligned}\densitymatrix^{\mathcal{S}}\left(t^{\prime}\right) & =\timeevop^{\mathcal{S}}\left(t^{\prime}\right)\left[\sum_{j}p_{j}\left(0\right)\projector_{j}\right]\timeevop^{\mathcal{S}\adj}\left(t^{\prime}\right)\\
 & =\timeevop^{\mathcal{S}}\left(t^{\prime}\right)\diag{\dots,p_{j}\left(0\right),\dots}\timeevop^{\mathcal{S}\adj}\left(t^{\prime}\right).
\end{aligned}
\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace}\label{eq:SubjectIntermedDensityMatrixWithoutDecoherence}
\end{equation}

By contrast, suppose that the environment indeed interacts with the
subject system to produce a division event \eqref{eq:SubjectDivisibleStochasticMatrixDivisionEvent}
at $t^{\prime}$. In that case, the standalone probability $p_{i}^{\mathcal{S}}\left(t\right)$
for the subject system to occupy its $i$th configuration at $t>t^{\prime}$
is given by the linear marginalization relationship \eqref{eq:SubjectFinalStandaloneProbabilitiesFromMarginalizationRelativeStochasticMatrix},
which can be written instead as 
\begin{equation}
p_{i}^{\mathcal{S}}\left(t\right)=\tr\parens{\projector_{i}\densitymatrix^{\mathcal{S}}\left(t\right)},\label{eq:SubjectFinalStandaloneProbabilityFromTrace}
\end{equation}
 where 
\begin{equation}
\densitymatrix^{\mathcal{S}}\left(t\right)\defeq\timeevop^{\mathcal{S}}\left(t\from t^{\prime}\right)\densitymatrix^{\mathcal{S}}\left(t^{\prime}\right)\timeevop^{\mathcal{S}\adj}\left(t\from t^{\prime}\right),\label{eq:SubjectFindDensityMatrixFromIntermedWithDecoherence}
\end{equation}
 and where, in turn, 
\begin{equation}
\densitymatrix^{\mathcal{S}}\left(t^{\prime}\right)\defeq\sum_{i^{\prime}}p_{i^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)\projector_{i^{\prime}}^{\mathcal{S}}=\diag{\dots,p_{i^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right),\dots},\label{eq:SubjectIntermedDensityMatrixWithDecoherence}
\end{equation}
 which is diagonal.

On comparing the two expressions \eqref{eq:SubjectIntermedDensityMatrixWithoutDecoherence}
and \eqref{eq:SubjectIntermedDensityMatrixWithDecoherence} for the
subject system's density matrix $\densitymatrix\left(t^{\prime}\right)$
at $t^{\prime}$, one sees that the interaction with the environment
has effectively eliminated the off-diagonal entries, or \emph{coherences},
in the subject system's density matrix. This phenomenon is called
\emph{decoherence}, and the foregoing analysis makes clear that decoherence
is nothing more than the mundane  leakage of correlations into the
environment when viewed through the lens of the Hilbert-space formulation.

This analysis also sheds new light on the meaning of coherences in
density matrices, as well as on \emph{superpositions} in state vectors,
where superpositions are related to coherences in the case of a rank-one
density matrix through the formula $\densitymatrix_{i_{1}i_{2}}\left(t\right)=\Psi_{i_{1}}\left(t\right)\overconj{\Psi_{i_{2}}\left(t\right)}$,
in accordance with \eqref{eq:RankOneDensityMatrixFactorizedStateVector}.
From the standpoint of this analysis, superpositions and coherences
are merely indications that one is catching a given system when it
is in the midst of an indivisible stochastic process, between division
events, rather than implying that the system is literally in \textquoteleft multiple
states at once.\textquoteright{}

These results may also help explain why the precise connection between
quantum theory and stochastic processes has historically remained
unclear for so long. If one assumes a Markov approximation, as is
often the case in the research literature on stochastic processes,
then coherences and superposition do not show up, meaning that density
matrices remain diagonal, state vectors remain trivial, and nontrivial
unistochastic dynamics cannot arise.\footnote{See~\citep{GlickAdami:2020manmqm} for an analysis of the connection
between decoherence and Markovian dynamics within the standard Hilbert-space
formulation of quantum theory.}

\subsection{Entanglement\label{subsec:Entanglement}}

Consider next a composite system $\mathcal{A}\mathcal{B}$ consisting
of a pair of subsystems $\mathcal{A}$ and $\mathcal{B}$. Suppose
that the two subsystems do not interact from the initial time $0$
up to some later time $t^{\prime}>0$, but then begin interacting
at $t^{\prime}$. 

For times $t$ between $0$ and $t^{\prime}$, the absence of interactions
means that the composite system's transition matrix $\stochasticmatrix^{\mathcal{A}\mathcal{B}}\left(t\right)$
factorizes into the tensor product of a transition matrix $\stochasticmatrix^{\mathcal{A}}\left(t\right)$
for $\mathcal{A}$ and a separate transition matrix $\stochasticmatrix^{\mathcal{B}}\left(t\right)$
for $\mathcal{B}$: 
\begin{equation}
\stochasticmatrix^{\mathcal{A}\mathcal{B}}\left(t\right)=\stochasticmatrix^{\mathcal{A}}\left(t\right)\tensorprod\stochasticmatrix^{\mathcal{B}}\left(t\right)\quad\textrm{for }0\leq t<t^{\prime}.\label{eq:SubsystemsFactorizableStochasticMatrixBeforeEntanglement}
\end{equation}
 Starting at the time $t^{\prime}$, however, the composite system's
transition matrix $\stochasticmatrix^{\mathcal{A}\mathcal{B}}\left(t\right)$,
which encodes cumulative statistical information and therefore correlations,
will fail to factorize between the two subsystems, in the sense that
\begin{equation}
\stochasticmatrix^{\mathcal{A}\mathcal{B}}\left(t\right)\ne\stochasticmatrix^{\mathcal{A}}\left(t\right)\tensorprod\stochasticmatrix^{\mathcal{B}}\left(t\right)\quad\textrm{for }t>t^{\prime},\label{eq:SubsystemsNotFactorizableStochasticMatrixAfterEntanglement}
\end{equation}
 for any possible transition matrices $\stochasticmatrix^{\mathcal{A}}\left(t\right)$
and $\stochasticmatrix^{\mathcal{B}}\left(t\right)$ that properly
capture the respective dynamics of the two subsystems. (It is worth
noting that this loss of factorization gives a highly general, model-independent
way to define an interaction.) Even if the two subsystems have a notion
of localizability in space, and are eventually placed at a large separation
distance at some time $t>t^{\prime}$, the composite system's transition
matrix will still fail to factorize between the two subsystems, thereby
leading to the appearance of what looks like nonlocal stochastic dynamics
across that separation distance.\footnote{Questions about nonlocality will be addressed in detail in Subsection~\ref{subsec:Nonlocality}.}

However, if the composite system exhibits a division event of the
form \eqref{eq:SubjectDivisibleStochasticMatrixDivisionEvent} at
some later time $t^{\prime\prime}>t^{\prime}$, perhaps due to interactions
between one of the subsystems and the larger environment, then the
composite system's transition matrix $\stochasticmatrix^{\mathcal{A}\mathcal{B}}\left(t\right)$
will divide at $t^{\prime\prime}$: 
\begin{equation}
\stochasticmatrix^{\mathcal{A}\mathcal{B}}\left(t\right)=\stochasticmatrix^{\mathcal{A}\mathcal{B}}\left(t\from t^{\prime\prime}\right)\stochasticmatrix^{\mathcal{A}\mathcal{B}}\left(t^{\prime\prime}\right)\quad\textrm{for }t>t^{\prime\prime}>t^{\prime}.\label{eq:SubsystemsDivisionEvent}
\end{equation}
 If the two subsystems $\mathcal{A}$ and $\mathcal{B}$ do not interact
with each other after $t^{\prime}$, then the \emph{relative} transition
matrix $\stochasticmatrix^{\mathcal{A}\mathcal{B}}\left(t\from t^{\prime\prime}\right)$
appearing in \eqref{eq:SubsystemsDivisionEvent} will factorize between
them, 
\begin{equation}
\stochasticmatrix^{\mathcal{A}\mathcal{B}}\left(t\from t^{\prime\prime}\right)=\stochasticmatrix^{\mathcal{A}}\left(t\from t^{\prime\prime}\right)\tensorprod\stochasticmatrix^{\mathcal{B}}\left(t\from t^{\prime\prime}\right),\label{eq:SubsystemsFactorizableRelativeStochasticMatrixAfterDivisionEvent}
\end{equation}
 so the two subsystems will cease exhibiting what had looked like
nonlocal stochastic dynamics.

This analysis precisely captures the quantum-theoretic notion of
\emph{entanglement}. Systems that interact with each other start
to exhibit what appears to be a nonlocal kind of stochastic dynamics,
even if the systems are moved far apart in physical space, and decoherence
by the environment effectively causes a breakdown in that apparent
dynamical nonlocality.

This stochastic picture of entanglement and nonlocality provides a
new way to understand why they occur in the first place. The indivisible
nature of generic stochastic dynamics could be viewed as a form of
nonlocality in time, which then leads to an apparent nonlocality across
space. A division event leads to an instantaneous restoration of locality
in time, which then leads to a momentary restoration of manifest locality
across space.

\section{Measurements\label{sec:Measurements}}

\subsection{Emergeables\label{subsec:Emergeables}}

The preceding sections have shown that a generalized stochastic system\textemdash that
is, a physical model with kinematics based on a configuration space
and dynamics based on a suitable stochastic law\textemdash is capable
of accounting for signature features of quantum theory, like superposition,
interference, decoherence, and entanglement. In addition, the Hilbert-space
side of the dictionary \eqref{eq:DefDictionary} contains many expressions
and equations that are identical to those found in quantum theory. 

However, an actual quantum system also includes observables beyond
those that are diagonal, as in \eqref{eq:DefDiagonalRandomVariableMatrix},
in a single basis. Indeed, the existence of noncommuting observables
represented by non-diagonal, self-adjoint matrices is another hallmark
feature of quantum theory.

Remarkably, a generalized stochastic system will generically contain
such observables as well. Specifically, Subsection~\ref{subsec:The-Measurement-Process}
will establish that non-diagonal, self-adjoint matrices represent
candidate observables that naturally satisfy the usual probabilistic
rules of quantum theory, including the Born rule, all without the
need to introduce any new fundamental axioms. In so doing, the analysis
ahead will demonstrate that the dictionary \eqref{eq:DefDictionary}
is not merely a tool for studying a broad class of stochastic processes,
but truly defines a comprehensive stochastic-quantum correspondence.

As motivation, let $A$ be a time-independent (diagonal) random variable
\eqref{eq:DefDiagonalRandomVariableMatrix}, and consider the time
derivative of its Heisenberg-picture counterpart $A^{H}\left(t\right)$,
as defined for a generic time-evolution operator $\dynop\left(t\right)$
by \eqref{eq:DefHeisenbergPicture}: 
\begin{equation}
\frac{dA^{H}\left(t\right)}{dt}=\frac{\partial\dynop^{\adj}\left(t\right)}{\partial t}A\dynop\left(t\right)+\dynop^{\adj}\left(t\right)A\frac{\partial\dynop\left(t\right)}{\partial t}.\label{eq:TimeDerivRandomVariable}
\end{equation}
 Evaluating this matrix in the limit $t\to0$ gives an $N\times N$
self-adjoint, generically non-diagonal matrix $\dot{A}$ at the initial
time $0$: 
\begin{equation}
\dot{A}\defeq\lim_{t\to0}\frac{dA^{H}\left(t\right)}{dt}=\dot{A}^{\adj}.\label{eq:DefTimeDerivEmergeableAtTimeZero}
\end{equation}
 This matrix will not generally commute with the original random variable
$A$ itself: 
\begin{equation}
\bracks{A,\dot{A}}\ne0.\label{eq:TimeDerivEmergeableNotCommuteWithRandomVar}
\end{equation}
 Nonetheless, the matrix $\dot{A}$ has physical uses. For example,
one has 
\begin{equation}
\tr\parens{\dot{A}\densitymatrix\left(0\right)}=\lim_{t\to0}\frac{d\expectval{A\left(t\right)}}{dt},\label{eq:TimeDerivExpectationValueFromTimeDerivEmergeable}
\end{equation}
 which is a perfectly meaningful physical quantity, even though the
time derivative of an expectation value is not necessarily the expectation
value of something physical.

The matrix $\dot{A}$ therefore resembles a random variable in some
ways, but incorporates stochastic dynamics directly into its definition
\eqref{eq:DefTimeDerivEmergeableAtTimeZero} through the time-evolution
operator $\dynop\left(t\right)$, and does not have a transparent
interpretation at the level of the generalized stochastic system's
underlying configuration space $\configspace$ alone. Instead, $\dot{A}$
is an emergent amalgam of kinematical and dynamical ingredients, so
it will be called an \emph{emergeable}.\footnote{There is a sense in which emergeables are not an entirely new idea,
but are closely related to emergent physical properties like temperatures
or pressures.} This terminology is intended to contrast $\dot{A}$ with the system's
genuine random variables, which could be called \emph{beables}\textemdash that
is, \textquoteleft be-ables\textquoteright \textemdash to invoke a
term coined by Bell in~\citep{Bell:1972sao}.

As a concrete example, consider a particle whose underlying \emph{position}
is regarded as a physical configuration, corresponding to some random
variable $A$. If the particle's dynamics is stochastic, in the sense
that the particle can be described as a generalized stochastic system,
then the particle's \emph{velocity} (or, equivalently, the particle's
\emph{momentum}) will not generally have a well-defined value at all
times, and will naturally be representable as an emergeable $\dot{A}$
along the lines described here.

\subsection{The Measurement Process\label{subsec:The-Measurement-Process}}

With all the requisite conceptual background now in place, one can
model the measurement of a generic observable as a physical process.
To start, consider a composite system $\mathcal{S}\mathcal{D}\mathcal{E}$
consisting of three subsystems that will be called a \emph{subject system}
$\mathcal{S}$, a \emph{measuring device} $\mathcal{D}$, and an
\emph{environment} $\mathcal{E}$. Note that one of the additional
goals ahead will be to identify the criteria for a subsystem like
$\mathcal{D}$ to be regarded as a genuine measuring device.

Focusing momentarily on the subject system $\mathcal{S}$, consider
an $N\times N$ self-adjoint matrix 
\begin{equation}
\tilde{A}^{\mathcal{S}}=\tilde{A}^{\mathcal{S}\adj},\label{eq:DefSubjectSystemObservableAsSelfAdjoint}
\end{equation}
 which may or may not be one of the subject system's diagonal random
variables.\footnote{More generally, one could take $\tilde{A}^{\mathcal{S}}$ to be a
\emph{normal matrix}, meaning a matrix that commutes with its adjoint
$\tilde{A}^{\mathcal{S}\adj}$.} As a concrete example, $\tilde{A}^{\mathcal{S}}$ could be an emergeable
like \eqref{eq:DefTimeDerivEmergeableAtTimeZero}.

By the \emph{spectral theorem}, $\tilde{A}^{\mathcal{S}}$ has a
\emph{spectral decomposition} of the form 
\begin{equation}
\tilde{A}^{\mathcal{S}}=\sum_{\alpha}\tilde{a}_{\alpha}\tilde{\projector}_{\alpha}^{\mathcal{S}},\label{eq:ObservableSpectralDecomposition}
\end{equation}
 where $\tilde{a}_{\alpha}$ are the eigenvalues of $\tilde{A}^{\mathcal{S}}$
and where $\tilde{\projector}_{\alpha}^{\mathcal{S}}$ are its eigenprojectors.
These eigenprojectors $\tilde{\projector}_{\alpha}^{\mathcal{S}}$
are not generically diagonal, but they nonetheless satisfy the analogues
of the mutual exclusivity condition \eqref{eq:ConfigurationProjectorsMutuallyExclusive},
\begin{equation}
\tilde{\projector}_{\alpha}^{\mathcal{S}}\tilde{\projector}_{\alpha^{\prime}}^{\mathcal{S}}=\delta_{\alpha\alpha^{\prime}}\tilde{\projector}_{\alpha}^{\mathcal{S}},\label{eq:ObservableProjectorsMutuallyExclusive}
\end{equation}
 and the completeness relation \eqref{eq:ConfigurationProjectorsComplete},
\begin{equation}
\sum_{\alpha}\tilde{\projector}_{\alpha}^{\mathcal{S}}=\idmatrix^{\mathcal{S}},\label{eq:ObservableProjectorsComplete}
\end{equation}
 where $\idmatrix^{\mathcal{S}}$ is the identity matrix for the subject
system. These eigenprojectors therefore constitute a projection-valued
measure (PVM) of their own. Letting $\tilde{e}_{\alpha}^{\mathcal{S}}$
be the corresponding orthonormal basis, one has, in analogy with \eqref{eq:ConfigurationBasisOrthonormalComplete},
the following statements: 
\begin{equation}
\tilde{e}_{\alpha}^{\mathcal{S}\adj}\tilde{e}_{\alpha^{\prime}}^{\mathcal{S}}=\delta_{\alpha\alpha^{\prime}},\quad\tilde{e}_{\alpha}^{\mathcal{S}}\tilde{e}_{\alpha}^{\mathcal{S}\adj}=\tilde{\projector}_{\alpha}^{\mathcal{S}}.\label{eq:ObservableBasisOrthonormalComplete}
\end{equation}

If $\tilde{A}^{\mathcal{S}}$ happens to be one of the subject system's
random variables \eqref{eq:DefDiagonalRandomVariableMatrix}, then
the eigenvalues $\tilde{a}_{\alpha}$ are the random variable's magnitudes,
and the eigenprojectors $\tilde{\projector}_{\alpha}^{\mathcal{S}}$
are the configuration projectors \eqref{eq:DefConfigurationProjectors}.
More generally, however, the eigenvalues $\tilde{a}_{\alpha}$ and
the eigenprojectors $\tilde{\projector}_{\alpha}^{\mathcal{S}}$ do
not yet have an obvious physical meaning.

Suppose that the measuring device $\mathcal{D}$ has configurations
$d\parens{\alpha}$ that can be labeled by the same index $\alpha$
that appears in the spectral decomposition \eqref{eq:ObservableSpectralDecomposition}.
Similarly, suppose that the environment $\mathcal{E}$ has configurations
$e\left(\alpha\right)$ that can also be labeled by $\alpha$.

Generalizing \eqref{eq:CompositeSubjectEnvironmentUnistochasticMatrixFromAbsValSquareUnitary}
from the earlier analysis of the decoherence process, suppose, moreover,
that the composite system $\mathcal{S}\mathcal{D}\mathcal{E}$ evolves
according to an overall unistochastic transition matrix: 
\begin{equation}
\stochasticmatrix_{ide,i_{0}d_{0}e_{0}}^{\mathcal{S}\mathcal{D}\mathcal{E}}\left(t\right)=\verts{\timeevop_{ide,i_{0}d_{0}e_{0}}^{\mathcal{S}\mathcal{D}\mathcal{E}}\left(t\right)}^{2}.\label{eq:CompositeSubjectDeviceEnvironmentUnistochasticMatrixFromAbsValSquareUnitary}
\end{equation}
 Generalizing \eqref{eq:CompositeSubjectEnvironmentIntermedWaveFunctionCorrelated}
and letting $\tilde{e}_{\alpha^{\prime},i^{\prime}}^{\mathcal{S}}$
denote the $i^{\prime}$th component of the basis vector $\tilde{e}_{\alpha^{\prime}}^{\mathcal{S}}$
with respect to the subject system's configuration basis $e_{i^{\prime}}^{\mathcal{S}}$,
suppose that the three subsystems interact up to a time $t^{\prime}>0$
in such a way that they end up with the overall wave function\footnote{It is straightforward to write down idealized examples of suitable
unitary time-evolution operators for the composite system. One choice
is $\timeevop^{\mathcal{S}\mathcal{D}\mathcal{E}}\left(t^{\prime}\right)\defeq\sum_{\alpha^{\prime}}\tilde{\projector}_{\alpha^{\prime}}^{\mathcal{S}}\tensorprod R_{d\parens{\alpha^{\prime}}}^{\mathcal{D}}\tensorprod R_{e\parens{\alpha^{\prime}}}^{\mathcal{E}}$,
where $\tilde{\projector}_{\alpha^{\prime}}^{\mathcal{S}}$ is the
$\alpha^{\prime}$th eigenprojector appearing in the spectral decomposition
\eqref{eq:ObservableSpectralDecomposition}, and where $R_{d\parens{\alpha^{\prime}}}^{\mathcal{D}}$
and $R_{e\parens{\alpha^{\prime}}}^{\mathcal{E}}$ are unitary transformations
for the measuring device and the environment that respectively put
them in the configurations $d\parens{\alpha^{\prime}}$ and $e\parens{\alpha^{\prime}}$.} 
\begin{equation}
\Psi_{i^{\prime}d^{\prime}e^{\prime}}^{\mathcal{S}\mathcal{D}\mathcal{E}}\left(t^{\prime}\right)=\sum_{\alpha^{\prime}}\tilde{\Psi}_{\alpha^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)\tilde{e}_{\alpha^{\prime},i^{\prime}}^{\mathcal{S}}\delta_{d^{\prime}d\parens{\alpha^{\prime}}}\delta_{e^{\prime}e\parens{\alpha^{\prime}}},\label{eq:CompositeSubjectDeviceEnvironmentIntermedJointWaveFunctionCorrelated}
\end{equation}
 and that, mirroring \eqref{eq:CompositeSubjectEnvironmentTimeEvOpFactorizesAfterInteraction},
the composite system's relative time-evolution operator factorizes
between the three subsystems for later times $t>t^{\prime}$: 
\begin{equation}
\eqsbrace{\begin{aligned}\timeevop^{\mathcal{S}\mathcal{D}\mathcal{E}}\left(t\from t^{\prime}\right) & =\timeevop^{\mathcal{S}}\left(t\from t^{\prime}\right)\tensorprod\timeevop^{\mathcal{D}}\left(t\from t^{\prime}\right)\\
 & \qquad\tensorprod\timeevop^{\mathcal{E}}\left(t\from t^{\prime}\right)\\
 & \qquad\qquad\qquad\textrm{for }t>t^{\prime}.
\end{aligned}
\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace}\label{eq:CompositeSubjectDeviceEnvironmentTimeEvOpFactorizesAfterInteraction}
\end{equation}
 Then the composite system's wave function for times $t>t^{\prime}$
after the interaction is 

\begin{equation}
\eqsbrace{\begin{aligned}\Psi_{ide}^{\mathcal{S}\mathcal{D}\mathcal{E}}\left(t\right) & =\sum_{i^{\prime},e^{\prime},d^{\prime}}\timeevop_{ide,i^{\prime}d^{\prime}e^{\prime}}^{\mathcal{S}\mathcal{D}\mathcal{E}}\left(t\from t^{\prime}\right)\Psi_{i^{\prime}d^{\prime}e^{\prime}}^{\mathcal{S}\mathcal{D}\mathcal{E}}\left(t^{\prime}\right)\\
 & =\sum_{i^{\prime}}\sum_{\alpha^{\prime}}\timeevop_{ii^{\prime}}^{\mathcal{S}}\left(t\from t^{\prime}\right)\tilde{\Psi}_{\alpha^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)\tilde{e}_{\alpha^{\prime},i^{\prime}}^{\mathcal{S}}\\
 & \qquad\times\timeevop_{dd\parens{\alpha^{\prime}}}^{\mathcal{D}}\left(t\from t^{\prime}\right)\timeevop_{ee\parens{\alpha^{\prime}}}^{\mathcal{E}}\left(t\from t^{\prime}\right).
\end{aligned}
\negthickspace\negthickspace\negthickspace\negthickspace}\label{eq:CompositeSubjectDeviceEnvironmentFinalWaveFunctionCalculation}
\end{equation}

Invoking the Born rule \eqref{eq:ConfigurationBornRuleFromStateVector},
it follows from the explicit expression \eqref{eq:CompositeSubjectDeviceEnvironmentFinalWaveFunctionCalculation}
for the composite system's wave function that the joint probabilities
for $t>t^{\prime}$ are given by 
\begin{equation}
p_{ide}^{\mathcal{S}\mathcal{D}\mathcal{E}}\left(t\right)=\absval{\Psi_{ide}^{\mathcal{S}\mathcal{D}\mathcal{E}}\left(t\right)}^{2}.\label{eq:CompositeSubjectDeviceEnvironmentFinalJointProbabilitiesFromAbsValSq}
\end{equation}
 Marginalizing over the configuration $i$ of the subject system $\mathcal{S}$
as well as the configuration $e$ of the environment $\mathcal{E}$,
and invoking the unitarity of both the subject system's relative time-evolution
operator $\timeevop^{\mathcal{S}}\left(t\from t^{\prime}\right)$
and the environment's relative time-evolution operator $\timeevop^{\mathcal{E}}\left(t\from t^{\prime}\right)$,
one obtains the standalone probabilities $p_{d}^{\mathcal{D}}\left(t\right)$
for the measuring device $\mathcal{D}$ alone for $t>t^{\prime}$:
\begin{equation}
\eqsbrace{\begin{aligned} & p_{d}^{\mathcal{D}}\left(t\right)=\sum_{i,e}p_{ide}^{\mathcal{S}\mathcal{D}\mathcal{E}}\left(t\right)\\
 & =\sum_{i_{1}^{\prime},i_{2}^{\prime}}\sum_{\alpha_{1}^{\prime},\alpha_{2}^{\prime}}\overconj{\timeevop_{dd\parens{\alpha_{1}^{\prime}}}^{\mathcal{D}}\left(t\from t^{\prime}\right)\tilde{\Psi}_{\alpha_{1}^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)\tilde{e}_{\alpha_{1}^{\prime},i_{1}^{\prime}}^{\mathcal{S}}}\\
 & \qquad\times\timeevop_{dd\parens{\alpha_{2}^{\prime}}}^{\mathcal{D}}\left(t\from t^{\prime}\right)\tilde{\Psi}_{\alpha_{2}^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)\tilde{e}_{\alpha_{2}^{\prime},i_{2}^{\prime}}^{\mathcal{S}}\\
 & \qquad\times\sum_{i}\overconj{\timeevop_{ii_{1}^{\prime}}^{\mathcal{S}}\left(t\from t^{\prime}\right)}\timeevop_{ii_{2}^{\prime}}^{\mathcal{S}}\left(t\from t^{\prime}\right)\\
 & \qquad\times\sum_{e}\overconj{\timeevop_{ee\parens{\alpha_{1}^{\prime}}}^{\mathcal{E}}\left(t\from t^{\prime}\right)}\timeevop_{ee\parens{\alpha_{2}^{\prime}}}^{\mathcal{E}}\left(t\from t^{\prime}\right)\\
 & =\sum_{\alpha^{\prime}}\verts{\timeevop_{dd\parens{\alpha^{\prime}}}^{\mathcal{D}}\left(t\from t^{\prime}\right)}^{2}\verts{\tilde{\Psi}_{\alpha^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)}^{2}.
\end{aligned}
\negthickspace\negthickspace\negthickspace}\label{eq:SubjectFinalStandaloneProbabilitiesAfterMeasurementCalculation}
\end{equation}

In the limit $t\to t^{\prime}$, the last line of \eqref{eq:SubjectFinalStandaloneProbabilitiesAfterMeasurementCalculation}
implies that 
\begin{equation}
p_{d\parens{\alpha^{\prime}}}^{\mathcal{D}}\left(t^{\prime}\right)=\verts{\tilde{\Psi}_{\alpha^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)}^{2}.\label{eq:DeviceBornRule}
\end{equation}
 Hence, the measuring device $\mathcal{D}$ has a standalone probability
$\verts{\tilde{\Psi}_{\alpha^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)}^{2}$
of ending up in its configuration $d\parens{\alpha^{\prime}}$, exactly
as predicted by the textbook version of the Born rule. One can then
naturally define an expectation value $\angs{\tilde{A}^{\mathcal{S}}\left(t^{\prime}\right)}$
for $\tilde{A}^{\mathcal{S}}$ at $t^{\prime}$ as the usual kind
of statistical average: 
\begin{equation}
\angs{\tilde{A}^{\mathcal{S}}\left(t^{\prime}\right)}\defeq\sum_{\alpha}\tilde{a}_{\alpha}p_{d\parens{\alpha^{\prime}}}^{\mathcal{D}}\left(t^{\prime}\right).\label{eq:ExpectationValueObservableFromMeasurementProbabilities}
\end{equation}

This analysis establishes that as long as there exists a form of unistochastic
time evolution \eqref{eq:CompositeSubjectDeviceEnvironmentUnistochasticMatrixFromAbsValSquareUnitary}
for the composite system $\mathcal{S}\mathcal{D}\mathcal{E}$ that
arrives at the wave function \eqref{eq:CompositeSubjectDeviceEnvironmentIntermedJointWaveFunctionCorrelated},
the matrix $\tilde{A}^{\mathcal{S}}$ represents a genuine observable,
in the sense that the time evolution \eqref{eq:CompositeSubjectDeviceEnvironmentUnistochasticMatrixFromAbsValSquareUnitary}
leads to the measuring device ending up in the correct configuration
with the correct Born-rule probability.

For times $t>t^{\prime}$ after the interaction, the last line of
\eqref{eq:SubjectFinalStandaloneProbabilitiesAfterMeasurementCalculation}
implies that the time $t^{\prime}$ is a division event for the measuring
device: 
\begin{equation}
\stochasticmatrix^{\mathcal{D}}\left(t\right)=\stochasticmatrix^{\mathcal{D}}\left(t\from t^{\prime}\right)\stochasticmatrix^{\mathcal{D}}\left(t^{\prime}\right)\quad\textrm{for }t>t^{\prime}.\label{eq:DeviceDivisionEvent}
\end{equation}
 Here the measuring device's dynamics for times $t>t^{\prime}$ is
given by the relative unistochastic transition matrix 
\begin{equation}
\stochasticmatrix_{dd\parens{\alpha^{\prime}}}^{\mathcal{D}}\left(t\from t^{\prime}\right)\defeq\verts{\timeevop_{dd\parens{\alpha^{\prime}}}^{\mathcal{D}}\left(t\from t^{\prime}\right)}^{2}.\label{eq:DeviceRelativeUnistochasticMatrix}
\end{equation}

By contrast, if the observable $\tilde{A}^{\mathcal{S}}$ is an emergeable,
as opposed to one of the subject system's (diagonal) random variables
\eqref{eq:DefDiagonalRandomVariableMatrix}, then the subject system
$\mathcal{S}$ does not experience a division event at $t^{\prime}$.
Instead, the subject system remains mired in indivisible time evolution
at $t^{\prime}$, with some stochastically evolving underlying configuration.
Moreover, if indeed $\tilde{A}^{\mathcal{S}}$ is an emergeable, then
the measurement result obtained by the measuring device is an emergent
effect of the interaction between the subject system and the measuring
device, in a sense suggested by Bohr in 1935~\citep{Bohr:1935cqmdoprbcc,Bell:1971itthvq},
rather than transparently revealing a physical aspect of the configuration
of the subject system alone. 

Despite $t^{\prime}$ not necessarily being a division event for the
subject system $\mathcal{S}$, one can nevertheless compute the standalone
probability $p_{i}^{\mathcal{S}}\left(t\right)$ for the subject system
to be in its $i$th configuration for times $t>t^{\prime}$ by marginalizing
over the measuring device $\mathcal{D}$ and the environment $\mathcal{E}$: 

\begin{equation}
\eqsbrace{\begin{aligned} & p_{i}^{\mathcal{S}}\left(t\right)=\sum_{d,e}p_{ide}^{\mathcal{S}\mathcal{D}\mathcal{E}}\left(t\right)\\
 & =\sum_{i_{1}^{\prime},i_{2}^{\prime}}\sum_{\alpha_{1}^{\prime},\alpha_{2}^{\prime}}\overconj{\timeevop_{ii_{1}^{\prime}}^{\mathcal{S}}\left(t\from t^{\prime}\right)\tilde{\Psi}_{\alpha_{1}^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)\tilde{e}_{\alpha_{1}^{\prime},i_{1}^{\prime}}^{\mathcal{S}}}\\
 & \qquad\times\timeevop_{ii_{2}^{\prime}}^{\mathcal{S}}\left(t\from t^{\prime}\right)\tilde{\Psi}_{\alpha_{2}^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)\tilde{e}_{\alpha_{2}^{\prime},i_{2}^{\prime}}^{\mathcal{S}}\\
 & \qquad\times\sum_{d}\overconj{\timeevop_{dd\parens{\alpha_{1}^{\prime}}}^{\mathcal{D}}\left(t\from t^{\prime}\right)}\timeevop_{dd\parens{\alpha_{2}^{\prime}}}^{\mathcal{D}}\left(t\from t^{\prime}\right)\\
 & \qquad\times\sum_{e}\overconj{\timeevop_{ee\parens{\alpha_{1}^{\prime}}}^{\mathcal{E}}\left(t\from t^{\prime}\right)}\timeevop_{ee\parens{\alpha_{2}^{\prime}}}^{\mathcal{E}}\left(t\from t^{\prime}\right)\\
 & =\sum_{\alpha^{\prime}}\left[\sum_{i_{1}^{\prime},i_{2}^{\prime}}\overconj{\timeevop_{ii_{1}^{\prime}}^{\mathcal{S}}\left(t\from t^{\prime}\right)}\timeevop_{ii_{2}^{\prime}}^{\mathcal{S}}\left(t\from t^{\prime}\right)\tilde{e}_{\alpha^{\prime},i_{2}^{\prime}}^{\mathcal{S}}\overconj{\tilde{e}_{\alpha^{\prime},i_{1}^{\prime}}^{\mathcal{S}}}\right]\\
 & \qquad\times\verts{\tilde{\Psi}_{\alpha^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)}^{2}.
\end{aligned}
\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace}\label{eq:SubjectFinalStandaloneProbabilitiesFromHybridIntermedDeviceCalculation}
\end{equation}

Recognizing $\verts{\tilde{\Psi}_{\alpha^{\prime}}^{\mathcal{S}}\left(t^{\prime}\right)}^{2}$
from \eqref{eq:DeviceBornRule} as the standalone probability $p_{d\parens{\alpha^{\prime}}}^{\mathcal{D}}\left(t^{\prime}\right)$
for the measuring device $\mathcal{D}$ to end up in its configuration
$d\parens{\alpha^{\prime}}$ at the time $t^{\prime}$, and recalling
both the configuration projectors $\projector_{i}^{\mathcal{S}}$
defined in \eqref{eq:DefConfigurationProjectors} as well as the eigenprojectors
$\tilde{\projector}_{\alpha}^{\mathcal{S}}$ appearing in the spectral
decomposition \eqref{eq:ObservableSpectralDecomposition} for $\tilde{A}^{\mathcal{S}}$,
one can write \eqref{eq:SubjectFinalStandaloneProbabilitiesFromHybridIntermedDeviceCalculation}
more succinctly as 
\begin{equation}
p_{i}^{\mathcal{S}}\left(t\right)=\tr\parens{\projector_{i}^{\mathcal{S}}\densitymatrix^{\mathcal{S}}\left(t\right)}.\label{eq:SubjectFinalStandaloneProbabilitiesFromTraceAfterMeasurement}
\end{equation}
 Here the subject system's density matrix $\densitymatrix^{\mathcal{S}}\left(t\right)$
for $t>t^{\prime}$ is given by 
\begin{equation}
\densitymatrix^{\mathcal{S}}\left(t\right)\defeq\timeevop^{\mathcal{S}}\left(t\from t^{\prime}\right)\left[\sum_{\alpha^{\prime}}p_{d\parens{\alpha^{\prime}}}^{\mathcal{D}}\left(t^{\prime}\right)\tilde{\projector}_{\alpha^{\prime}}^{\mathcal{S}}\right]\timeevop^{\mathcal{S}\adj}\left(t\from t^{\prime}\right).\label{eq:SubjectFinalDensityMatrixAfterMeasurement}
\end{equation}
 One can therefore recast the expectation value \eqref{eq:ExpectationValueObservableFromMeasurementProbabilities}
for $\tilde{A}^{\mathcal{S}}$ as 
\begin{equation}
\angs{\tilde{A}^{\mathcal{S}}\left(t^{\prime}\right)}=\tr\parens{\tilde{A}^{\mathcal{S}}\densitymatrix^{\mathcal{S}}\left(t^{\prime}\right)},\label{eq:ExpectationValueObservableFromTrace}
\end{equation}
 which precisely mirrors the formula \eqref{eq:ExpectationValueRandomVariableFromTrace}
for the expectation value of a (diagonal) random variable.

Furthermore, \eqref{eq:SubjectFinalStandaloneProbabilitiesFromHybridIntermedDeviceCalculation}
yields a \emph{linear} relationship between the standalone probabilities
$p_{d\parens{\alpha^{\prime}}}^{\mathcal{D}}\left(t^{\prime}\right)$
for the measuring device $\mathcal{D}$ at $t^{\prime}$ and the standalone
probabilities $p_{i}^{\mathcal{S}}\left(t\right)$ for the subject
system $\mathcal{S}$ at $t>t^{\prime}$: 
\begin{equation}
p_{i}^{\mathcal{S}}\left(t\right)=\sum_{\alpha^{\prime}}\stochasticmatrix_{i,d\parens{\alpha^{\prime}}}^{\mathcal{S}\mathcal{D}}\left(t\from t^{\prime}\right)p_{d\parens{\alpha^{\prime}}}^{\mathcal{D}}\left(t^{\prime}\right).\label{eq:SubjectFinalStandaloneProbabilitiesFromHybridIntermedDevice}
\end{equation}
The entries $\stochasticmatrix_{i,d\parens{\alpha^{\prime}}}^{\mathcal{S}\mathcal{D}}\left(t\from t^{\prime}\right)$
of the \emph{hybrid} relative transition matrix appearing here are
given explicitly by 
\begin{equation}
\eqsbrace{\begin{aligned} & \stochasticmatrix_{i,d\parens{\alpha^{\prime}}}^{\mathcal{S}\mathcal{D}}\left(t\from t^{\prime}\right)\\
 & \defeq\sum_{i_{1}^{\prime},i_{2}^{\prime}}\overconj{\timeevop_{ii_{1}^{\prime}}^{\mathcal{S}}\left(t\from t^{\prime}\right)}\timeevop_{ii_{2}^{\prime}}^{\mathcal{S}}\left(t\from t^{\prime}\right)\tilde{e}_{\alpha^{\prime},i_{2}^{\prime}}^{\mathcal{S}}\overconj{\tilde{e}_{\alpha^{\prime},i_{1}^{\prime}}^{\mathcal{S}}}.
\end{aligned}
\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace}\label{eq:SubjectDeviceHybridStochasticMatrix}
\end{equation}
 Because these matrix entries do not depend on the measuring device's
standalone probabilities $p_{d\parens{\alpha^{\prime}}}^{\mathcal{D}}\left(t^{\prime}\right)$,
they naturally serve as \emph{conditional probabilities} for the
subject system $\mathcal{S}$ to be in its $i$th configuration at
the time $t>t^{\prime}$, given that the measuring device $\mathcal{D}$
is in its configuration $d\parens{\alpha^{\prime}}$ at $t^{\prime}$:
\begin{equation}
p^{\mathcal{S}\mathcal{D}}\left(i,t\given d\parens{\alpha^{\prime}},t^{\prime}\right)\defeq\stochasticmatrix_{i,d\parens{\alpha^{\prime}}}^{\mathcal{S}\mathcal{D}}\left(t\from t^{\prime}\right).\label{eq:SubjectDeviceHybridConditionals}
\end{equation}


\subsection{Wave-Function Collapse\label{subsec:Wave-Function-Collapse}}

Importantly, notice that one can write the hybrid transition matrix
\eqref{eq:SubjectDeviceHybridStochasticMatrix} in an overall form
that resembles the dictionary \eqref{eq:DefDictionary}: 

\begin{equation}
\eqsbrace{\begin{aligned} & \stochasticmatrix_{i,d\parens{\alpha^{\prime}}}^{\mathcal{S}\mathcal{D}}\left(t\from t^{\prime}\right)\\
 & \qquad=\tr\parens{\timeevop^{\mathcal{S}\adj}\left(t\from t^{\prime}\right)\projector_{i}^{\mathcal{S}}\timeevop^{\mathcal{S}}\left(t\from t^{\prime}\right)\tilde{\projector}_{\alpha}^{\mathcal{S}}}.
\end{aligned}
\negthickspace\negthickspace\negthickspace\negthickspace}\label{eq:SubjectDeviceHybridStochasticMatrixFromDictionary}
\end{equation}
 Rearranging the right-hand side gives the equation 
\begin{equation}
\stochasticmatrix_{i,d\parens{\alpha^{\prime}}}^{\mathcal{S}\mathcal{D}}\left(t\from t^{\prime}\right)=\tr\parens{\projector_{i}^{\mathcal{S}}\densitymatrix^{\mathcal{S}\given\alpha^{\prime},t^{\prime}}\left(t\right)},\label{eq:SubjectDeviceHybridStochasticMatrixFromTraceConditionalDensityMatrix}
\end{equation}
 with a \emph{conditional} density matrix $\densitymatrix^{\mathcal{S}\given\alpha^{\prime},t^{\prime}}\left(t\right)$
for the subject system $\mathcal{S}$ at the time $t>t^{\prime}$
naturally defined by time-evolving the eigenprojector $\tilde{\projector}_{\alpha^{\prime}}^{\mathcal{S}}$
from $t^{\prime}$ to $t$: 
\begin{equation}
\densitymatrix^{\mathcal{S}\given\alpha^{\prime},t^{\prime}}\left(t\right)\defeq\timeevop^{\mathcal{S}}\left(t\from t^{\prime}\right)\tilde{\projector}_{\alpha^{\prime}}^{\mathcal{S}}\timeevop^{\mathcal{S}\adj}\left(t\from t^{\prime}\right).\label{eq:DefSubjectConditionalDensityMatrix}
\end{equation}

Thus, the calculation \eqref{eq:SubjectFinalStandaloneProbabilitiesFromHybridIntermedDeviceCalculation}
reduces to the statement that the standalone probabilities $p_{i}^{\mathcal{S}}\left(t\right)$
for the subject system at $t>t^{\prime}$ are given by 
\begin{equation}
p_{i}^{\mathcal{S}}\left(t\right)=\tr\parens{\projector_{i}^{\mathcal{S}}\densitymatrix^{\mathcal{S}}\left(t\right)},\label{eq:SubjectFinalStandaloneProbabilitiesFromTraceAfterMeasurementForCollapse}
\end{equation}
 where the subject system's density matrix $\densitymatrix^{\mathcal{S}}\left(t\right)$,
which was originally defined in \eqref{eq:SubjectFinalDensityMatrixAfterMeasurement},
can equivalently be expressed as a probabilistic mixture of the conditional
density matrices $\densitymatrix^{\mathcal{S}\given\alpha^{\prime},t^{\prime}}\left(t\right)$
defined in \eqref{eq:DefSubjectConditionalDensityMatrix}, statistically
weighted by the measurement probabilities $p_{d\parens{\alpha^{\prime}}}^{\mathcal{D}}\left(t^{\prime}\right)$:
\begin{equation}
\densitymatrix^{\mathcal{S}}\left(t\right)\defeq\sum_{\alpha^{\prime}}\densitymatrix^{\mathcal{S}\given\alpha^{\prime},t^{\prime}}\left(t\right)p_{d\parens{\alpha^{\prime}}}^{\mathcal{D}}\left(t^{\prime}\right).\label{eq:SubjectFindDensityMatrixFromIntermedAfterMeasurement}
\end{equation}

Taking stock of these results, one sees that to make future predictions
for $t>t^{\prime}$ about the subject system $\mathcal{S}$, conditioned
on the measuring device's result $d\parens{\alpha^{\prime}}$ at $t^{\prime}$,
one uses the conditional probabilities \eqref{eq:SubjectDeviceHybridStochasticMatrixFromTraceConditionalDensityMatrix},
in which the subject system's density matrix has been effectively
replaced with the conditional density matrix $\densitymatrix^{\mathcal{S}\given\alpha^{\prime},t^{\prime}}\left(t\right)$.
This conditional density matrix corresponds to a \emph{collapsed}
state vector or wave function defined as 
\begin{equation}
\Psi^{\mathcal{S}\given\alpha^{\prime},t^{\prime}}\left(t\right)\defeq\timeevop\left(t\from t^{\prime}\right)\tilde{e}_{\alpha}^{\mathcal{S}}.\label{eq:SubjectWaveFunctionCollapse}
\end{equation}
 The phenomenon of \emph{wave-function collapse} therefore reduces
to a prosaic example of conditioning, a conclusion that represents
another new result.

By contrast, for an observer who does not know the specific measurement
result $d\parens{\alpha^{\prime}}$, the correct density matrix $\densitymatrix^{\mathcal{S}}\left(t\right)$
to use is the one defined in \eqref{eq:SubjectFindDensityMatrixFromIntermedAfterMeasurement}.
Again, this density matrix consists of an appropriate probabilistic
mixture of conditional or collapsed density matrices that are statistically
weighted over the measurement results.

\subsection{The Measurement Problem\label{subsec:The-Measurement-Problem}}

According to the foregoing treatment of the measurement process, a
measuring device is an ordinary physical system that can carry out
a measurement of an observable, and then ends up in a final configuration
that reflects a definite measurement outcome. The probabilities for
a measuring device's various possible measurement outcomes are given
by the textbook Born rule \eqref{eq:DeviceBornRule}, and conditioning
on the specific measurement outcome leads to the textbook formula
\eqref{eq:SubjectWaveFunctionCollapse} for wave-function collapse.
Hence, the picture of quantum theory presented in this paper arguably
has the resources to solve the measurement problem~\citep{Myrvold:2022piiqt}. 

The stochastic-quantum correspondence is also helpful for understanding
the measurement process in another important way. Textbook treatments
of quantum theory typically regard measuring devices as axiomatic
 primitives or posits, without providing clear principles for deciding
which kinds of systems merit being called measuring devices and which
do not. The approach taken toward the measurement process in this
paper not only gives a candidate resolution of the measurement problem,
but also yields a natural set of criteria for defining what counts
as a good measuring device in the first place, without the need to
regard measuring devices as special among all other systems in any
truly fundamental way.

Based on the approach in this paper, one sees that a good measuring
device should be a physical system with at least as many configurations
as possible outcomes for the observable to be measured (at least up
to the desired level of experimental resolution), it should admit
an overall form of dynamics that results in the correct final correlations,
and it should be in sufficiently strong contact with a noisy and intrusive
environment to generate a robust division event at the conclusion
of the measurement interaction. It is worth noting that the first
two of these three criteria would be standard requirements for a measuring
device even without worrying about indivisible stochastic dynamics
or quantum theory.\footnote{Without the third criterion\textemdash strong contact with an environment\textemdash one
obtains a ``latent measurement'' in the language of~\citep{Dicke:1989qmsal,GlickAdami:2020manmqm}.}

\subsection{The Uncertainty Principle\label{subsec:The-Uncertainty-Principle}}

Again, the foregoing treatment of the measurement process leads to
the textbook Born rule \eqref{eq:DeviceBornRule} and the textbook
formula \eqref{eq:SubjectWaveFunctionCollapse} for wave-function
collapse. As a consequence, any pair of observables $\tilde{A},\tilde{B}$
and their respective standard deviations $\Delta\tilde{A},\Delta\tilde{B}$
will satisfy the \emph{Heisenberg-Robertson uncertainty principle}~\citep{Heisenberg:1927udaidqkum,Robertson:1929tup},
\begin{equation}
\Delta\tilde{A}\,\Delta\tilde{B}\geq\frac{1}{2}\verts{\tr\parens{i\bracks{\tilde{A}\tilde{B}-\tilde{B}\tilde{A}}\densitymatrix}},\label{eq:HeisenbergRobertsonUncertaintyPrinciple}
\end{equation}
 as follows from any of the standard proofs.\footnote{\label{fn:UncertaintyPrincipleProof}Here is one proof, which is adapted
from~\citep{Stueckelberg:1960qtirhs}: Because density matrices are
positive semidefinite, it follows from the spectral theorem that for
any $N\times N$ matrix $X$ and any $N\times N$ density matrix $\densitymatrix$,
one has the inequality $\tr\parens{X^{\adj}X\densitymatrix}\geq0$.
Let $\tilde{A},\tilde{B}$ be any pair of observables, assumed without
any real loss of generality to have vanishing expectation values $\angs{\tilde{A}}=\angs{\tilde{B}}=0$.
Let $x$ be a variable real number and let $X\defeq x\tilde{A}+i\tilde{B}$.
Then $\tr\parens{X^{\adj}X\densitymatrix}\geq0$ becomes the inequality
$f\left(x\right)\geq0$, where $f\left(x\right)\defeq ax^{2}+bx+c$
is a quadratic function with coefficients $a\defeq\parens{\Delta\tilde{A}}^{2}$,
$b\defeq\tr\parens{i\bracks{\tilde{A}\tilde{B}-\tilde{B}\tilde{A}}\densitymatrix}$,
and $c\defeq\parens{\Delta\tilde{B}}^{2}$. The minimum value of $f\left(x\right)$
is $-b^{2}/4a+c$, so one arrives at the inequality $-b^{2}/4a+c\geq0$,
which one can rearrange to give the Heisenberg-Robertson uncertainty
principle.~QED}

The stochastic-quantum correspondence goes beyond replicating the
uncertainty principle by painting a clearer picture of what the uncertainty
principle physically means. Consider for simplicity the case in which
$\tilde{A}=A$ is a random variable, or beable, and $\tilde{B}$ is
an emergeable, in the language of Subsection~\ref{subsec:Emergeables}.
Then $A$ has a direct interpretation solely in terms of the subject
system's configuration space, whereas $\tilde{B}$ encodes an emergent
pattern in the subject system's dynamics that can nonetheless show
up in the measurement outcomes of a measuring device.

Suppose that $A$ has a definite value or magnitude at some initial
time $0$. Then the subject system must be in a specific configuration
with probability $1$ at the initial time $0$. The overall stochastic
dynamics will then lead to uncertainty in the outcome of any measurement
of $\tilde{B}$.

Suppose that one goes ahead and measures $\tilde{B}$, so that a definite
measurement outcome emergently shows up in the configuration of a
measuring device at some time $t^{\prime}>0$. Then the analysis in
Subsection~\ref{subsec:Wave-Function-Collapse} implies that there
is an inevitable disturbance in the subject system that leads its
density matrix to end up effectively as a non-diagonal matrix equal
to an eigenprojector of $\tilde{B}$. A non-diagonal density matrix
signifies  that the system is in the midst of an indivisible stochastic
process, as explained in Subsection~\ref{subsec:Decoherence}. In
the present circumstances, that indivisible stochastic process is
precisely one that would ensure that if $\tilde{B}$ were measured
again shortly after $t^{\prime}$, then the measuring device would
obtain the same outcome for $\tilde{B}$ as before. However, being
in the s of an indivisible stochastic process also implies uncertainty
in the subject system's underlying configuration, thereby rendering
the value of $A$ uncertain.

\section{Further Implications\label{sec:Further-Implications}}

\subsection{Symmetries\label{subsec:Symmetries}}

The stochastic-quantum correspondence developed in this paper provides
new ways to think about \emph{dynamical symmetries} in quantum theory,
meaning transformations that leave the dynamics invariant. Going in
the other direction, the stochastic-quantum correspondence also makes
it more straightforward to impose dynamical symmetries systematically
as constraints in the construction of the dynamics for a given generalized
stochastic system.

Classically, any invertible transformation of a system's configurations
$i=1,\dots,N$ is a permutation transformation of the configuration
projectors \eqref{eq:DefConfigurationProjectors}: 
\begin{equation}
\eqsbrace{\begin{aligned} & \projector_{i}\mapsto\projector_{\sigma\left(i\right)},\\
 & \quad\textrm{with }\set{\sigma\left(1\right),\dots,\sigma\left(N\right)}=\set{1,\dots,N}.
\end{aligned}
}\label{eq:ClassicalSymmetryTransformationAsPermutation}
\end{equation}
 More generally, a transformation between two PVMs $\projector_{1},\dots,\projector_{N}$
and $\tilde{\projector}_{1},\dots,\tilde{\projector}_{N}$ is always
a similarity transformation of the form 
\begin{equation}
\projector_{i}\mapsto\tilde{\projector}_{i}\defeq V^{\adj}\projector_{i}V,\label{eq:DefGeneralSymmetryTransformation}
\end{equation}
 where $V$ is some unitary operator.\footnote{Proof: Let $e_{1},\dots,e_{N}$ be the orthonormal configuration basis
\eqref{eq:DefConfigurationBasis}, with $e_{i}^{\adj}e_{j}=\delta_{ij}$
and $e_{i}e_{i}^{\adj}=\projector_{i}$ as in \eqref{eq:ConfigurationBasisOrthonormalComplete},
and let $\tilde{e}_{1},\dots,\tilde{e}_{N}$ be an orthonormal basis
related to the new projectors $\tilde{\projector}_{i}$ in the analogous
way, with $\tilde{e}_{i}^{\adj}\tilde{e}_{j}=\delta_{ij}$ and $\tilde{e}_{i}\tilde{e}_{i}^{\adj}=\tilde{\projector}_{i}$.
Then the $N\times N$ matrix defined by $V\defeq\sum_{i}e_{i}\tilde{e}_{i}^{\adj}$
is unitary and satisfies $V^{\adj}\projector_{i}V=\tilde{\projector}_{i}$.
Going the other way, if $V$ is an $N\times N$ unitary matrix, then
the $N\times N$ matrices defined for $i=1,\dots,N$ by $\tilde{\projector}_{i}\defeq V^{\adj}\projector_{i}V$
are guaranteed to constitute a PVM.~QED} This similarity transformation reduces to the configurational transformation
\eqref{eq:ClassicalSymmetryTransformationAsPermutation} if and only
if $V$ is a permutation matrix.

The more general transformation \eqref{eq:DefGeneralSymmetryTransformation}
is a dynamical symmetry, meaning that it leaves the stochastic dynamics
invariant, precisely if the right-hand side of the stochastic-quantum
dictionary \eqref{eq:DefDictionary} remains unchanged: 
\begin{equation}
\tr\parens{\dynop^{\adj}\left(t\right)\tilde{\projector}_{i}\dynop\left(t\right)\tilde{\projector}_{j}}=\tr\parens{\dynop^{\adj}\left(t\right)\projector_{i}\dynop\left(t\right)\projector_{j}}.\label{eq:DynamicalSymmetryDictionaryTransformedProjectors}
\end{equation}
 This condition is equivalent to the statement that 
\begin{equation}
\tr\parens{\tilde{\dynop}^{\adj}\left(t\right)\projector_{i}\tilde{\dynop}\left(t\right)\projector_{j}}=\tr\parens{\dynop^{\adj}\left(t\right)\projector_{i}\dynop\left(t\right)\projector_{j}},\label{eq:DynamicalSymmetryDictionaryTransformedTimeEvOp}
\end{equation}
 where 
\begin{equation}
\tilde{\dynop}\left(t\right)\defeq V\dynop\left(t\right)V^{\adj}.\label{eq:SymmetryTransformedTimeEvOp}
\end{equation}
 Re-expressing both sides of the equivalent condition \eqref{eq:DynamicalSymmetryDictionaryTransformedTimeEvOp}
in terms of squared absolute values, as in \eqref{eq:StochasticMatrixEntryFromAbsValSquare},
one sees that \eqref{eq:SymmetryTransformedTimeEvOp} is a dynamical
symmetry precisely if 
\begin{equation}
\verts{\tilde{\dynop}_{ij}\left(t\right)}^{2}=\verts{\dynop_{ij}\left(t\right)}^{2}.\label{eq:DynamicalSymmetryTransfAbsValSqCondition}
\end{equation}

It follows immediately that $\tilde{\dynop}\left(t\right)$ can differ
from $\dynop\left(t\right)$ by at most a Schur-Hadamard gauge transformation
\eqref{eq:DefLocalInTimeSchurHadamardGaugeTransformation}, so a necessary
and sufficient condition for a unitary matrix $V$ to give a dynamical
symmetry is that 
\begin{equation}
V\dynop\left(t\right)V^{\adj}=\dynop\left(t\right)\hadamardprod\begin{pmatrix}e^{i\theta_{11}\left(t\right)} & e^{i\theta_{12}\left(t\right)}\\
e^{i\theta_{21}\left(t\right)} & \ddots\\
 &  & e^{i\theta_{NN}\left(t\right)}
\end{pmatrix}.\label{eq:DynamicalSymmetryAsLocalInTimeEntrywisePhaseTransformation}
\end{equation}
 As special cases, this condition includes \emph{unitary} dynamical
symmetries, 
\begin{equation}
V\dynop\left(t\right)V^{\adj}=\dynop\left(t\right),\label{eq:UnitaryDynamicalSymmetr}
\end{equation}
 as well as \emph{anti-unitary} dynamical symmetries, 
\begin{equation}
V\dynop\left(t\right)V^{\adj}=\overconj{\dynop\left(t\right)},\label{eq:AntiUnitaryDynamicalSymmetry}
\end{equation}
 but \eqref{eq:DynamicalSymmetryAsLocalInTimeEntrywisePhaseTransformation}
may also open up the possible existence of dynamical symmetries distinct
from these two cases.

For the specific case of an anti-unitary dynamical symmetry, note
that if one redefines $V\mapsto\overconj V$, which is still unitary,
then one can re-express \eqref{eq:AntiUnitaryDynamicalSymmetry} in
the somewhat more conventional form 
\begin{equation}
VK\dynop\left(t\right)KV^{\adj}=\dynop\left(t\right).\label{eq:AntiUnitaryDynamicalSymmetryUsingComplexConjOp}
\end{equation}
 Here $K$ denotes the \emph{complex-conjugation operator}, meaning
that $K$ is an \emph{involution}, 
\begin{equation}
K^{2}=1,\label{eq:ComplexConjugationOperatorInvolution}
\end{equation}
 and, for any $N\times N$ matrix $X$, one has 
\begin{equation}
KXK=\overconj X.\label{eq:ComplexConjugationOperatorOnMatrix}
\end{equation}
 The composite operator $VK$ as a whole is then said to be an \emph{anti-unitary operator}.
Anti-unitary operators play an important role in describing \emph{time-reversal symmetries}.\footnote{Intriguingly, because $K$ anticommutes with $i$, meaning that $Ki=-iK$,
the three mathematical objects $i$, $K$, and $iK$ satisfy $-i^{2}=K^{2}=\left(iK\right)^{2}=iK\left(iK\right)=1$,
and therefore generate a Clifford algebra isomorphic to the \emph{pseudo-quaternions}~\citep{Stueckelberg:1960qtirhs}.
In a sense, then, the Hilbert spaces of quantum systems are actually
defined not over the complex numbers alone, but over the pseudo-quaternions. }

If $\dynop\left(t\right)=\timeevop\left(t\right)$ is unitary, then
$V\dynop\left(t\right)V^{\adj}$ will likewise be unitary. In that
case, suppose either that $V$ is continuously connected to the identity
matrix $\idmatrix$ by some smooth parameter, with a corresponding
self-adjoint generator $G=G^{\adj}$, or, alternatively, that $V$
is an involution, meaning that $V^{2}=\idmatrix$, in which case $G\defeq V=V^{\adj}$
is \emph{itself} self-adjoint. Either way, the expectation value $\expectval{G\left(t\right)}$
is a physically meaningful quantity, and \emph{Noether's theorem}
easily follows as the statement that this expectation value is constant
in time, or \emph{conserved}: 
\begin{equation}
\expectval{G\left(t\right)}=\tr\parens{G\timeevop\left(t\right)\densitymatrix\left(0\right)\timeevop^{\adj}\left(t\right)}=\expectval{G\left(0\right)}.\label{eq:NoethersTheorem}
\end{equation}


\subsection{Dilations\label{subsec:Dilations}}

In most textbook treatments of quantum theory, a quantum system is
axiomatically defined as a particular Hilbert space together with
a preferred set of self-adjoint operators designated as observables
with predetermined physical meanings, along with a particular Hamiltonian
to define the system's time evolution.\footnote{In some circumstances, it may turn out to be more convenient to define
a quantum system by a formal C{*}-algebra of observables alone, without
picking a specific Hilbert-space representation~\citep{Haag:1993lqpfpa,Strocchi:2008aittmsoqm,Feintzeig:2016oamiqt}.} From that point of view, modifying a system's Hilbert-space formulation
in any nontrivial way would necessarily mean fundamentally modifying
the system itself.

From the alternative point of view developed in this paper, by contrast,
a Hilbert-space formulation is merely a collection of mathematical
tools for constructing the dynamics of a given generalized stochastic
system. The generalized stochastic system itself is ultimately defined
by a configuration space and a dynamical law that stand apart from
any arbitrary choice of Hilbert-space formulation. As a consequence,
one is free to modify a generalized stochastic system's Hilbert-space
formulation as needed, much like changing from one gauge choice to
another in a gauge theory, or like adding physically meaningless variables
to the Lagrangian formulation of a deterministic classical system.

With this motivation in place, recall again the basic stochastic-quantum
dictionary \eqref{eq:DefDictionary}: 
\begin{equation}
\stochasticmatrix_{ij}\left(t\right)=\tr\parens{\dynop^{\adj}\left(t\right)\projector_{i}\dynop\left(t\right)\projector_{j}}.\label{eq:DefDictionaryForDilations}
\end{equation}
 The Hilbert-space formulation expressed by the right-hand side can
be manipulated for convenience, provided that the left-hand side of
the dictionary remains unchanged.

In particular, for any integer $D\geq2$, one can freely enlarge,
or \emph{dilate}, the Hilbert-space formulation to a larger dimension
$ND$ by the following \emph{dilation transformation}: 
\begin{equation}
\eqsbrace{\begin{aligned}\dynop\left(t\right) & \mapsto\dynop\left(t\right)\tensorprod\idmatrix^{\mathcal{I}},\\
\projector_{i}\left(t\right) & \mapsto\projector_{i}\left(t\right)\tensorprod\idmatrix^{\mathcal{I}},\\
\projector_{j}\left(t\right) & \mapsto\projector_{j}\left(t\right)\tensorprod\projector_{\gamma}^{\mathcal{I}}.
\end{aligned}
}\label{eq:DefDilation}
\end{equation}
 Here $\idmatrix^{\mathcal{I}}$ is the $D\times D$ identity matrix
on a new \emph{internal} Hilbert space $\hilbspace_{\mathcal{I}}$,
and $\projector_{1}^{\mathcal{I}},\dots,\projector_{D}^{\mathcal{I}}$
collectively form any PVM on that internal Hilbert space satisfying
the usual conditions of mutual exclusivity, 
\begin{equation}
\projector_{\gamma}^{\mathcal{I}}\projector_{\gamma^{\prime}}^{\mathcal{I}}=\delta_{\gamma\gamma^{\prime}}\projector_{\gamma}^{\mathcal{I}},\label{eq:InternalProjectorsMutuallyExclusive}
\end{equation}
 and completeness, 
\begin{equation}
\sum_{\gamma=1}^{D}\projector_{\gamma}^{\mathcal{I}}=\idmatrix^{\mathcal{I}}.\label{eq:InternalProjectorsComplete}
\end{equation}
 It is then a mathematical identity that one can rewrite the stochastic-quantum
dictionary \eqref{eq:DefDictionary} as 
\begin{equation}
\eqsbrace{\begin{aligned}\stochasticmatrix_{ij}\left(t\right) & =\tr\left(\tr_{\mathcal{I}}\left(\left[\dynop^{\adj}\left(t\right)\tensorprod\idmatrix^{\mathcal{I}}\right]\left[\projector_{i}\tensorprod\idmatrix^{\mathcal{I}}\right]\right.\right.\\
 & \qquad\times\left.\left.\left[\dynop\left(t\right)\tensorprod\idmatrix^{\mathcal{I}}\right]\left[\projector_{j}\tensorprod\projector_{\gamma}^{\mathcal{I}}\right]\right)\right),
\end{aligned}
\negthickspace\negthickspace\negthickspace\negthickspace}\label{eq:DilatedDictionary}
\end{equation}
 with a second trace, or \emph{partial trace}, over the internal
Hilbert space $\hilbspace_{\mathcal{I}}$. The choice of value for
the label $\gamma$ here is immaterial, with different choices of
$\gamma$ related by gauge transformations.

One can equivalently write the dilated form \eqref{eq:DilatedDictionary}
of the dictionary in \emph{block-matrix form} as 
\begin{equation}
\stochasticmatrix_{ij}\left(t\right)=\tr_{\mathcal{I}}\left(\left[\dynop_{ij}\left(t\right)\right]^{\mathcal{I}\adj}\left[\dynop_{ij}\left(t\right)\right]^{\mathcal{I}}\projector_{\gamma}^{\mathcal{I}}\right).\label{eq:DilationStochasticMatrixDictionaryBlockForm}
\end{equation}
 Here $\left[\dynop_{ij}\left(t\right)\right]^{\mathcal{I}}$ is
a diagonal $D\times D$ matrix consisting of repeated copies of the
specific entry $\dynop_{ij}\left(t\right)$ (for fixed $i,j$) along
the diagonal: 
\begin{equation}
\left[\dynop_{ij}\left(t\right)\right]^{\mathcal{I}}\defeq\dynop_{ij}\left(t\right)\,\idmatrix^{\mathcal{I}}.\label{eq:DefDilatedBlockTimeEvOp}
\end{equation}
 Meanwhile, the adjoint operation $\adj$ in \eqref{eq:DilationStochasticMatrixDictionaryBlockForm}
acts on this $D\times D$ block matrix $\left[\dynop_{ij}\left(t\right)\right]^{\mathcal{I}}$,
so it does not transpose the indices $i$ and $j$ on the $N\times N$
matrix $\dynop_{ij}\left(t\right)$ itself: 
\begin{equation}
\left[\dynop_{ij}\left(t\right)\right]^{\mathcal{I}\adj}\defeq\overconj{\left[\dynop_{ij}\left(t\right)\right]}^{\mathcal{I}}.\label{eq:DefAdjointDilatedBlockTimeEvOp}
\end{equation}
 It follows that 
\begin{equation}
\left[\dynop_{ij}\left(t\right)\right]^{\mathcal{I}\adj}\left[\dynop_{ij}\left(t\right)\right]^{\mathcal{I}}\projector_{\gamma}^{\mathcal{I}}=\verts{\dynop_{ij}\left(t\right)}^{2}\projector_{\gamma}^{\mathcal{I}},\label{eq:CalculationBlockMatrixForDilatedDictionary}
\end{equation}
 so the trace over $\hilbspace_{\mathcal{I}}$ indeed yields $\verts{\dynop_{ij}\left(t\right)}^{2}=\stochasticmatrix_{ij}\left(t\right)$,
as required.

In this dilated version of the Hilbert-space formulation, Schur-Hadamard
gauge transformations \eqref{eq:DefLocalInTimeSchurHadamardGaugeTransformation}
are enhanced to the following local-in-time gauge transformations,
which have not yet been described in the research literature and therefore
constitute another new result: 
\begin{equation}
\left[\dynop_{ij}\left(t\right)\right]^{\mathcal{I}}\mapsto V_{\left(ij\right)}^{\mathcal{I}}\left(t\right)\left[\dynop_{ij}\left(t\right)\right]^{\mathcal{I}}.\label{eq:LocalInTimeBlockwiseUnitaryTransformations}
\end{equation}
 Here $V_{\left(ij\right)}^{\mathcal{I}}\left(t\right)$ are a set
of $N^{2}$ unitary, $D\times D$ matrices, where each such unitary
matrix as a whole is labeled by a specific pair $\left(ij\right)$
of configuration labels: 
\begin{equation}
V_{\left(ij\right)}^{\mathcal{I}\adj}\left(t\right)=\parens{V_{\left(ij\right)}^{\mathcal{I}}\left(t\right)}^{-1}.\label{eq:LocalInTimeBlockwiseUnitaryMatrices}
\end{equation}

The gauge transformations \eqref{eq:LocalInTimeBlockwiseUnitaryTransformations}
will not generally preserve the factorization $\dynop\left(t\right)\tensorprod\idmatrix^{\mathcal{I}}$
appearing in \eqref{eq:DilatedDictionary}, so they motivate considering
more general $ND\times ND$ time-evolution operators $\tilde{\dynop}\left(t\right)$,
in terms of which the dilated dictionary \eqref{eq:DilatedDictionary}
takes the form 
\begin{equation}
\stochasticmatrix_{ij}\left(t\right)=\tr\left(\tr_{\mathcal{I}}\left(\tilde{\dynop}^{\adj}\left(t\right)\left[\projector_{i}\tensorprod\idmatrix^{\mathcal{I}}\right]\tilde{\dynop}\left(t\right)\left[\projector_{j}\tensorprod\projector_{\gamma}^{\mathcal{I}}\right]\right)\right).\label{eq:DilatedDictionaryGenericTimeEvOp}
\end{equation}
 Any $ND\times ND$ matrix $\tilde{\dynop}\left(t\right)$ appearing
on the right-hand side of this dictionary and satisfying the natural
generalization of the summation condition \eqref{eq:SumTimeEvOpAbsValSqEq1}
is guaranteed to lead to a valid transition matrix $\stochasticmatrix_{ij}\left(t\right)$
on the left-hand side, so working with a dilated Hilbert-space formulation
essentially provides a larger \textquoteleft canvas\textquoteright{}
for designing transition matrices.

As a simple example of a dilation for the case $D=2$, one can formally
eliminate the complex numbers from a quantum system's Hilbert space~\citep{Myrheim:1999qmoarhs}.
Specifically, by increasing the system's Hilbert-space dimension from
$N$ to $2N$, one can replace the imaginary unit $i\defeq\sqrt{-1}$
with the real-valued $2\times2$ matrix $\left(\begin{smallmatrix}0 & -1\\
1 & 0
\end{smallmatrix}\right)$, with the enhanced version \eqref{eq:LocalInTimeBlockwiseUnitaryTransformations}
of Schur-Hadamard gauge transformations now consisting of two-dimensional
rotations of the internal Hilbert space $\hilbspace_{\mathcal{I}}$.\footnote{Importantly, notice that the proof of the uncertainty principle presented
in Footnote~\ref{fn:UncertaintyPrincipleProof} works just as well
with the imaginary unit $i$ represented by a $2\times2$ matrix in
this way.} One can then represent the complex-conjugation operator $K$ appearing
in \eqref{eq:AntiUnitaryDynamicalSymmetryUsingComplexConjOp} as the
real-valued $2\times2$ matrix $\left(\begin{smallmatrix}0 & 1\\
1 & 0
\end{smallmatrix}\right)$. The result is that all unitary and anti-unitary operators become
$2N\times2N$ real orthogonal matrices. One cost of using this \textquoteleft real\textquoteright{}
representation, however, is that the Hilbert spaces of composite systems
will not factorize as neatly into Hilbert spaces for their constituent
subsystems.\footnote{Without increasing the dimension $N$ of a system's Hilbert space,
one could instead attempt to limit the appearance of the complex numbers
in a system's Hilbert-space formulation by using the original Schur-Hadamard
gauge transformation \eqref{eq:DefLocalInTimeSchurHadamardGaugeTransformation}
to make all the entries of the system's time-evolution operator $\dynop\left(t\right)$
real-valued. In this alternative approach, however, a unistochastic
transition matrix $\stochasticmatrix\left(t\right)$ may not be expressible
in terms of a unitary or orthogonal time-evolution operator, and the
complex numbers will generally still be needed anyway to define various
observables.}

As a much more significant application of dilations, recall that any
transition matrix $\stochasticmatrix_{ij}\left(t\right)$ has a Kraus
decomposition \eqref{eq:DictionaryFromKrausDecomposition}: 
\begin{equation}
\stochasticmatrix_{ij}\left(t\right)=\sum_{\beta=1}^{N}\tr\parens{\krausmatrix_{\beta}^{\adj}\left(t\right)\projector_{i}\krausmatrix_{\beta}\left(t\right)\projector_{j}}.\label{eq:DictionaryFromKrausDecompositionForDilation}
\end{equation}
 The \emph{Stinespring dilation theorem}~\citep{Stinespring:1955pfoc,Keyl:2002foqit}
then guarantees that by an appropriate dilation to a larger Hilbert
space if necessary, one can express $\stochasticmatrix_{ij}\left(t\right)$
in terms of a unitary time-evolution operator $\tilde{\timeevop}\left(t\right)$:
\begin{equation}
\stochasticmatrix_{ij}\left(t\right)=\tr\left(\tr_{\mathcal{I}}\left(\tilde{\timeevop}^{\adj}\left(t\right)\left[\projector_{i}\tensorprod\idmatrix^{\mathcal{I}}\right]\tilde{\timeevop}\left(t\right)\left[\projector_{j}\tensorprod\projector_{\gamma}^{\mathcal{I}}\right]\right)\right).\label{eq:DictionaryAfterDilation}
\end{equation}
 This fact makes clear the inevitability of unitary time evolution
in quantum theory. As mentioned earlier, it also implies at the level
of a theorem~\citep{Barandes:2023tsqt} that every generalized stochastic
system can be regarded as a subsystem of a unistochastic system.\footnote{From the starting assumptions presented here, one can sketch the following
proof: Given $N\times N$ Kraus operators $\krausmatrix_{\beta}\left(t\right)$,
with $\beta=1,\dots,N$, define an $N^{3}\times N^{2}$ matrix $\tilde{V}\left(t\right)$
according to $\tilde{V}_{\left(i\beta m\right)\left(jl\right)}\left(t\right)\defeq\krausmatrix_{\beta,ij}\left(t\right)\delta_{lm}$,
treating $\left(i\beta m\right)$ as the first index of $\tilde{V}\left(t\right)$
and treating $\left(jl\right)$ as its second index. One can show
that this matrix satisfies $\tilde{V}^{\adj}\left(t\right)\tilde{V}\left(t\right)=\idmatrix_{N^{2}\times N^{2}}$,
so it defines a partial isometry, which can always be extended to
a unitary $N^{3}\times N^{3}$ matrix $\tilde{\timeevop}_{\left(i\beta m\right)\left(ja\right)}\left(t\right)$
by adding $N^{3}-N^{2}$ additional columns that are mutually orthogonal
with each other and with the previous $N^{2}$ columns already in
$\tilde{V}\left(t\right)$, where the new index $a$ runs through
$N^{2}$ possible values. These additional columns can always be chosen
so that at the initial time $0$, where $V\left(0\right)=\idmatrix$
is the $N\times N$ identity matrix, they make $\tilde{\timeevop}\left(0\right)$
coincide with the $N^{3}\times N^{3}$ identity matrix. The last step
is to show that $\tilde{\timeevop}\left(t\right)$ satisfies \eqref{eq:DictionaryAfterDilation},
whose right-hand side reduces to $\sum_{\beta,m}\verts{\tilde{\timeevop}_{\left(i\beta m\right)\left(j\gamma\right)}\left(t\right)}^{2}=\sum_{\beta}\verts{\krausmatrix_{\beta,ij}\left(t\right)}^{2}$.~QED}

As yet another key application of dilations, a dilated Hilbert-space
formulation can make it possible to describe new kinds of emergeables.
Some of these \emph{dilation-emergeables} may be observables that
can yield definite results in measurement processes, along the lines
of Subsection~\ref{subsec:The-Measurement-Process}, despite not
having a direct meaning solely at the level of the system's underlying
configuration space.

In this way, a generalized stochastic system based on a configuration
space can easily accommodate emergent observables that describe empirically
meaningful patterns in the dynamics and that model all kinds of quantum
phenomena. Indeed, obtaining a unitary time-evolution operator for
a given system may require dilating the Hilbert space in just this
way, as in \eqref{eq:DictionaryAfterDilation}.

It is important to keep in mind that whether or not one actually carries
out this formal dilation of the Hilbert-space formulation, the stochastic
dynamics of the underlying generalized stochastic system will still
be the same. Any emergent patterns in the system's stochastic dynamics
that are made manifest by the dilation, as represented by any new
dilation-emergeables that arise, were always there all along, albeit
in a non-manifest way.

An important example of this last application is \emph{intrinsic spin}.
To introduce spin as a dilation-emergeable, one merely dilates the
Hilbert space to $ND$ dimensions, introduces a $D$-dimensional representation
of $SO\left(3\right)$ for the internal Hilbert space, and then requires
that the dilated time-evolution operator has the appropriate form
of rotation symmetry. This approach to representing spin ensures that
despite picking an arbitrary three-dimensional coordinate axis in
the process of formally carrying out the dilation of the Hilbert space\textemdash such
as by choosing the spin-$z$ operator to be diagonal on the dilated
Hilbert space\textemdash the underlying generalized stochastic system
does not fundamentally involve any preferred direction or entail any
basic violation of rotation invariance.

\subsection{Nonlocality\label{subsec:Nonlocality}}

This paper has shown that systems based on trajectories in configuration
spaces and evolving according to generically indivisible stochastic
dynamics have Hilbert-space representations and can replicate the
usual mathematical formalism and empirical predictions of quantum
theory.

Technically speaking, the configurations in this new picture for quantum
theory play the role of \emph{hidden variables}, meaning physical
parameters that exist separately from wave functions and density matrices.
Of course, one could argue that configurations should more properly
be called\emph{ }\emph{physical variables}, given that wave functions
and density matrices arise from the stochastic-quantum correspondence
merely as secondary, representational constructs, rather than as fundamental
entities in their own right. Either way, any mention of \textquoteleft hidden
variables\textquoteright{} immediately raises questions about the
potential invocation of \emph{nonlocality}, the study of which has
motivated famous papers like that of Einstein, Podolsky, and Rosen~\citep{EinsteinPodolskyRosen:1935cqmdprbcc},
and has led to the development of a number of important \emph{no-go theorems}~\citep{Bell:1964oeprp,ClauserHorneShimonyHolt:1969pettlhvt,Bell:1975ttolb,GreenbergerHorneZeilinger:1989gbbt}.

Before assessing the implications of these no-go theorems for the
picture described in this paper, it will be important to note that
these theorems do not rule out the possibility of hidden variables
altogether. Nor do these theorems imply that introducing hidden variables
would necessarily make quantum theory any more dynamically nonlocal
than it already is.

Being mindful of these caveats, there is ample reason to probe the
question of nonlocal dynamics in the approach to quantum theory taken
in this paper. After all, looking back at the discussion of entanglement
in Subsection~\ref{subsec:Entanglement}, a pair of systems that
interact at some time will generically exhibit what look like nonlocal
stochastic dynamics after that time, at least until the later occurrence
of a division event due to decoherence by an external system.

In what follows, it will be important to be keep in mind the distinction
between \emph{deterministic} hidden-variables theories and \emph{stochastic}
hidden-variables theories.

Bell's original nonlocality theorem, as formulated and proved in 1964~\citep{Bell:1964oeprp},
only addressed the case of a \emph{deterministic} hidden-variables
theory. Specifically, Bell showed that if one assumes that a theory's
hidden variables uniquely determine measurement outcomes, and if one
further assumes that the hidden variables are local in the sense that
measurement results should not depend on the settings of faraway measuring
devices, then one arrives at an inequality that is expressly violated
by quantum theory. Bell's 1964 theorem therefore establishes that
any purported formulation of quantum theory based on local deterministic
hidden variables is ruled out empirically.

At first glance, there might have seemed to be just two available
options in response to Bell's nonlocality theorem. Either one could
accept a theory of \emph{nonlocal} deterministic hidden variables,
or one could \emph{deny} the existence of nonlocal deterministic hidden
variables and thereby try to avoid having to introduce any ostensibly
new dynamical nonlocality into quantum theory.

However, for a hidden-variables theory based on \emph{stochastic}
dynamics rather than on \emph{deterministic} dynamics, the question
of dynamical nonlocality becomes murkier. The generalization to stochastic
dynamics means that one needs to rely on more abstract, statistical
conditions for establishing whether or not the theory's hidden variables
behave in a dynamically local manner.

The most frequently cited statistical locality criterion for stochastic
hidden-variables theories was formulated by Bell later on, in 1975~\citep{Bell:1975ttolb,Butterfield:1992btwit,MyrvoldGenoveseShimony:2021bst}.
That statistical locality criterion is a statement about how rich
a theory's hidden variables should be in order for the theory to qualify
as dynamically local.

To formulate Bell's statistical locality criterion, one starts by
considering the case of a measurement outcome $x$ based on local
measurement settings $a$, and a far-separated measurement outcome
$y$ based on local measurement settings $b$. Then one supposes that
the joint probabilities $p\left(x,y\given a,b\right)$ for the measurement
results $x$ and $y$, conditioned on the measurement settings $a$
and $b$, show a statistical correlation. Bell argued that in order
for the theory in question to be considered dynamically local, the
theory should contain enough hidden variables to account for the statistical
correlation in the following precise sense: if one conditions on all
the hidden variables $\lambda$ in the past light cone of the two
measurements, then those hidden variables should screen off the correlation
between the measurement results, meaning that the joint probabilities
should factorize according to 
\begin{equation}
p\left(x,y\given a,b,\lambda\right)=p\left(x\given a,\lambda\right)p\left(y\given b,\lambda\right).\label{eq:BellLocalityFactorizationCriterion}
\end{equation}

Bell's statistical locality criterion is precisely the condition that
the theory in question should have enough hidden variables to ensure
that the factorization \eqref{eq:BellLocalityFactorizationCriterion}
is always possible. Based on this statistical locality criterion,
which should hold even in cases of \textquoteleft one-shot\textquoteright{}
measurements in which certain measurement outcomes can be assigned
a 100\% probability~\citep{GreenbergerHorneZeilinger:1989gbbt},
one can again derive predictions that are violated by quantum theory,
just as in the case of a deterministic hidden-variables theory.

However, Bell's statistical locality criterion is broader than the
conditions he studied in his 1964 theorem on deterministic hidden-variables
theories. Bell's statistical locality condition is so broad, in fact,
that Bell used it to argue that textbook quantum theory is \emph{itself}
already dynamically nonlocal~\citep{Bell:1975ttolb,Bell:1990lnc}.

To understand why, observe that textbook quantum theory is committed
to the existence of measurement settings and definite measurement
outcomes that end up behaving precisely as a (highly incomplete) set
of stochastically evolving hidden variables. In other words, although
textbook quantum theory is not a \emph{deterministic} hidden-variables
theory, it is, in fact, a \emph{stochastic} hidden-variables theory.

The stochastic-quantum correspondence derived in this paper makes
these commitments by textbook quantum theory manifest. Indeed, one
can regard textbook quantum theory as the insistence that for any
measurement set-up consisting of a subject system $\mathcal{S}$,
a measuring device $\mathcal{D}$, and an environment $\mathcal{E}$,
as laid out in Subsection~\ref{subsec:The-Measurement-Process},
the configurations of $\mathcal{D}$ are to be treated as hidden variables
(that is, as beables), whereas the configurations of $\mathcal{S}$
and $\mathcal{E}$ are to be regarded merely as emergeables.

The stochastic-quantum correspondence therefore brings newfound clarity
to the outstanding foundational problems of textbook quantum theory.
Specifically, one sees from this novel perspective that the seemingly
arbitrary division of the world into measuring devices, which purportedly
have underlying configurations, and all other systems, which purportedly
do not, leads directly to all the usual mysteries about the measurement
process according to textbook quantum theory. After all, what, in
the end, determines whether a given system counts as a measuring device,
and therefore merits having underlying configurations?

More relevant to the present discussion is that because textbook quantum
theory includes stochastic hidden variables for measuring devices,
and because those stochastic hidden variables are insufficient to
ensure the factorization property \eqref{eq:BellLocalityFactorizationCriterion},
the nonlocality theorems that employ Bell's statistical locality criterion
imply that textbook quantum theory is itself dynamically nonlocal.
Hence, there is no real cost to upgrading the configurations of $\mathcal{S}$
and $\mathcal{E}$ to being hidden variables on an equal footing with
the configurations of $\mathcal{D}$. These additional hidden variables
do not lead to the factorization property \eqref{eq:BellLocalityFactorizationCriterion}
either, but they also do not lead to any trouble for the \emph{no-communication theorem}~\citep{GhirardiRiminiWeber:1980agaastttqmmp,Jordan:1983qcdnts},
which precludes using quantum theory to send controllable signals
faster than light.

The main conclusion of this analysis is that if one takes Bell's statistical
locality criterion seriously as the proper way to identify dynamical
nonlocality, then textbook quantum theory is already dynamically nonlocal,
so adding some additional hidden variables to the theory will not
ultimately make that dynamical nonlocality any worse. Alternatively,
one could instead dispute that Bell's statistical locality criterion
is an acceptable criterion for locality in the first place, perhaps
by arguing that local correlation-producing interactions are valid
common-causes explanations, but are simply not the sorts of things
that can be conditioned on. However, if one denies the validity of
Bell's criterion, then it cannot be used to argue that the picture
of quantum theory presented in this paper is dynamically nonlocal.
Either way, the approach taken toward quantum theory in this paper
is no more or less dynamically nonlocal than textbook quantum theory
already is.

It may be that a more empirically meaningful notion of dynamical locality
is the condition that new statistical correlations can only arise
between systems that are in local contact, either directly or through
the mediation of other systems. As the no-communication theorem shows,
this empirical locality criterion is satisfied by textbook quantum
theory, as well as by any physical theory or interpretative framework
that reproduces the predictions of textbook quantum theory, and ultimately
ensures the impossibility of sending controllable messages faster
than light. One could argue that any stronger locality requirements,
such as Bell's statistical locality condition, go beyond what is empirically
verifiable, and are therefore matters of metaphysics.

A number of other important no-go theorems have been proved over the
years, including von Neumann's early no-go theorem~\citep{vonNeumann:1927wadq,vonNeumann:1932mgdq},
the Kochen-Specker theorem~\citep{KochenSpecker:1967phvqm}, the
Pusey-Barrett-Rudolph theorem~\citep{PuseyBarrettRudolph:2012rqs},
and Myrvold's no-go theorem~\citep{Myrvold:2002mir}. These theorems
either assume that \emph{all} observables are true random variables
(that is, beables) that exist at the level of the given system's configuration
space, or they assume that measurements are \emph{passive operations}
that merely reveal pre-existing values of observables without altering
the behavior of measured systems in the process, or they assume that
measurements are fundamentally \emph{irreversible}, or they assume
the existence of \emph{additional} probability formulas. Because the
picture of quantum theory introduced in this paper refrains from making
any of these assumptions, it is consistent with these theorems.

\section{Discussion and Future Work\label{sec:Discussion-and-Future-Work}}

This paper has shown that one can reconstruct the mathematical formalism
and all the empirical predictions of quantum theory using simpler,
more physically transparent axioms than the standard Dirac-von Neumann
axioms. Rather than postulating Hilbert spaces and their ingredients
from the beginning, one instead posits a physical model, called a
\emph{generalized stochastic system}, based on trajectories in configuration
spaces following generically indivisible stochastic dynamics. The
\emph{stochastic-quantum correspondence} then connects generalized
stochastic systems with quantum systems in a fundamental way, showing
that every quantum system can be viewed as the Hilbert-space representation
of an underlying generalized stochastic system.

This perspective deflates some of the most mysterious features of
quantum theory. In particular, one sees that density matrices, wave
functions, and all the other appurtenances of Hilbert spaces, while
highly useful, are merely gauge variables. These appurtenances should
therefore not be assigned direct physical meanings or treated as though
they directly represent physical objects, any more than Lagrangians
or Hamilton's principal functions directly represent physical objects.
Superposition is then not a literal smearing of physical objects,
but is merely a mathematical artifact of catching a system in the
middle of an indivisible stochastic process, as represented using
a Hilbert-space formulation and wave functions.

Moreover, from this standpoint, \emph{canonical quantization} need
not be regarded as the promotion of classical observables to noncommutative
operators by fiat, but can be implemented (when mathematically feasible)
simply by generalizing a classical system's dynamics from being deterministic
to being stochastic, with all the exotic features of quantum theory
then emerging automatically. As a consequence, this formulation of
canonical quantization potentially offers more straightforward techniques
for coupling classical systems to quantum systems in real-world applications.

In an important sense, the stochastic-quantum correspondence also
legitimizes many standard practices followed in physics and in other
scientific areas, like astronomy, chemistry, biology, and paleontology.
To see why, notice that according to the thoroughly instrumentalist
and operationalist Dirac-von Neumann axioms, the \emph{only} predictions
provided by textbook quantum theory are predictions of measurement
outcomes, probabilities of measurement outcomes, and expectation values
that are averages of measurement outcomes statistically weighted by
measurement-outcome probabilities~\citep{GriffithsSchroeter:2018iqm,Townsend:2012maqm,Shankar:1994pqm,SakuraiNapolitano:2010mqm,SchumacherWestmoreland:2010qpsi}.
Meanwhile, scientists in all areas of research talk about other kinds
of phenomena\textemdash from the mixing of gases in the primordial
universe to the spontaneous appearance of genetic mutations\textemdash that
presumably just \emph{happen} in some way, according to \emph{happening}
probabilities, in the past, present, or future. Strictly speaking,
however, the \emph{happening} of phenomena lies outside the axiomatic
ambit of textbook quantum theory, which refers only to connecting
the measurement settings of chemical detectors and telescopes to the
probabilities of their measurement outcomes. The inability of textbook
quantum theory to account for the happening of phenomena either means
that scientists are not speaking honestly or coherently about their
research, or that textbook quantum theory is inadequate as a physical
theory.

Decoherence alone cannot bridge the gap between measurement-outcome
probabilities and happening probabilities, because decoherence can
only temporarily change whatever orthonormal basis momentarily diagonalizes
a system's density matrix (and, after all, every density matrix is
always diagonal in \emph{some} orthonormal basis). After a system
undergoes decoherence, textbook quantum theory then still requires
one to make a direct appeal to the measurement axioms to translate
the final density matrix into a statement about probabilities, which
will then axiomatically end up being measurement-outcome probabilities,
rather than happening probabilities.

Nor can appealing to some sort of \emph{thermodynamic limit} resolve
the discrepancy either. In order for a limit in a physical context
to make sense, there should be clearly physical ingredients or constituents
involved. Furthermore, the end result of the limit should gradually
emerge as a better and better physical approximation at \emph{finite}
stages of the limiting process, simply because a rigorous limit consists
of inequalities between finite (if arbitrarily large or small) parameters.
For example, in the \emph{hydrodynamic limit} of a system of classical
interacting particles, the particles are the physical ingredients,
and one sees fluid-like behavior gradually emerge as a better and
better physical approximation as the number of particles progressively
increases. In the case of textbook quantum theory, by contrast, every
finite stage of any purported thermodynamic limit features only measurement
outcomes and measurement-outcome probabilities, so there are no clearly
physical ingredients or constituents, and the gap between measurement
outcomes and the happening of phenomena never closes.

The stochastic-quantum correspondence yields a much richer version
of quantum theory in which physical phenomena really happen, with
probabilities that are really happening probabilities, and therefore
vindicates the ways that scientists talk about the world. Measurement-outcome
probabilities are then merely a special case, arising when what is
actually happening is a change to the configuration of a measuring
device.

Because  this paper's approach invokes hidden variables in the form
of underlying physical configurations, this framework for quantum
theory shares some aspects with the \emph{de Broglie-Bohm formulation},
or \emph{Bohmian mechanics}~\citep{deBroglie:1930iswm,Bohm:1952siqtthvi,Bohm:1952siqtthvii}.
However, in contrast to this paper's approach, Bohmian mechanics
employs deterministic dynamics, and features a fundamental guiding
equation that explicitly breaks Lorentz invariance by singling out
a preferred foliation of spacetime into spacelike hypersurfaces. This
paper instead takes seriously what experiments strongly suggest\textemdash that
the dynamics of quantum theory is indeterministic, and that there
is no fundamentally preferred foliation of spacetime. The formulation
of quantum theory in this paper is also more flexible and model-independent
than Bohmian mechanics, and works for all kinds of quantum systems,
from particles to fields and beyond.

In contrast with the \emph{Everett interpretation}~\citep{Everett:1957rsfqm,Everett:1973tuwf},
also known as the \emph{\textquoteleft many worlds\textquoteright{} interpretation},
 the framework presented in this paper assumes that quantum systems,
like classical systems, have definite configurations in configuration
spaces, and does not attempt to derive probability from non-probabilistic
assumptions or grapple with fundamental aspects of personal identity
in a universe continuously branching into large (and somewhat undefined)
numbers of parallel worlds. The approach in this paper is therefore
more modest, metaphysically speaking, than the Everett interpretation.

Neither this paper's approach nor the Everett interpretation satisfies
the statistical locality criterion formulated by Bell and described
in Subsection~\ref{subsec:Nonlocality}, but the Everett interpretation
arguably exhibits a different notion of dynamical locality at a level
of description that \emph{transcends} its individual world-branches~\citep{Wallace:2012temqtattei}.
However, because each individual world-branch looks no more or less
nonlocal than the world according to textbook quantum theory, it
is not clear whether the Everett interpretation's dynamical locality
is more than a metaphysical statement, nor is it easy to discern what
concrete benefits it truly provides.

Unlike \emph{stochastic-collapse theories}~\citep{GhirardiRiminiWeber:1986udmms,BassiGhirardi:2003drm},
this paper does not invoke any fundamental violations of unitarity,
nor does it require introducing any new constants of nature to specify
dynamical-collapse rates.

Looking forward, it would be interesting to see what implications
the stochastic-quantum correspondence could have for both phenomenological
stochastic processes, like those in biology or finance, as well as
for future work in fundamental physics, like quantum gravity.

More broadly, by recasting the Hilbert-space formulation of quantum
theory as merely a convenient way to represent a large class of stochastic
processes, one opens the door to searching for totally different representations
that might look nothing at all like Hilbert spaces and that could
allow for the construction of more general kinds of stochastic processes.
Perhaps one could even find a way to generalize beyond stochastic
processes altogether. work.

\section*{Acknowledgments}

The author would especially like to acknowledge Emily Adlam, David
Albert, Howard Georgi, David Kagan, and Logan McCarty for extensive
discussions during the writing of this paper. The author would also
like to thank Scott Aaronson, Guido Bacciagaluppi, David Baker, Craig
Callender, Ignacio Cirac, Iris Cong, Jordan Cotler, Erik Curiel, Louis
Deslauriers, Melissa Franklin, Philip Goyal, David Griffiths, Mina
Himwich, Jenann Ismael, Daniel Jafferis, David Kaiser, Efthimios Kaxiras,
Gregory Kestin, Matthew Kleban, Serhii Kryhin, Barry Loewer, Curtis
McMullen, Simon Milz, Arjun Mirani, Wayne Myrvold, Filip Niewinski,
Jill North, John Norton, Matthew Reece, Vivishek Sudhir, Noel Swanson,
Xi Yin, and Nicole Yunger Halpern for helpful conversations.

\newpage
\raggedright
\bibliographystyle{1_home_jacob_Documents_Work_My_Papers_Stochasti____Quantum_Theory__2023__custom-abbrvunsrturl}
\bibliography{0_home_jacob_Documents_Work_My_Papers_Bibliography_Global-Bibliography}

\end{document}
