\section{Introduction}
\label{sec:introduction}

In today's era of high-performance computing, GPUs have emerged as the most popular solution for accelerating compute-intensive workloads due to their massive parallelism and high memory bandwidth. 
However, harnessing the full potential of GPUs can be challenging, especially for legacy CPU applications that were not designed with GPU acceleration in mind.
Porting such applications to the GPU can be time-consuming, error-prone, and usually requires significant development effort.
One needs to identify code regions amenable to acceleration, manage distinct memories, synchronize host and device execution, and handle library functions that are not executable on the device.
These tasks increase the complexity of porting and may require significant re-architecting efforts, making it difficult for non-experts to leverage GPUs for performance gains or even to initiate offloading any part of a large legacy application.

% In today's era of high-performance computing, GPUs have become the most popular solution for accelerating compute-intensive workloads, thanks to their massive parallelism and high memory bandwidth. However, exploiting the full potential of GPUs can be challenging, particularly for legacy CPU applications that were not designed with GPU acceleration in mind. Porting such applications to the GPU can be time-consuming, error-prone, and often requires significant development effort. This involves identifying code regions that are suitable for acceleration, managing distinct memories, synchronizing host and device execution, and handling library functions that are not executable on the device. These tasks increase the complexity of porting and may require significant re-architecting efforts, making it difficult for non-experts to leverage GPUs for performance gains or even to initiate offloading any part of a large legacy application.

\begin{figure}[b]
    \vspace{-3mm}
    \centering
    \resizebox{0.9\linewidth}{!}{ 
      \input{figures/high_level_overview}
    }
     \vspace{-3mm}
    \caption{
    Bird's-eye view of the \emph{GPU First} methodology.
    All components on a grid background are provided or generated by the approach. 
    The loader is the entry point for the operating system and responsible to setup the environment on the device.
    The application executable (top right) is produced from the unmodified legacy source code but runs on the GPU.
    A partial \texttt{libc} GPU implementation provides relatively fast device side runtime calls while other library calls are translated into remote procedure calls (RPCs).
    Our RPC scheme will also orchestrate memory movement for arguments and underlying objects, forward the calls to existing system libraries, and return the result to the application thread waiting on the GPU.
    }
    \Description{
    Bird's-eye view of the \emph{GPU First} methodology.
    All components on a grid background are provided or generated by the approach. 
    The loader is the entry point for the operating system and responsible to setup the environment on the device.
    The application executable (top right) is produced from the unmodified legacy source code but runs on the GPU.
    A partial \texttt{libc} GPU implementation provides relatively fast device side runtime calls while other library calls are translated into remote procedure calls (RPCs).
    Our RPC scheme will also orchestrate memory movement for arguments and underlying objects, forward the calls to existing system libraries, and return the result to the application thread waiting on the GPU.
    }
    % \vspace{-0mm}
    \label{fig:high_level_overview}
\end{figure}

To overcome these challenges, we propose a novel compilation scheme we call ``\emph{GPU First}'' which puts a legacy application on the GPU before any manual porting effort has been started.
Our approach, sketched in \Cref{fig:high_level_overview}, leverages the portability of LLVM's OpenMP offloading to directly compile and run a host application for a GPU, without any source modification.
Instead, the application is compiled for the GPU architecture and started using a provided GPU loader.
Library calls inside the application are either resolved through our partial \texttt{libc} GPU implementation or via automatically generated remote procedure calls (RPCs) to the host.
By adopting the \emph{GPU First} approach, users can seamlessly test and profile their application directly on GPU hardware.
While the performance of full applications running on (current) GPUs is generally not better than CPU execution, it allows developers to easily identify how well existing parallel regions map to the GPU.
Furthermore, it enables rapid testing of code modifications, e.g., data layout transformations or iteration order modifications, on real GPU hardware.
%This could lead to faster development times and more efficient use of computational resources.
We believe this significant simplification will facilitate the adoption of GPU acceleration in various domains and help developers harness the full potential of modern hardware. 

To evaluate the effectiveness of our approach, we performed experiments on two HPC proxy applications with OpenMP CPU and GPU parallelism, four micro benchmarks with originally GPU only parallelism, as well as three benchmarks from the SPEC OMP 2012 suite featuring hand-optimized OpenMP CPU parallelism.
Our results demonstrate that transparent porting is possible and exploration of GPU performance is feasible for non-experts.
For existing parallel loops, we can closely match the performance of corresponding manually offloaded kernels, achieving up to $14.36\times$ speedup on the GPU compared to the CPU implementation of the HPC proxy application.
This validates our assumptions that the \emph{GPU First} methodology can effectively guide porting efforts, e.g., by identifying parallel regions that require reorganization to achieve good scaling behavior on the GPU, and by allowing fast comparison of different algorithmic and implementation choices.

The main contributions of this paper are summarized here while limitations are explained in detail in \Cref{sec:limitations}.

\setlist[enumerate]{leftmargin=15pt}
\begin{enumerate}
\item A novel compilation scheme that allows to target GPUs for a large set of legacy CPU applications by automatically enabling host-only library calls via a generated RPC interface that translate arguments and mitigate underlying memory for use in dedicated memory environments.
\item A parallelism expansion scheme that allows to map OpenMP parallel directives (and parallel loops) from a single thread block (aka. work group), which is the natural OpenMP offload mapping, to the entire GPU for realistic performance studies.
\item A GPU-optimized partial \texttt{libc} implementation that allows fast execution of runtime calls that do not require operating system support, including a GPU-optimized allocator.
\item An evaluation of the \emph{GPU First} approach on SPEC OMP 2012 benchmarks as well as HPC proxy applications. The former shows the applicability of our scheme to large codes while the latter highlights how parallel loops of the CPU version perform very similar to the manually offloaded GPU kernels. Thus, \emph{GPU First} is well suited to guide porting efforts for legacy applications that already use OpenMP parallelism on the CPU.
\end{enumerate}

The rest of the paper is organized as follows.
In \Cref{sec:background}, we provide background information on OpenMP target offloading and the RPC mechanism.
In \Cref{sec:implementation}, we describe the design and conceptual implementation of our \emph{GPU First} method.
\Cref{sec:limitations} discusses the limitations of our approach and potential directions for future work.
In \Cref{sec:evaluation}, we present our evaluation results to demonstrate the effectiveness of our approach to guide porting efforts for legacy applications.
We will talk about related works in \Cref{sec:related-works} before we conclude with \Cref{sec:conclusion} and insights on the potential impact of our work on the field of parallel computing.