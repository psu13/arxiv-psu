\section{Limitations and Future Works}
\label{sec:limitations}

% \emph{GPU First} is a proof-of-concept implementation to showcase that the advancements in GPUs, combined with modern compiler technology, can significantly simplify our approach to GPU programming.
% Old restrictions, e.g., the absence of recursion, lack of atomic accesses, etc., which effectively lead to the current design of offload languages should be questioned and alternative methods, such as \emph{GPU First},  should be explored.
% That said, there are various technical challenges to overcome which we will discuss briefly.
\emph{GPU First} is a proof-of-concept implementation that showcases how advancements in GPUs, coupled with modern compiler technology, can simplify the approach to GPU programming significantly.
The conventional restrictions, such as the absence of recursion, lack of atomic accesses, and so on, that have led to the current design of offload languages should be reconsidered, and alternative methods, such as \emph{GPU First}, should be explored.
Despite its potential, there are various technical challenges that \emph{GPU First} needs to overcome, which we will discuss briefly.


\subsection{Multiple Levels of Indirection}

In \Cref{sec:rpc-and-memory} we described that we move underlying objects to and from the host when a pointer to them is used in an RPC call.
This allows the library function to access the object, e.g., to write the result from a file I/O operation into that memory.
However, we do not yet try to move more than one level of memory which prevents the host function from accessing objects through indirection.
As an example, a host function might be passed a \lstinline|int**| and we would migrate the object the outer pointer points to automatically.
Thus, when the initial pointer (after translation to the host value) is dereferences, the migrated memory is accessed.
If, however, the resulting \lstinline|int*| is accessed by the host, the value will likely point to device memory.
While it is possible to move and update pointers for multiple levels, the precision and efficiency of the approach will depend on the availability of domain knowledge about accesses.
Annotated library headers, as generated by the ``HTO''~\cite{poster/MosesHTO}, would likely make this feasible in practice.
A system with unified shared would not encounter this problem at all.


\subsection{Reverse Offloading of Code}

In our prototype we only execute host landing-pad functions generated during compilation.
However, we plan to extend this capability in the future to allow for the execution of other host code, such as when a function pointer is passed to the host via an RPC or when an object method is invoked as part of an RPC.
The first step would be to generate potentially host executed code for the host as well, which could be as simple as generating all code for the host and the GPU.
In the second step we need to translate function pointers from the device to the host value when objects are moved from the device to the host, or alternatively when a fault is caused while trying to execute code through a device function pointer.
If objects or function pointers are created on the host as part of an RPC the revere procedure is required.
This shortcoming is less severe in legacy C code but shows when C++ objects are created on the device and used on the host as their virtual table contains both, an additional level of indirection and pointers to device-only code.
That said, C++ objects that are only used on the device are already supported, including virtual function calls and other inheritance-related features.

While the above limits the applicability of the \emph{GPU First} methodology, there are related opportunities to improve performance in the presence of code regions that should be executed on the host.
So far, only single library calls are issued on the host, however, entire code regions in the original application could benefit from execution on the host.
The applicability limitations discussed so far notwithstanding, we could outline the region and treat the code as if it was originally in an external library.
As such, our RPC generation would take care of the call and (single-level) memory movement.
% In our current prototype, we can only execute host landing-pad functions that are generated during compilation.
% However, we plan to extend this capability in the future to allow for the execution of other host code, such as when a function pointer is passed to the host via an RPC or when an object method is invoked as part of an RPC.
% The first step towards achieving this goal would be to generate potentially host executed code for the host as well, which could be as simple as generating all code for both the host and the GPU.
% In the second step, we would need to translate function pointers from the device to the host value when objects are moved from the device to the host, or when a fault occurs while trying to execute code through a device function pointer.
% If objects or function pointers are created on the host as part of an RPC, the reverse procedure would be required.
% This shortcoming is more apparent when dealing with C++ objects created on the device and used on the host, as their virtual table contains an additional level of indirection and pointers to device-only code.
% Nevertheless, our methodology already supports C++ objects that are only used on the device, including virtual function calls and other inheritance-related features.

% While the limitations discussed above constrain the applicability of the \emph{GPU First} methodology, there are related opportunities to improve performance in the presence of code regions that should be executed on the host.
% Currently, only single library calls are issued on the host, but entire code regions in the original application could benefit from host execution.
% Although the applicability limitations still apply, we could delineate the region and treat the code as if it were in an external library.
% As such, our RPC generation would take care of the call and (single-level) memory movement.

\subsection{Multi-Team Execution with Communication}

Another limitation of our work is that we only rewrite certain parts of the code to support multi-team execution in our prototype.
For example, we change the work-sharing schedule and make sure the user observed thread Ids are continues across the threads in the different teams (ref.~\Cref{fig:parallel-kernel}).
While we do not yet rewrite inter-thread communication, such as \lstinline{reduction} clauses, most common cases could be handled through additional engineering effort.

%Despite this limitation, it is important to note that this is not necessarily a limitation of the \emph{GPU First} scheme itself.
%With proper front-end and runtime support, the \lstinline{reduction} clause~could~be~incorporated~seamlessly.

\subsection{Single-Threaded RPC Handling}

Our prototype features single-threaded RPC handling which can, in parallel regions, significantly impact performance.
However, since multi-threaded RPC schemes can be implemented, this is not a conceptual limitation and will also not influence our benchmarks that do not issue RPC calls from inside parallel regions.
