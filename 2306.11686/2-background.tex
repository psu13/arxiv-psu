\begin{figure}[b]
\vspace*{-3mm}
\resizebox{.9\linewidth}{!}{
\input{figures/background_overview}
}
\vspace*{-3mm}
\caption{Overview of the compilation and execution path of the direct GPU compilation framework introduced by \citet{DBLP:conf/llvmhpc/TianHPCD22}.
In our work the source of the application still remains unchanged.
The source wrapper and user wrapper files are taken from the original direct GPU compilation paper.
The \texttt{libc} implementation has been extended for this work, and the compiler is now augmented to automatically generate RPC calls and expand source parallelism to the entire GPU device.
The figure was adapted from Figure 1 in \citet{DBLP:conf/iwomp/TianHTCD22}.
The highlighted parts are our contribution.}
\Description{
Overview of the compilation and execution path of the direct GPU compilation framework introduced by \citet{DBLP:conf/llvmhpc/TianHPCD22}.
In our work the source of the application still remains unchanged.
The source wrapper and user wrapper files are taken from the original direct GPU compilation paper.
The \texttt{libc} implementation has been extended for this work, and the compiler is now augmented to automatically generate RPC calls and expand source parallelism to the entire GPU device.
The figure was adapted from Figure 1 in \citet{DBLP:conf/iwomp/TianHTCD22}.
The highlighted parts are our contribution.
}
\label{fig:compilation_execution_path}
\end{figure}

\section{Background}
\label{sec:background}

With the introduction of the \lstinline{target} construct in OpenMP 4.0, it became possible to execute a code region on a target device like a GPU~\cite{DBLP:conf/sc/BertolliAEOSJCS14} or FPGA~\cite{DBLP:conf/iwomp/MayerKP19}.
In this section, we will provide an overview of the LLVM/OpenMP execution model and explain the compilation and execution path of the \emph{GPU First} methodology. 
Additionally, we will introduce the basics of host remote procedure calls (RPCs).

\subsection{OpenMP Execution Model}\label{sec:execution-model}

An OpenMP program begins with a single \emph{initial thread} executing sequentially.
When any thread encounters a \lstinline{parallel} construct, it creates a new team with zero or more additional threads, and each of these threads executes the associated code.
%\Cref{fig:execution-model-host-openmp} illustrates an example of a host program with a \lstinline{parallel} construct.
%The initial thread executes Region 1, and when it encounters the \lstinline{parallel} construct, it creates a team of threads, each executing Region 2.
%After Region 2 is executed by all threads in the team, only the initial thread continues with the sequential execution of Region 3.

The execution of a \lstinline{target} region is similar to the program start.
A single initial thread executes the device code sequentially.
The \lstinline{parallel} construct works again similar, tough, due to the synchronization requirements of the OpenMP standard, compilers will only use threads of the same thread block / workgroup for the new team.
To utilize the entire device, OpenMP introduced the \lstinline{teams} construct which starts a league of teams each with an independent initial thread.
To distribute work across the league and a team one can use the \lstinline|distribute| and \lstinline|for| constructs, or, alternatively, the global coordinates for manual work-sharing.
%For example, in \Cref{fig:execution-model}, when the target region starts execution, $N$ teams are created.
%Only one thread, the initial thread, in each team executes Region 1 independently.
%When those threads come to the \lstinline{parallel} construct, all threads in each team execute Region 2.
%After the execution of Region 2, only the $N$ initial threads execute Region 3.

%\begin{figure}[t]
%\begin{minipage}{\linewidth}
%\vspace{2mm}
%\begin{lstlisting}[frame=lines]
%int main(int argc, char **argv) {
%  /* region 1 */
%#pragma omp parallel
%  { /* region 2 */ }
%  /* region 3 */
%}
%\end{lstlisting}
%\subcaption{An example of a host OpenMP program with three regions.
%Region 1 and 3 are serial execution region, and region 2 is parallel region.}
%\label{fig:execution-model-host-openmp}
%\vspace{3mm}
%\end{minipage}
%
%\begin{minipage}{\linewidth}
%\begin{lstlisting}[frame=lines]
%int main(int argc, char *argv[]) {
%#pragma omp target teams num_teams(N)
%  {
%    /* region 1 */
%#pragma omp parallel
%    { /* region 2 */ }
%    /* region 3 */
%  }
%}
%\end{lstlisting}
%\subcaption{The OpenMP target offloading version of \Cref{fig:execution-model-host-openmp}.}
%\label{fig:execution-model}
%\vspace*{-1mm}
%\end{minipage}
%
%\caption{An example of host OpenMP program and its corresponding target offloading code to show the OpenMP execution model.}
%\label{fig:openmp-execution-model}
%\end{figure}



\subsection{Compilation and Execution Path}

Our methodology is based on the compilation and execution path proposed by \citet{DBLP:conf/llvmhpc/TianHPCD22} as part of their direct GPU compilation framework.
Similar to their approach, our methodology compiles the user application for the GPU without modifying the application source code.
However, we augment the compiler in two distinct ways to achieve portability and performance.
First, we automatically generate RPC calls and, second, we expand eligible source parallelism to the entire GPU.
In addition, we enhance their wrapper scripts and partial \texttt{libc} implementation as discussed in \Cref{sec:allocators}.
\Cref{fig:compilation_execution_path}, adapted from \citet{DBLP:conf/llvmhpc/TianHPCD22}, provides an overview of our augmented compilation and execution path.
%Note that our new compiler passes are embedded in the link-time-optimizations (LTO) pipeline of LLVM/Clang.
The main and user wrapper fulfill the same functionality as described by \citet{DBLP:conf/llvmhpc/TianHPCD22}, namely to compile the application code for the GPU architecture, load the environment, e.g, command line options, onto the device and finally transfer control to the user provided \lstinline|main| function on the GPU.
The partial \texttt{libc} implementation is embedded into the application during compile time.
The LLVM/OpenMP offloading runtime, orchestrates the offloading and provides necessary functionality to the RPC host server when it communicates with the GPU threads via ``shared'', in our case, managed, memory.


\subsection{Host Remote Procedural Call}
One of the key challenges in executing a CPU program on a GPU are external functions that are only defined in system or third-party libraries.
We employ remote procedural call (RPC) to utilize the existing host functions during the device execution.
In principle, this allows GPU code to call any function on the host almost as if they were local.
A crucial requirement is the coordination between the host and GPU, that can be achieved through a synchronous, stateless client-server protocol~\cite{DBLP:conf/asplos/SilbersteinFKW13,DBLP:conf/ipps/MikushinLZB14,DBLP:conf/llvmhpc/TianHPCD22}.
In this protocol, the GPU (client) sends requests to the host (server) and waits for the host to acknowledge the completion of the requested function.
Data transfer, e.g., for the arguments and return value, are handled either explicitly \cite{DBLP:conf/llvmhpc/TianHPCD22} or implicitly \cite{DBLP:conf/ipps/MikushinLZB14}.
