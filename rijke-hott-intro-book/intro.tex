\chapter{Introduction}

If I asked you to imagine a collection of things, you might think of the books on your shelf, all the different kinds of shoes you have, the things you would bring with you on your next trip, and so on. More abstractly, those collections of things can be thought of as a clearly defined set of items. We have a perfectly good mathematical theory about this: set theory.

Now what if I asked you to think of the collection of all groups? Surely, there are very many groups, so one might restrict to groups of a certain size. But there is something much more fundamental than matters of size. Our group theory teachers also taught us to not bother too much with different presentations of isomorphic groups. Having an isomorphism between two groups $G$ and $H$ makes these groups the same for all practical purposes. At this point the teacher will usually assure you that this is okay, even if set theory insists that the two isomorphic groups with different presentations are different objects in the collection of all groups. Your teacher was right to do so, because it helped you focus on the more important aspects of groups.

Homotopy Type Theory (HoTT) is an emerging field of mathematics and computer science that extends Martin-LÃ¶f's dependent type theory by the addition of the univalence axiom and higher inductive types. In HoTT we think of types as spaces, dependent types as fibrations, and of the identity types as path spaces.

\section{The main results in this book}
We begin the book with an explanation of the rules of type theory
\begin{enumerate}
\item The fact that $0\neq 1$.
\item The fundamental theorem of identity types.
\item The type of natural numbers is a set.
\item The uncountability of $2^\N$.
\item The infinitude of primes
\item The structure identity principle for groups.
\item The construction of the fundamental cover of the circle.
\item The descent theorem for pushouts
\item The irrationality of $\sqrt{2}$.
\item The equivalence of the category of groups and the category of pointed connected $1$-types.
\item The construction of the Hopf fibration.
\item The Blakers-Massey theorem.
\item The stabilization theorem of higher groups.
\end{enumerate}


\section{The Curry-Howard correspondence}
%Dependent type theory is designed to reflect closely on actual mathematical practice and is compatible with classical logic. The foundational issue that isomorphic objects may have wildly different encodings in set-theoretic language, complicating the verification of mathematics, is addressed in type theory, where objects can only ever be defined up to equivalence. Despite the fact that dependent type theory is of constructive nature, it is important to note that type theory is not anti-classical: at the loss of certain properties of constructive type theory constructivists may care about, the axiom of choice may be assumed in type theory and it is in fact consistent with the univalence axiom. This may be helpful to obtain some classical results in type theory.

%One of the important properties that dependent type theory has (when the axiom of choice is not assumed) is that  

%From a logical point of view, type theory can be seen as a deductive system for constructive logic, in which types are propositions of which the constituents are precisely its proofs. In the view of Heyting, `to know the meaning of a proposition is to know which constructions can be considered as proofs of that proposition'. For instance, a proof of the proposition $A\to B$ is an algorithm that transforms proofs of $A$ into proofs of $B$.

From a syntactic point of view, type theory is a just a deductive system, or a language with enough structure to encode (most) mathematical practice. If one thinks of type theory as a deductive system, then it is natural to think of types as propositions. The terms of a type are then its proofs. However, one important difference between types and propositions is that types may have different terms, whereas propositions are completely determined by their truth value, and therefore do not have intrinsic structure beyond their provability. In other words, if there are two proofs of a given proposition $P$, then these two proofs are never regarded as distinct elements of that proposition (although they might be distinct in a syntactic sense). Nevertheless, the analogy between types and propositions holds up quite well, and is made precise in the \define{Curry-Howard correspondence}, see \cref{table:ch}.

The phenomenon that types may have distinct terms is known as \define{proof-relevance}: to construct a term of a given type with a certain property it often matters how that term is constructed. This is of course no different in mathematical practice. For example, every now and then one encounters in a mathematical exposition a proposition that of the form `structures $A$ and $B$ are isomorphic', with the isomorphism being constructed in the proof. Here it matters of course how that isomorphism is constructed, and that specific isomorphism might even be used later on. Thus, the idea of proof-relevance is nothing new.

Since types may possess many terms, one might observe that there are also formal similarities between types and sets. Indeed, a set is completely determined by how one can give an element of that set, in a similar way that a proposition is determined by how one can give a proof of that proposition. The Curry-Howard correspondence also provides a translation between types and sets.

An important difference between type theory and set theory, which makes type theory more useful as a language for formalizing mathematical constructions, is that the theory of types is itself a deductive system, whereas the theory of sets is formulated on a \emph{separate} deductive system: first order logic. Moreover, one may extract programs from proofs: a proof of the existence of an object with a certain property yields a construction of that object together with a proof that the constructed object indeed satisfies the stated property.

\begin{table}\label{table:ch}
\caption{The Curry-Howard correspondence}
\begin{center}
\begin{tabular}{lll}
\toprule
\emph{First order logic} & \emph{Set theory} & \emph{Type theory}\\
\midrule
Propositions & Sets & Types\\
Predicates & Families of sets & Dependent types\\
Proofs & Elements & Terms \\
$\top$ & $\{\emptyset\}$ & $\unit$\\
$\bot$ & $\emptyset$ & $\emptyt$ \\
$P \land Q$ & $A \times B$ & $A \times B$ \\
$P \vee Q$ & $A \sqcup B$ & $A + B$ \\
$\exists x.P(x)$ & $\coprod_{i\in I}A_i$ & $\sm{x:A}B(x)$ \\
$\forall x.P(x)$ & $\prod_{i\in I}A_i$ & $\prd{x:A}B(x)$\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\section{Types in mathematical practice}


To illustrate the concept of type dependency, let us have a closer look at the anatomy of the following purposefully simple lemma.

\begin{lem}\label{lem:unit}
Given a binary operation $\mu:A\times A\to A$ on a set $A$, any $u_l\in A$ satisfying satisfying the left unit law $\mu(u_l,x)=x$, and any $u_r\in A$ satisfying the right unit law $\mu(x,u_r)=x$, one has $u_l=u_r$. 
\end{lem}

\begin{proof}
Since $u_l$ is a left unit, we have in particular $u_l=\mu(u_l,u_r)$. Furthermore, since $u_r$ is a right unit we have in particular $\mu(u_l,u_r)=u_r$. Thus, we have $u_l=\mu(u_l,u_r)=u_r$. 
\end{proof}

\begin{samepage}
By the hypotheses of \cref{lem:unit}, we start the proof with the following set of presuppositions:
\begin{align*}
A & : \mathbf{Set} \\
\mu & : A\times A\to A \\
u_l & : A \\
p & : \forall x.\,\mu(u_l,x)=x\\
u_r & : A \\
q & : \forall x.\,\mu(x,u_r)=x,
\end{align*}
and the task is to show that $u_l=u_r$.
\end{samepage}

This list of assumptions is called the context of our proof, and the goal $u_l=u_r$ is a type in this context. 
Note that $\mathbf{Set}$ is a type in the empty context (where no assumptions are made), $A\times A\to A$ is a type in the context $A:\mathbf{Set}$, also $A$ is a type in the context $A:\mathrm{Set}$, and $\forall x.\,\mu(u_l,x)=x$ is a type in context $A:\mathbf{Set},\mu:A\times A\to A,u_l:A$, and so on.
In principle, one could give such a finite list of presumed structure for any mathematical text at any position in the text.

More generally, \define{contexts} are lists of `typed' variable declarations. By `typed' we mean that any variable is assigned a (unique) type. A context is always finite, and the variables in a context can have any type, possibly depending on variables that have been declared previously. In our example, the variable $p:\forall x.\,\mu(u_l,x)=x$ depends on $A:\mathbf{Set}$, $\mu:A\times A\to A$, and $u_l:A$. 
