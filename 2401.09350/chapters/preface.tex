\preface
\addcontentsline{toc}{chapter}{Preface}

We are witness to a few years of remarkable developments
in Artificial Intelligence with the use of advanced machine learning algorithms,
and in particular, \emph{deep learning}. Gargantuan, complex neural networks
that can learn through self-supervision---and quickly so with the aid of specialized
hardware---transformed the research landscape so dramatically that, overnight it seems,
many fields experienced not the usual, incremental progress, but rather a leap forward.
Machine translation, natural language understanding, information retrieval,
recommender systems, and computer vision are but a
few examples of research areas that have had to grapple with the shock.
Countless other disciplines beyond computer science such as robotics, biology, and chemistry
too have benefited from deep learning.

These neural networks and their training algorithms may be complex, and
the scope of their impact broad and wide,
but nonetheless they are simply functions in a high-dimensional space.
A trained neural network takes a \emph{vector} as input, crunches and transforms
it in various ways, and produces another vector, often in some other space.
An image may thereby be turned into a vector, a song into a sequence of vectors,
and a social network as a structured collection of vectors.
It seems as though much of human knowledge,
or at least what is expressed as text, audio, image, and video,
has a vector representation in one form or another.

It should be noted that representing data as vectors is not unique to neural networks
and deep learning. In fact, long before learnt vector representations of pieces of
data---what is commonly known as ``embeddings''---came along, data
was often encoded as hand-crafted \emph{feature} vectors. Each feature quantified
into continuous or discrete values some facet of the
data that was deemed relevant to a particular task (such as classification or regression).
Vectors of that form, too, reflect
our understanding of a real-world object or concept.

If new and old knowledge can be squeezed into a collection of
learnt or hand-crafted vectors, what useful things does that enable us to do?
A metaphor that might help us think about that question is this:
An ever-evolving database full of such vectors that capture
various pieces of data can be understood as a \emph{memory} of sorts.
We can then recall information from this memory to answer questions,
learn about past and present events, reason about new problems,
generate new content, and more.

\section*{Vector Retrieval}

Mathematically, ``recalling information'' translates to finding vectors that are
most \emph{similar} to a \emph{query} vector. The query vector represents what
we wish to know more about, or recall information for. So, if we have a
particular question in mind, the query is the vector representation of that question.
If we wish to know more about an event, our query is that event expressed as a vector.
If we wish to predict the function of a protein, perhaps we may learn a thing or two
from known proteins that have a similar structure to the one in question,
making a vector representation of the structure of our new protein a query.

Similarity is then a function of two vectors, quantifying how similar two vectors are.
It may, for example, be based on the Euclidean distance
between the query vector and a database vector, where similar vectors have a smaller
distance. Or it may instead be based on the inner product between two vectors.
Or their angle. Whatever function we use to measure similarity between pieces
of data defines the structure of a database.

Finding $k$ vectors from a database that have the highest similarity to a query vector
is known as the top-$k$ retrieval problem.
When similarity is based on the Euclidean distance, 
the resulting problem is known as \emph{nearest neighbor} search.
Inner product for similarity leads to a problem known as \emph{maximum inner product search}.
Angular distance gives \emph{maximum cosine similarity} search.
These are mathematical formulations of the mechanism we called ``recalling information.''

\bigskip

The need to search for similar vectors from a large database
arises in virtually every single one of our online transactions.
Indeed, when we search the web for information about a topic, the search engine itself
performs this similarity search over millions of web documents to find what may
lexically or semantically match our query. Recommender systems find the most similar
items to your browsing history by encoding items as vectors and, effectively, searching
through a database of such items. Finding an old photo in a photo library,
as another routine example, boils down to performing a similarity search over vector
representations of images.

A neural network that is trained to perform a general task such as question-answering,
could conceivably augment its view of the world by ``recalling'' information from such a database
and finding answers to new questions.
This is particularly useful for \emph{generative} agents such as chatbots
who would otherwise be frozen in time, and whose knowledge limited to
what they were exposed to during their training. With a vector database on the side,
however, they would have access to real-time information and can deduce new observations
about content that is new to them.
This is, in fact, the cornerstone of what is known as retrieval-augmented generation,
an emerging learning paradigm.

\bigskip

Finding the most similar vectors
to a query vector is easy when the database is small or when time is not of the essence:
We can simply compare every vector
in the database with the query and sort them by similarity. When the database grows large
and the time budget is limited, as is
often the case in practice, a na\"ive, exhaustive comparison of a query with database vectors
is no longer realistic.
That is where \textbf{vector retrieval} algorithms become relevant.

For decades now, research on vector retrieval has sought to improve the efficiency
of search over large vector databases. The resulting literature is rich with
solutions ranging from heavily theoretical results to performant empirical heuristics.
Many of the proposed algorithms have undergone rigorous benchmarking
and have been challenged in competitions at major conferences.
Technology giants and startups alike have invested heavily in developing
open-source libraries and managed infrastructure that offer fast and scalable
vector retrieval.

That is not the end of that story, however. Research continues to date.
In fact, how we do vector retrieval today faces a stress-test as
databases grow orders of magnitude larger than ever before.
None of the existing methods, for example, proves easy to scale to a
database of billions of high-dimensional vectors, or a database
whose records change frequently. 

\section*{About This Monograph}
The need to conduct more research underlines the importance of making the existing
literature more readily available and the research area more inviting.
That is partially fulfilled with existing surveys that report
the state of the art at various points in time. However, these publications
are typically focused on a single class of vector retrieval algorithms,
and compare and contrast published methods by their empirical performance alone.
Importantly, no manuscript has yet summarized major algorithmic milestones
in the vast vector retrieval literature, or has been prepared to serve as a reference
for new and established researchers.

That gap is what this monograph intends to close.
With the goal of presenting the fundamentals of vector retrieval as a sub-discipline,
this manuscript delves into important data structures and algorithms that have
emerged in the literature to solve the vector retrieval problem efficiently and effectively.

\subsection*{Structure}
This monograph is divided into four parts. The first part introduces the problem
of vector retrieval and formalizes the concepts involved. The second part delves into
retrieval algorithms that help solve the vector retrieval problem efficiently and effectively.
Part three is devoted to vector compression. Finally, the fourth part presents a review
of background material in a series of appendices.

\subsubsection*{Introduction}
We start with a thorough \textbf{introduction} to the problem itself in Chapter~\ref{chapter:flavors}
where we define the various flavors of vector retrieval.
We then elaborate what is so difficult about the problem in high-dimensional spaces
in Chapter~\ref{chapter:instability}.

In fact, sometimes high-dimensional spaces are hopeless. However, in reality data often
lie on some low-dimensional space, even though their na\"ive vector representations
are in high dimensions. In those cases, it turns out, we can do much better. Exactly how
we characterize this low \textbf{``intrinsic'' dimensionality} is the topic of
Chapter~\ref{chapter:intrinsic-dimensionality}.

\subsubsection*{Retrieval Algorithms}
With that foundation in place and the question clearly formulated,
the second part of the monograph explores the different classes of existing solutions
in great depth.
We close each chapter with a summary of algorithmic insights.
There, we will also discuss what remains challenging and explore future research directions.

We start with \textbf{branch-and-bound} algorithms in Chapter~\ref{chapter:branch-and-bound}.
The high-level idea is to lay a hierarchical mesh over the space, then given a query point
navigate the hierarchy to the cell that likely contains the solution.
We will see, however, that in high dimensions, the basic forms of these methods become highly inefficient
to the point where an exhaustive search likely performs much better.

Alternatively, instead of laying a mesh over the space, we may define a fixed
number of buckets and map data points to these buckets with the property that,
if two data points are close to each other according to the distance function,
they are more likely to be mapped to the same bucket. When processing a query,
we find which bucket it maps to and search the data points in that bucket.
This is the intuition that led to the family of \textbf{Locality Sensitive Hashing} (LSH)
algorithms---a topic we will discuss in depth in Chapter~\ref{chapter:lsh}.

Yet another class of ideas adopts the view that data points are nodes in a graph.
We place an edge between two nodes if they are among each others' nearest neighbors.
When presented with a query point, we enter the graph through one of the nodes
and greedily traverse the edges by taking the edge that leads to the minimum distance
with the query. This process is repeated until we are stuck in some (local) optima.
This is the core idea in \textbf{graph} algorithms, as we will learn in Chapter~\ref{chapter:graph}.

The final major approach is the simplest of all: Organize the data points into small clusters during
pre-processing. When a query point arrives, solve the``cluster retrieval'' problem first, then
solve retrieval on the chosen clusters.
We will study this \textbf{clustering} method in detail in
Chapter~\ref{chapter:ivf}.

As we examine vector retrieval algorithms, it is inevitable that we must
ink in extra pages to discuss why similarity based on inner product is special
and why it poses extra challenges for the algorithms in each
category---many of these difficulties
will become clear in the introductory chapters.

There is, however, a special class of algorithms specifically for inner product.
\textbf{Sampling} algorithms take advantage of the linearity of inner product to
reduce the dependence of the time complexity on the number of dimensions.
We will review example algorithms in Chapter~\ref{chapter:sampling}.

\subsubsection*{Compression}

The third part of this monograph concerns the storage of vectors and their distance computation.
After all, the vector retrieval problem is not just concerned with
the time complexity of the retrieval process itself, but also aims to reduce the
size of the data structure that helps answer queries---known as the index.
Compression helps that cause.

In Chapter~\ref{chapter:quantization} we will review how vectors
can be \textbf{quantized} to reduce the size of the index
while simultaneously facilitating fast computation of the distance function
in the compressed domain! That is what makes quantization effective but challenging.

Related to the topic of compression is the concept of \textbf{sketching}.
Sketching is a technique to project a high-dimensional vector into
a low-dimensional vector, called a \emph{sketch}, such that certain properties
(e.g., the $L_2$ norm, or inner products between any two vectors) are \emph{approximately}
preserved. This probabilistic method of reducing dimensionality naturally
connects to vector retrieval.
We offer a peek into the vast sketching literature in Chapter~\ref{chapter:sketching}
and discuss its place in the vector retrieval research.
We do so with a particular focus on \emph{sparse} vectors in an inner product
space---contrasting sketching with quantization methods that are more appropriate
for \emph{dense} vectors.

\subsection*{Objective}

It is important to stress, however, that the purpose of this monograph is \emph{not}
to provide a comprehensive survey or comparative analysis of every published work
that has appeared in the vector retrieval literature.
There is simply too many empirical works with volumes of heuristics and engineering
solutions to cover. Instead, we will give an in-depth, didactic treatment of foundational
ideas that have caused a seismic shift in how we approach the problem,
and the theory that underpins them.

By consolidating these ideas, this monograph hopes to make this fascinating field
more inviting---especially to the uninitiated---and enticing as a research topic
to new and established researchers. We hope the reader will find that this
monograph delivers on these objectives.

\subsection*{Intended Audience}

This monograph is intended as an introductory text
for graduate students who wish to embark on research on vector retrieval.
It is also meant to serve as a self-contained reference that captures
important developments in the field, and as such, may be useful to established
researchers as well.

As the work is geared towards researchers, however, it naturally emphasizes the theoretical
aspects of algorithms as opposed to their empirical behavior or experimental
performance. We present theorems and their proofs, for example.
We do not, on the other hand, present experimental results or compare
algorithms on datasets systematically. There is also no discussion around the
use of the presented algorithms in practice, notes on implementation and libraries, or
practical insights and heuristics that are often critical to making these algorithms
work on real data.
As a result, practitioners or applied researchers may not find
the material immediately relevant.

Finally, while we make every attempt to articulate the theoretical results
and explain the proofs thoroughly, having some familiarity with linear algebra
and probability theory helps digest the results more easily.
We have included a review of the relevant concepts and results from
these subjects in Appendices~\ref{appendix:probability}
(probability),~\ref{appendix:measure} (concentration inequalities),
and~\ref{appendix:linear-algebra} (linear algebra) for convenience.
Should the reader wish to skip the proofs, however, the narrative
should still paint a complete picture of how each algorithm works.
