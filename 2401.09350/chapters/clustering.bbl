\begin{thebibliography}{11}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arthur and Vassilvitskii(2007)]{kmeansplusplus_2007}
D.~Arthur and S.~Vassilvitskii.
\newblock K-means++: The advantages of careful seeding.
\newblock In \emph{Proceedings of the Eighteenth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 1027--1035, 2007.

\bibitem[Auvolat et~al.(2015)Auvolat, Chandar, Vincent, Larochelle, and
  Bengio]{auvolat2015clustering}
A.~Auvolat, S.~Chandar, P.~Vincent, H.~Larochelle, and Y.~Bengio.
\newblock Clustering is efficient for approximate maximum inner product search,
  2015.

\bibitem[Babenko and Lempitsky(2012)]{invertedMultiIndex}
A.~Babenko and V.~Lempitsky.
\newblock The inverted multi-index.
\newblock In \emph{2012 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 3069--3076, 2012.

\bibitem[Bruch(2021)]{bruch2021xendcg}
S.~Bruch.
\newblock An alternative cross entropy loss for learning-to-rank.
\newblock In \emph{Proceedings of the Web Conference 2021}, page 118–126,
  2021.

\bibitem[Bruch et~al.(2019)Bruch, Wang, Bendersky, and Najork]{bruch2019xendcg}
S.~Bruch, X.~Wang, M.~Bendersky, and M.~Najork.
\newblock An analysis of the softmax cross entropy loss for learning-to-rank
  with binary relevance.
\newblock In \emph{Proceedings of the 2019 ACM SIGIR International Conference
  on Theory of Information Retrieval}, page 75–78, 2019.

\bibitem[Bruch et~al.(2023{\natexlab{a}})Bruch, Lucchese, and
  Nardini]{bruch2023fntir}
S.~Bruch, C.~Lucchese, and F.~M. Nardini.
\newblock Efficient and effective tree-based and neural learning to rank.
\newblock \emph{Foundations and Trends in Information Retrieval}, 17\penalty0
  (1):\penalty0 1--123, 2023{\natexlab{a}}.

\bibitem[Bruch et~al.(2023{\natexlab{b}})Bruch, Nardini, Ingber, and
  Liberty]{bruch2023bridging}
S.~Bruch, F.~M. Nardini, A.~Ingber, and E.~Liberty.
\newblock Bridging dense and sparse maximum inner product search,
  2023{\natexlab{b}}.

\bibitem[Chierichetti et~al.(2007)Chierichetti, Panconesi, Raghavan, Sozio,
  Tiberi, and Upfal]{chierichetti2007clusterPruning}
F.~Chierichetti, A.~Panconesi, P.~Raghavan, M.~Sozio, A.~Tiberi, and E.~Upfal.
\newblock Finding near neighbors through cluster pruning.
\newblock In \emph{Proceedings of the Twenty-Sixth ACM SIGMOD-SIGACT-SIGART
  Symposium on Principles of Database Systems}, pages 103--112, 2007.

\bibitem[Guo et~al.(2020)Guo, Sun, Lindgren, Geng, Simcha, Chern, and
  Kumar]{scann}
R.~Guo, P.~Sun, E.~Lindgren, Q.~Geng, D.~Simcha, F.~Chern, and S.~Kumar.
\newblock Accelerating large-scale inference with anisotropic vector
  quantization.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, 2020.

\bibitem[J\'egou et~al.(2011)J\'egou, Douze, and Schmid]{pq}
H.~J\'egou, M.~Douze, and C.~Schmid.
\newblock Product quantization for nearest neighbor search.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 33\penalty0 (1):\penalty0 117--128, 2011.

\bibitem[Paulev\'e et~al.(2010)Paulev\'e, J\'egou, and Amsaleg]{kmeanslsh}
L.~Paulev\'e, H.~J\'egou, and L.~Amsaleg.
\newblock Locality sensitive hashing: A comparison of hash function types and
  querying mechanisms.
\newblock \emph{Pattern Recognition Letters}, 31\penalty0 (11):\penalty0
  1348--1358, 2010.

\end{thebibliography}
