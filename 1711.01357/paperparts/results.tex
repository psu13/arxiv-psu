\begin{figure*}[h!]
	\begin{center}
		\hspace*{-.4in}
		\vspace*{-0.3in}
		\begin{tabular}{   c | c  c  c  c  }
			%\hline
			& \large{\textsf{VIDEO 1 }} &\large{\textsf{VIDEO 2}}   &\large{\textsf{VIDEO 3}} &\large{\textsf{VIDEO 4}}      \\ 
			&\vspace{-.1in}&&&\\
			& \large{\textsf{Pure Rotation}} &\large{\textsf{Rotating Hotspot}}   &\large{\textsf{Face-on Disk}} &\large{\textsf{Edge-on Disk}}      \\ \hline
			&\vspace{-.1in}&&&\\
			\multirow{1}{*}[0.9in]{ \rotatebox[origin=t]{90}{\small{\textsf{SINGLE FRAME}} }}
			&
			{{\includegraphics[height=.15\linewidth]{figures/starwarps_results/rotation30/gt/frames/gt_15_colorbar.pdf}} } 
			\multirow{2}{*}[.9in]{ \includegraphics[width=0.06\linewidth]{figures/cbar/vertical_cbar_0to12.pdf} }
			&
			\includegraphics[height=.15\linewidth]{figures/starwarps_results/hotspot100sR2/gt/frames/gt_115_colorbar.pdf} 
			\multirow{2}{*}[.9in]{ \includegraphics[width=0.06\linewidth]{figures/cbar/vertical_cbar_0to16.pdf} }
			&
			\includegraphics[height=.15\linewidth]{figures/starwarps_results/hotakamovie_02/gt/frames/gt_85_colorbar.pdf} 
			\multirow{2}{*}[.9in]{ \includegraphics[width=0.0668\linewidth]{figures/cbar/vertical_cbar_0to4.pdf} }
			&
			\includegraphics[height=.15\linewidth]{figures/starwarps_results/hotakamovie_45/gt/frames/gt_63_colorbar.pdf} 
			\multirow{2}{*}[.9in]{ \includegraphics[width=0.06\linewidth]{figures/cbar/vertical_cbar_0to8.pdf} }
			\\
			\multirow{1}{*}[0.7in]{ \rotatebox[origin=t]{90}{\small{\textsf{MEAN}} }}
			&
			{{\includegraphics[height=.15\linewidth]{figures/starwarps_results/rotation30/gt/pavgimg_colorbar.pdf}} } \hspace{.55in} &
			\includegraphics[height=.15\linewidth]{figures/starwarps_results/hotspot100sR2/gt/pavgimg_colorbar.pdf} \hspace{.55in} &
			\includegraphics[height=.15\linewidth]{figures/starwarps_results/hotakamovie_02/gt/pavgimg_colorbar.pdf} \hspace{.55in} &
			\includegraphics[height=.15\linewidth]{figures/starwarps_results/hotakamovie_45/gt/pavgimg_colorbar.pdf} \hspace{.55in}
			\\ \hline
			&\vspace{-.1in}&&&\\
			\multirow{1}{*}[0.9in]{ \rotatebox[origin=t]{90}{\small{\textsf{STD. DEVIATION}} }}
			& \hspace{0.01in}
			{{\includegraphics[height=.147\linewidth]{figures/starwarps_results/rotation30/gt/stdevimg_noaxis_r2.pdf}} } \hspace{.59in} & 
			
			\hspace{0.01in}
			 \includegraphics[height=.147\linewidth]{figures/starwarps_results/hotspot100sR2/gt/stdevimg_noaxis_r2.pdf}  \hspace{.59in} &
			
			\hspace{-0.01in} \includegraphics[height=.147\linewidth]{figures/starwarps_results/hotakamovie_02/gt/stdevimg_noaxis_r2.pdf}  \hspace{.55in} & 
			\hspace{-0.01in}
			\includegraphics[height=.147\linewidth]{figures/starwarps_results/hotakamovie_45/gt/stdevimg_noaxis_r2.pdf} \hspace{.55in} 
			\\
		\end{tabular}
		\vspace{.1in}
		\caption{{\bf Ground truth videos:} The four ground truth sequences used to demonstrate results. We show a single frame from each sequence, the mean frame, and the spatial standard deviation of flux density. Video 1 consists of a $160 \mu$-arcsecond image~\cite{avery} that rotates $180^{\circ}$ over the course of a 12 hour observation (24 hour rotational period). Video 2 is a $120 \mu$-arcsecond view of an edge-on black hole disk with a rotating ``hot spot" predicted by~\cite{Broderick_Loeb_2006} with a rotational period of 2.78 hours. Video 3 and 4 are generated using a model of a black hole observed face on and at a $45^{\circ}$ inclination with a $160 \mu$-arcsecond field of view~\cite{Shiokawa_2013}. They assume a spin of 0.9375 with an Innermost Stable Circular Orbit (ISCO) rotational period of 8.96 minutes. The specified FOV and colormaps for the single frame and mean images are used for each corresponding video throughout the remainder of the paper. }
		%video 1: 1.2 max val
		%video 2: 1.6 maxval
		%video 3: .4 maxval
		%video 4: .8 maxval
		\label{fig:groundtruth}
	\end{center}
		\vspace{-.2in}
\end{figure*}

\vspace{-.07in}
\section{Dynamic Imaging Results}
\label{sec:results}

As data from the EHT 2017 campaign is yet to be released, in this section we demonstrate our method on synthetic EHT data and real data from the Very Long Baseline Array (VLBA). Additional results can be seen in the supplemental document and video. The StarWarps algorithm has been implemented as part of the publicly available python \texttt{eht-imaging}\footnote{\url{https://github.com/achael/eht-imaging}} library~\cite{andrew}.

\vspace{-.2in}
\subsection{Synthetic Data Generation} 

We demonstrate our algorithm on synthetic data generated from four different sequences of time-varying sources. These sequences include two realistic fluid simulations of a black hole accretion disk for different observing orientations~\cite{Shiokawa_2013}, a realistic sequence of a ``hot spot" rotating around a black hole~\cite{Broderick_Loeb_2006}, and a toy sequence evolving with pure rotation. The field of view of each sequence ranges from 120 to 160 $\mu$-arcseconds. A still frame from each sequence is shown in Figure~\ref{fig:groundtruth}. To help give a sense of the variation in each sequence, the figure also displays the mean and standard deviation of flux density. We refer to these sequences by their video number, indicated in the figure. 



In order to demonstrate the quality of results under various observing conditions, VLBI observations of SgrA* at 1.3 mm (230 GHz) are simulated assuming
 three different telescope arrays.  The first array, EHT2017, consists of the 8 telescopes at 6 distinct locations that were used to collect measurements for the Event Horizon Telescope in the spring of 2017. The uv-coverage for this array can be seen in Figure~\ref{fig:staticimaging}. The second array, EHT2017+, augments the EHT2017 array with 3 potential additions to the EHT: Plateau de Bure (PDB), Haystack (HAY), and Kitt Peak (KP) Observatory. 
Details on telescopes used in the EHT2017 and EHT2017+ array are shown in Table~\ref{tab:telescopes}.  
The third array, FUTURE, consists of 9 additional telescopes. The uv-coverage of these latter two arrays, along with a colorbar indicating the time of each measurement, is shown in Figures~\ref{fig:uvcov2}. 
%Details on telescopes used in these arrays are shown in the supplemental material.


Visibility measurements are generated using the python \texttt{eht-imaging} library~\cite{andrew}. %assuming a 4 GHz bandwidth with a 100 second integration time. 
Realistic thermal noise, resulting from a bandwidth ($\Delta \nu$) of 4 GHz and a 100 second integration time ($t_{\rm int}$), is introduced on each visibility. The standard deviation of thermal noise is given by
\begin{align}
\sigma = \frac{1}{0.88} \sqrt{\frac{\mbox{SEFD}_1 \times \mbox{SEFD}_2 }{2 \times \Delta \nu \times t_{\rm int} } },
\label{eq:thermal}
\end{align}

\noindent{for System Equivalent Flux Density (SEFD) of the two telescopes corresponding to each visibility\footnote{The factor of 1/0.88 is due to information loss due to recording 2-bit quantized data-streams at each telescope~\cite{TMS}.}~\cite{taylor1999synthesis}.  %, and using bandwidth $\Delta \nu$ and integration time $t_{int}$.}
% in the absense of atmospheric error
%Atmospheric phase error is also introduced into measurements using the \texttt{eht-imaging} library. 
Random station-based atmospheric phases drawn from a uniform distribution at each time step are introduced into measurements using the \texttt{eht-imaging} library. 
In Videos 2-4 a set of measurements is sampled every 5 minutes over a roughly 14 hour duration, resulting in 173 time steps. In Video 1 only 30 time steps are measured over a 12 hour duration. 
}


\begin{figure}[h!]
	\centering
	%{\includegraphics[height=.28\linewidth]{figures/uvcoverage/uv_eht2017.pdf}}
	{\includegraphics[height=.38\linewidth]{figures/uvcoverage/uv_ehtfuture2_2.pdf}}
	{\includegraphics[height=.38\linewidth]{figures/uvcoverage/uv_ehtfuture1_2.pdf}}
	\caption{{\bf Time-varying uv-coverage:} The uv-coverage for EHT2017+ and FUTURE arrays when observing SgrAâˆ—. The uv-coverage for the EHT2017 array can be seen in Figure~\ref{fig:staticimaging}. 
		Baselines are colored by the time of each observation relative to the start time, indicated by the colorbar to the right.}
	\label{fig:uvcov2}

\end{figure}


\begin{table}[!t]
	\begin{center}
		\caption{EHT 2017 Station Parameters}
		\label{tab::eht_station}
		\begin{tabular}{ccc}
 Telescope & Location & SEFD (Jy) \\ \hline
 ALMA & Chile & 110 \\ 
 APEX & Chile & 22000 \\ 
 LMT & Mexico & 560 \\
 SMT & Arizona & 11900 \\ 
 SMA & Hawaii & 4900 \\
 JCMT & Hawaii & 4700 \\ 
 PV & Spain & 2900 \\ 
 SPT & South Pole & 1600 \\ \hline
 PDB* & France & 1600 \\ 
 HAY* & Massachusetts & 2500 \\ 
 KP* & Arizona & 2500 \\ \hline
		\end{tabular}\\
	\end{center}
	\bigskip
	\footnotesize{The location and SEFD of each telescope in the EHT2017 and EHT2017+ arrays. These parameters and locations were used to generate the uv-trajectories in frequency space shown in Figure~\ref{fig:staticimaging} and~\ref{fig:uvcov2}. Telescope names followed by a star (*) were not included in the EHT2017 array.}
	\label{tab:telescopes}
	\vspace{-0.3in}
\end{table}



%\begin{table}[h!]
%\begin{center}
%\begin{tabular}{ c c c }
% Telescope & Location & SEFD (Jy) \\ \hline
% ALMA & Chile & 110 \\ 
% APEX & Chile & 22000 \\ 
% LMT & Mexico & 560 \\
% SMT & Arizona & 11900 \\ 
%SMA & Hawaii & 4900 \\
%JCMT & Hawaii & 4700 \\ 
% PV & Spain & 2900 \\ 
% SPT & South Pole & 1600 \\ \hline
% PDB* & France & 1600 \\ 
% HAY* & Massachusetts & 2500 \\ 
%  KP* & Arizona & 2500 \\ \hline
%\end{tabular}
%\end{center}
%\caption{ The location and SEFD of each telescope in the EHT2017 and EHT2017+ arrays. These parameters and locations were used to generate the uv-trajectories in frequency space shown in Figure~\ref{fig:staticimaging} and~\ref{fig:uvcov2}. Telescope names followed by a star (*) were not included in the EHT2017 array. }
%\label{tab:telescopes}
%\vspace{-0.3in}
%\end{table}

\vspace{-.1in}
\subsection{Static Evolution Model (No Warp)}
\label{sec:nomotionresults}

We first demonstrate results of our method under a static evolution model. In this case, we fix parameters $\theta$ such that $A=\mathds{1}$. This assumes that there is no global motion under a persistent warp field, but only perturbations around a fairly static scene. Despite this incorrect assumption (especially in Videos 1 and 2), this simple model results in reconstructions that surpass the state-of-the-art methods, and recovers distinctive structures that appear in the underlying source images. 


\subsubsection{Synthetic Data Result Comparison}

\input{paperparts/figures/compare_1_new}

Figure~\ref{fig:staticevolutionresults} shows example reconstructions, and corresponding measured error (NRMSE), for combinations of the 4 source videos observed under the 3 telescope arrays. For these results we have set $a=2$, $c=1/2$, and $\bQ = 10^{\text{-}7} \mathds{1}$. The main portion of this figure is broken up into 4 quadrants, each containing results for one video. From left to right, up to down, each quadrant corresponds to Video 1-4 respectively. The ground truth mean image for each video is shown in the upper table. These images correspond to those shown in Figure~\ref{fig:groundtruth}, but are smoothed to 3/4 the nominal resolution of the interferometer to help illustrate the level of resolution we aim to recover. 

%\vspace{0.05in}
Horizontally within each quadrant we present results obtained using data with varying degrees of difficulty. As the number of telescopes in the array increases, so does the spatial frequency coverage. Therefore, reconstructing an accurate video with the FUTURE array is a much easier task than with the EHT2017 array.
%Additionally, having a linear measurement function, $f(\im)=\FTmtx \im $, results in a convex inference method. 
Additionally, using complex visibilities that are not subject to atmospheric errors is much easier than having to recover images from phase corrupted measurements. 
In the case where there are atmospheric phase errors (ATM.), we constrain the reconstruction problem using a combination of visibility amplitude and bispectrum data products. This results in a non-convex problem (that we approximate with series of linearizations) that is much more difficult to solve than when using complex visibilities when there is no atmospheric phase error (NO ATM.). We demonstrate results on the EHT2017 array for both cases, and the EHT2017+ and FUTURE arrays in the case of atmospheric error. 



%\vspace{0.05in}
Vertically within each quadrant we illustrate the results of our method, StarWarps, by displaying the average frame reconstructed. 
We compare our method to two state-of-the-art Bayesian-style methods.~\cite{andrew} solves for a single image by imposing a combination of MEM and TV priors. This method performs well in the case of a static source (see Figure~\ref{fig:staticimaging}), however, in the case of an evolving source it often results in artifact-heavy reconstructions that are difficult to interpret. In~\cite{freek} the authors attempt to mitigate this problem by first smoothing the time-varying data products before imaging.
%\footnote{In the case of no atmospheric error the visibilities are smoothed. In the case of atmospheric error the bispectrum and visibility amplitudes are smoothed.}. 
This approach was originally designed to work on mutli-epoch data; we find it is unable to accurately recover the source structure from a single day (epoch) observation. 
%Although this approach can sometimes work well, we found that it often requires manual tuning and does not always help to improve reconstructions. 
Results of~\cite{freek} are reconstructed by an author of the method. 



\input{paperparts/figures/nopropcomparison}

\input{paperparts/figures/dynamicalimaging_cmp}



\begin{figure}[h!]
	\vspace*{-.3in}
	\centering
	%{\includegraphics[height=.28\linewidth]{figures/uvcoverage/uv_eht2017.pdf}}
	\subfigure[Fig.~\ref{fig:dynamicimagingcmp} frame uv-coverage  ]{\includegraphics[height=.43\linewidth]{figures/dynamicimagingcmp/uvcovarage/hotoka_uvcov.pdf}}
	\subfigure[Fig.~\ref{fig:m87} frame uv-coverage]{\includegraphics[height=.43\linewidth]{figures/dynamicimagingcmp/uvcovarage/m87_uvcov.pdf}}
	\vspace{-.1in}
	\caption{{\bf Single frame uv-coverage:} (a) The uv-coverage for the first frame shown in Figure~\ref{fig:dynamicimagingcmp} contains 21 measurements while (b) the uv-coverage for the first frame shown in Figure~\ref{fig:m87} contains 1736. When the measurements provided are very sparse, as in (a), StarWarps significantly outperforms~\cite{Johnson_dynamical}. However, in the case of many measurements, as in (b),~\cite{Johnson_dynamical} achieves better results with a higher dynamic range.  }
	\label{fig:uvcov3}
	\vspace{-.25in}
	
\end{figure}


\input{paperparts/figures/m87}


\subsubsection{The Importance in Propagating Uncertainty}

StarWarps uses a multivariate Gaussian regularizer for imaging, which leads to a straightforward optimization method that propagates information through time. The uncertainty of each reconstructed image is encompassed in its approximated covariance matrix (${\bf P}_{t|t}$), which informs the reconstruction of each neighboring latent image. Although this covariance matrix is sometimes a crude estimate of the true uncertainty, it is still crucial in reconstructing faithful images when measurements are especially sparse.

The importance of propagating uncertainty through the covariance matrix is demonstrated in Figure~\ref{fig:propinfo}. This figure shows the effect of turning on and off the covariance propagation. Covariance propagation can be easily turned off by setting ${\bf P}_{t|t}=0$ at each forward and backward update. Results in the figure are shown on simulated data from the EHT2017 array on Video 3, with and without atmospheric error. Note that in both cases, propagating the covariance matrix helps to substantially improve results. This is true even even in the case of atmospheric error, when the measurement function $f(\im)$ is non-linear and the covariance matrix is only a rough approximation of the true uncertainty. 





\subsubsection{Dynamical Imaging Comparison}

%StarWarps uses a multivariate Gaussian imaging regularizer for interferometric imaging, which enables a straightforward optimization method that propagates information through time.  
%In~\cite{Johnson_dynamical} (in prep), we develop alternative methods for reconstructing video from interferometric data. These methods allow for greater flexibility to incorporate a variety of imaging assumptions. However, they are prone to local minima, and thus come at the expense of much more difficulty in converging to the true underlying structure.  
%These approaches have significantly different strengths and may ultimately lead to hybrid approaches for video reconstruction that produce higher quality results, even with noisy and sparse data. 


As discussed in Section~\ref{sec:setup}, the Dynamical Imaging method presented in~\cite{Johnson_dynamical} was developed simultaneously, and shares many similarities to the work presented in this paper: they both aim to solve for a video rather than a static image. However, they have significant differences, leading to different strengths and weaknesses. The framework of~\cite{Johnson_dynamical} allows for more sophisticated image and temporal regularization, at the cost of a difficult optimization problem that does not propagate uncertainty. This results in sharper and cleaner videos when there is sufficient data, but can lead to poor results when there are very few measurements. 
Conversely, StarWarps' use of very simple Gaussian image and temporal regularization results in blurry results, but allows us to propagate an approximation of uncertainty (through the covariance matrix) and produce better results when very few measurements are available.

A comparison of results from~\cite{Johnson_dynamical} and StarWarps on EHT2017 simulated data can be seen in Figure~\ref{fig:dynamicimagingcmp}. Results of~\cite{Johnson_dynamical} were produced using $\mathcal{R}_{\Delta I}$ and KL $\mathcal{R}_{\Delta t}$ temporal regularization, and Maximum Entropy and Total Variation Squared image regularization. Note that for this especially sparse data,~\cite{Johnson_dynamical} on its own does not faithfully reconstruct the ring structure of the underlying source. StarWarps is able to produce a ring, but with a number of blurry artifacts. Initializing~\cite{Johnson_dynamical} with the output of StarWarps produces the cleanest result. 
Although the StarWarps method runs faster on this example than~\cite{Johnson_dynamical} (84 seconds vs 204 seconds in Python on a 2.8 GHz Intel Core i7), StarWarps is memory intensive and its computational complexity scales poorly with increasing image size compared to~\cite{Johnson_dynamical}.  
%However, in most relevant cases StarWarps can still be easily run on a personal computer. 
To help solve these issues, in the future ideas from Ensemble Kalman Filters could be adapted in order to avoid StarWarp's costly matrix inversions and reduce the method's memory footprint~\cite{evensen2003ensemble}.  

An additional result comparing the two methods can be seen in Figure~\ref{fig:m87}, which is discussed in the next section. In this example there is sufficient data to reconstruct each frame independently, and~\cite{Johnson_dynamical} is able to produce a cleaner image with a higher dynamic range than StarWarps. 

Figure~\ref{fig:uvcov3} compares the uv-coverage of a single frame for Figures~\ref{fig:dynamicimagingcmp} and Figure~\ref{fig:m87}, highlighting that StarWarps is comparatively strongest  in the case of sparse data, as will be available for the EHT. These examples demonstrate that StarWarps and~\cite{Johnson_dynamical} are complementary methods, and may ultimately lead to hybrid approaches for video reconstruction that produce higher quality results. 


%even with noisy and sparse data. 

%these methods are complementary to one another and can be used together to produce better results. 

 
%only a few measurements are available. 


\subsubsection{Application to Real VLBI Data}


Although StarWarps was developed with the considerations of the EHT in mind, it can be applied to VLBI data taken from other sources and telescope arrays. For instance, galactic relativistic jet sources (``microquasars") often show variability over the course of a single observation~\cite{timedeprecon}. 
However, due to physical constraints, most VLBI telescope networks observe sources that do not evolve this quickly, such as distant jets from the cores of Active Galactic Nuclei. In these cases, traditional static imaging approaches can be applied to each night of data to produce faithful reconstructions. Yet, by jointly processing the data taken over a larger span of time, we are able to make movies of long-term source evolution that preserve continuity of features through time, thus reducing the flickering that occurs when independently reconstructing each frame. 

In Figure~\ref{fig:m87} we demonstrate StarWarps on archival data taken of the M87 jet. This data was taken using the Very Long Baseline Array (VLBA) as part of the M87 Movie Project~\cite{walker2016observations}. Ten epochs of data between the beginning of January and end of August in 2007 were processed simultaneously. Images were reconstructed with a 10 m-arcsecond field of view with $\npix=70$ pixels. 
%Note this field of view is an order of magnitude larger than is expected for EHT observations. 

Unlike as expected in EHT observations, the dynamic range of the M87 Jet is very high. In order to faithfully reconstruct a high dynamic range image using the simple Gaussian prior, we have incorporated gamma correction into our measurement function. Rather than reconstruct a video containing linear-scale images, we instead reconstruct gamma-corrected images. To do this we replace the measurement function $f(\im)$ with $f(\im^{\frac{1}{\gamma}})$. During reconstruction of this M87 Jet video we have used $\gamma=1/2$. Although these images still do not have the same dynamic range that is achieved through other imaging methods~\cite{walker2016observations,Johnson_dynamical}, StarWarps is still able to recover the faint arms of the jet. 

The reconstructed movie produced by StarWarps shows outward motion along the jet. While this motion is hard to see in Figure~\ref{fig:m87}'s static frames, by visualizing a slice of each image (indicated by the cyan line) through time the motion becomes more apparent. The resulting Space $\times$ Time image shows a brighter region of emission moving along the arm of the jet. We show the same Space $\times$ Time reconstruction for different weightings of temporal regularization, $\bQ$. Note that as temporal regularization increases, by decreasing $\bQ$, the Space $\times$ Time image becomes more uniform in time. 
%See the supplemental material for reconstructed videos of this source. 
 
\vspace{-.2in}
\subsection{Unknown Evolution  Model (Learn Warp)}

\input{paperparts/figures/flowfields}
\input{paperparts/figures/timeevolution}

In Section~\ref{sec:nomotionresults} we showed that a static model can often substantially improve results over the state-of-the-art methods, even when there is significant global motion. However, when a source's emission region evolves in a similar way over time, we are able to further improve results by simultaneously estimating a persistent warp field along with the video frames.
%erratic  
%can be modeled by a persistnat change 
We demonstrate the StarWarps EM approach proposed in Section~\ref{sec:dynamic_inference_unknown}, on Videos 1 and 2. In results presented, we have assumed an affine motion basis with no translation ($\theta$ consists of 4 parameters), and have allowed the method to converge over 30 EM iterations. % in Figures BLAH and BLAH for Videos 1 and 2 respectively. 

%The recovered warp fields for videos 1 and 2 is visualized in Figure~\ref{fig:warpfield}. 
Figure~\ref{fig:warpfield} shows the warp field recovered by our EM algorithm. 
Results were obtained from data with and without atmospheric error. In Video 1 the true underlying motion of the emission region can be perfectly captured by the affine model we assume. This allows us to freely recover a very similar warp field. 
However, in the ``hot spot" video (Video 2), there does not exist a persistent warp field that fits the data, let alone an affine warp field. Although the true motion cannot be described by our model, we still recover an accurate estimate indicating the direction of motion. 
%The recovered motion fields for videos 1 and 2 is visualized in Figure~\ref{fig:warpfield}. 


Figure~\ref{fig:motion} helps to further visualize the recovered motion in the ``hot spot" video by showing how the intensities of a region evolve over time. Results of our method are compared to that of a simple baseline method that we refer to as `snapshot imaging'. In snapshot imaging each frame is independently reconstructed using only the small number of measurements taken at that time step.  In particular, we use the MEM \& TV method shown in Figure~\ref{fig:staticimaging} to reconstruct each snapshot. 
Our results using StarWarps show substantial improvement over snapshot imaging, especially in the case of data containing atmospheric phase error.


We expand upon these results in the supplemental material's video and document. 
%Figures~\ref{fig:rotation_example1} and~\ref{fig:rotation_example2} 
Figures 1 and 2 in the supplemental material document compare results obtained when we assume no global motion ($A=\mathds{1}$) to those when we allow the method to search for a persistent warp field. %Results are shown in two settings: when data is generated using the EHT2017+ array assuming no atmospheric phase error (VIS), as well as when phase errors are introduced (AMP \& BISP) into the measurements. At each time, only a small number of measurements are observed (indicated by the corresponding uv-coverage). However, by propagating information across the video we are able to reconstruct good quality images at each time step. 
In the case of large global motion, most of the reconstructed motion is suppressed when we assume $A=\mathds{1}$. However, by solving for the low dimensional parameters of the warp field, $\theta$, we can learn about the underlying dynamics and sometimes produce higher quality videos. %, while also inferring the underlying dynamics of the source. 










%In the case of using complex visibilities, both our method and snapshot imaging produce meaningful results. Although distinctive features of the true underlying image are recovered by both methods, the quality of our StarWarps reconstructions is higher. However, in the case of data containing atmospheric phase errors our method shows substantial improvement over snapshot imaging. As the closure phase and bispectrum are invariant to the absolute position of the source, each snapshot reconstruction produces an image that is shifted by a different amount. This makes it challenging to align the snapshot frames to pull out meaningful structure in the reconstructed video when there is sparse uv coverage. For this reason our method substantially outperforms snapshot imaging. 










%\input{paperparts/figures/rotation_example}







\vspace{-.15in}
\section{Conclusion}
\label{sec:conclusion}

Traditional interferometric imaging methods are designed under the assumption that the target source is static over the course of an observation~\cite{TMS}. However, as we continue to push instruments to recover finer angular resolution, this assumption may no longer be valid. For instance, the innermost orbital periods around the Milky Way's supermassive black hole, Sgr A*, are just minutes~\cite{orbitalperiod}. In these cases, we have demonstrated that traditional imaging methods often break down. 

In this work, we propose a way to model VLBI measurements that allows us to recover both the appearance and dynamics of a rapidly evolving source. Our proposed approach, StarWarps, reconstructs a video rather than a static image. By propagating information across time, it produces significant improvements over conventional approaches to create static images or a series of snapshot images in time.

Our technique will hopefully soon allow for video reconstruction of sources that change on timescales of minutes, allowing a real-time view of the most energetic and explosive events in the universe. 

\vspace{-.2in}




%Methods such as this will make it possible to use imaging to study quickly-changing astronomical source and open up many scientific opportunities once barred from this form of study. This paper represents a first step into this undoubtedly rich field of astronomical dynamical imaging.


%In this work we propose a new way to model VLBI measurements that allows us to recover both the appearance and dynamics of an evolving emission. In past work, imaging methods were designed under the assumption that the source is static over the course of an observation. However, as we continue to push our instruments to be able to recover finer angular resolution, this assumption is no longer valid. For instance, the immediate environment around the Milky Way's galactic black hole, SgrA*, and micro-quasar jets evolve over the course of just minutes. In these cases, we have shown that traditional imaging methods often break down. Our proposed approach, StarWarps, reconstructs a video rather than a static image and propagates information across time in an attempt to retain as much image detail as possible. 
 
%This paper represents a first step into this undoubtedly rich field of astronomical dynamical imaging. However, we believe there is much more than can be done in the future to improve these methods. 
%In this paper, we make the choice to use a simple multivariate Gaussian imaging regularizer. This allows us to better understand the system and derive a straight-forward, simple optimization method.
%However, just as more sophisticated regularizer have propelled the single-image imaging methods, we believe that these can be built on top of our method to further improve results. Methods such as this will make it possible to use imaging to study quickly-changing astronomical source and open up many scientific opportunities once barred from this form of study. 

 


