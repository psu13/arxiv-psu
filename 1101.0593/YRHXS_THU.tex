%--
%- Local macros
%--
%\newcommand{\ssA}{{\scriptscriptstyle{A}}}
\newcommand{\ssB}{{\scriptscriptstyle{B}}}
%\newcommand{\ssS}{{\scriptscriptstyle{S}}}
%\newcommand{\ssH}{{\scriptscriptstyle{H}}}
%\newcommand{\ssh}{{\scriptscriptstyle{h}}}
%\newcommand{\ssW}{{\scriptscriptstyle{W}}}
%\newcommand{\ssT}{{\scriptscriptstyle{T}}}
%\newcommand{\ssZ}{{\scriptscriptstyle{Z}}}
\newcommand{\ssR}{{\scriptscriptstyle{R}}}
\newcommand{\ssF}{{\scriptscriptstyle{F}}}
\newcommand{\ssL}{{\scriptscriptstyle{L}}}
\newcommand{\ssQ}{{\scriptscriptstyle{Q}}}
\newcommand{\ssE}{{\scriptscriptstyle{E}}}
%\newcommand{\bqas}{\begin{eqnarray*}}
%\newcommand{\eqas}{\end{eqnarray*}}
%\newcommand{\nl}{\nonumber\\}
%\def\mnew{\mpar{\hfil NEW \hfil}\ignorespaces}
%\newcommand{\lpar}{\left(}                            % bracketing
%\newcommand{\rpar}{\right)} 
%\newcommand{\lrbr}{\left[}
%\newcommand{\rrbr}{\right]}
%\newcommand{\lcbr}{\left\{}
%\newcommand{\rcbr}{\right\}} 
%\newcommand{\rbrak}[1]{\lrbr#1\rrbr}
%\newcommand{\bq}{\begin{equation}}                    % equationing
%\newcommand{\eq}{\end{equation}}
%\newcommand{\bqa}{\arraycolsep 0.14em\begin{eqnarray}}
%\newcommand{\eqa}{\end{eqnarray}}
%\newcommand{\ba}[1]{\begin{array}{#1}}
%\newcommand{\ea}{\end{array}}
%\newcommand{\ben}{\begin{enumerate}}
%\newcommand{\een}{\end{enumerate}}
%\newcommand{\bei}{\begin{itemize}}
%\newcommand{\eei}{\end{itemize}}
%\newcommand{\eqn}[1]{Eq.(\ref{#1})}
%\newcommand{\eqns}[2]{Eqs.(\ref{#1})--(\ref{#2})}
%\newcommand{\eqnss}[1]{Eqs.(\ref{#1})}
%\newcommand{\eqnsc}[2]{Eqs.(\ref{#1}) and (\ref{#2})}
%\newcommand{\eqnst}[3]{Eqs.(\ref{#1}), (\ref{#2}) and (\ref{#3})}
%\newcommand{\eqnsf}[4]{Eqs.(\ref{#1}), (\ref{#2}), (\ref{#3}) and (\ref{#4})}
%\newcommand{\eqnsv}[5]{Eqs.(\ref{#1}), (\ref{#2}), (\ref{#3}), (\ref{#4}) and (\ref{#5})}
%\newcommand{\tbn}[1]{Tab.~\ref{#1}}
%\newcommand{\tabn}[1]{Tab.~\ref{#1}}
%\newcommand{\tbns}[2]{Tabs.~\ref{#1}--\ref{#2}}
%\newcommand{\tabns}[2]{Tabs.~\ref{#1}--\ref{#2}}
%\newcommand{\tbnsc}[2]{Tabs.~\ref{#1} and \ref{#2}}
%\newcommand{\fig}[1]{Fig.~\ref{#1}}
%\newcommand{\figs}[2]{Figs.~\ref{#1}--\ref{#2}}
%\newcommand{\sect}[1]{Section~\ref{#1}}
%\newcommand{\sects}[2]{Section~\ref{#1} and \ref{#2}}
%\newcommand{\sectm}[2]{Section~\ref{#1} -- \ref{#2}}
%\newcommand{\subsect}[1]{Subsection~\ref{#1}}
%\newcommand{\subsectm}[2]{Subsection~\ref{#1} -- \ref{#2}}
%\newcommand{\appendx}[1]{Appendix~\ref{#1}}
%\newcommand{\hsp}{\hspace{.5mm}}
%\def\negs{\hspace{-0.26in}}
%\def\negsh{\hspace{-0.13in}}
%%--
%\newcommand{\barq}{\overline q}
%\newcommand{\barb}{\overline b}
%\newcommand{\bmid}{\Bigr|}
%--
%\definecolor{Orange}{named}{Orange}
%\definecolor{Purple}{named}{Purple}
%\newcommand{\htr}[1]{{\color{red} #1}}
%\newcommand{\htb}[1]{{\color{blue} #1}}
%\newcommand{\htg}[1]{{\color{green} #1}}
%\newcommand{\hto}[1]{{\color{Orange}  #1}}
%\newcommand{\htp}[1]{{\color{purple}  #1}}
%\definecolor{Lightblue}{cmyk}{0.9,0.1,0.1, 0.3}
%\newcommand{\lbl}[1]{\color{Lightblue}#1\color{Lightblue}}
%--
\section{Parametric and theoretical uncertainties\footnote{A.~Denner,
S.~Dittmaier, S.~Forte and G.~Passarino.}}
\label{THUsection}
%--
\subsection{Introduction}
%--
In this note we address the following questions: definition of theoretical
uncertainties (THU) for LHC predictions, their statistical meaning, inclusion 
of parametric 
uncertainties (PU), their combination.
For the latter we want to stress that the solution (how to combine) relies on
some implicit assumptions; any variation in the assumptions leads to a 
somehow different solution. In this case intuition may still help 
to qualitatively guess how the value of the measurement is affected.

The first step that we need to do is establish the definition of PUs and THUs.
Following this we need to describe the issue (problem) of combination. 
%--
\subsection{Parametric uncertainties}
%--
In our attempt to encode an acceptable definition of theoretical uncertainty  
for observables at the LHC we differentiate parametric uncertainties -- those 
related to the value of input parameters -- from true theoretical uncertainties 
reflecting our lack of knowledge about higher orders in perturbation theory.
%--
\begin{itemize}
\item{{\bf PU}}, Parametric uncertainties, will always be there, but 
eventually reduced when
more precise experiments produce improved results. They should not be mixed
with THU, but listed as
%--

\bq
{\cal O} = x.xxx \pm 0.00y\;\hbox{(param)}\;{}^{+0.00a}_{-0.00b}\;\hbox{(th)}.
\eq
%--
Ideally and assuming that the central value will not change significantly, the
better way of dealing with future improvements is as follows:
%--
\begin{enumerate}
\item Produce for each observable ${\cal O}$, which is a
function of parameters $\{p_1\,\dots\,p_n\}$, the central value
%--
\bq
{\cal O}\lpar p^c_1\,\dots\,p^c_n\rpar,
\eq
%--
\item Provide derivatives
%--
\bq
\frac{d {\cal O}}{d p_i}, \qquad \forall i.
\eq
%--
\end{enumerate}
In this way users will not have to re-run codes as soon as an improved 
measurement of $p_i$ is available.
\end{itemize}
%--
Here, the recommendation is that parametric errors cannot be neglected and 
calculations should include them in their final estimate. 

The main difference between PU and THU is that PU are distributed according to a 
known (usually Gaussian) distribution while the statistical interpretation of THU is
less clear, and they are arguably distributed according to a flat distribution. 
Sometimes the uncertainty on $\alphas$ (say) is added
in quadrature to the scale uncertainty (see Section \ref{scale}), which is  
questionable if the former is Gaussian and the latter is flat. 
It is worth mentioning that we are discussing essentially Standard Model PUs.
%-- 
\subsection{THU, understanding the origin of the problem}
%--
\label{orprob}
In this and the next section we are going to discuss two separate 
issues~\cite{Collins:1990bu} that are sometimes mixed:
%--
\bei
\item What is the optimal choice for QCD scales?
\item Can one use scale variation to estimate higher-order corrections?
\eei
%--

We begin by addressing the first question. Let us for a moment
concentrate on the uncertainty induced by 
variations of the renormalization scale, $\mu_{\ssR}$, and of the
factorization scale, $\mu_{\ssF}$.
The question is: Do we have a $\mu_{\ssR}$ problem in QED?
The answer is {\em yes}, but is it a real problem? This time the answer 
is {\em no}, because we have a {\em physical} subtraction point, $q^2 = 0$,
for photons with momentum transfer $q$, which defines the Thomson limit.
The next question is: Do we have a $\mu_{\ssR}$ problem in the electroweak 
(EW) theory?
Yes, but it is not a real problem since, once again, we have a 
{\em physical} subtraction point(s), since the electromagnetic coupling can still
be fixed in the Thomson limit and the weak mixing angle can be tied to
the ratio of the $\PW$- and $\PZ$-boson masses. Stated in other words, our 
calculations depend on $\mu_{\ssR}$, but once Lagrangian parameters (masses
and couplings) are replaced by data (according to the renormalization
program) this dependence disappears. Of course, in perturbation theory,
the numerical output depends on the set of data that we have chosen, 
therefore the next question will be: Do we have large logs in our radiative
corrections? The answer is {\em yes} for all cases where the coupling is related
to a EW gauge boson, i.e.\ $\PGg, \PW$, or $\PZ$, with momentum
transfer at the EW scale or higher.
In the EW part of our theory the first step of the 
solution will be: Use the $G_{\ssF}\,$-scheme, not $\alpha(0)$, which is 
equivalent to say {\em resum} large logarithms that are connected to the
running of $\alpha(q^2)$ from $q^2=0$ to the EW scale.
In the $G_{\ssF}$ scheme, $G_{\ssF}$ and the gauge-boson masses are used 
as input parameters and the electromagnetic coupling is derived according to
$\alpha_{G_{\ssF}} = \sqrt{2} G_{\ssF}\MW^2(1-\MW^2/\MZ^2)/\pi$.
Actually, the message would be: For an 
observable at a scale $s$ do not use an input parameter set at a scale 
$s_0 \muchless s$ unless you know how to resum large logarithms.
Of course, there may be more logarithms of large scale ratios
connected to effects other than the running of $\alpha$ in addition
(collinear photon radiation, EW Sudakov logarithms, etc.).
Furthermore, the $G_{\ssF}$  scheme should not be used for couplings that 
concern external photons where $\alpha(0)$ is appropriate.

What to do in QCD? Resummation is the keyword~\cite{Catani:2003zt} but, 
admittedly, apart from the running of the strong coupling it is not
always available. There, the most useful keyword will be 
{\em minimization}. To understand the problem consider a physical
observable which is affected by (large) QCD corrections. Since we have
no analogue of $G_{\ssF}$ in QCD, our LO calculation will always contain logarithms
$\ln(s/\mu_{\ssR})$ where $s$ is the scale where we want to study the process. 
Ideally, one should find a scale $s_0$ where some data is available and 
renormalization means the replacement
$\ln(s/\mu_{\ssR}) \to \ln(s/s_0)$
and $s_0$ should not be far away from $s$. This is not (yet) possible in 
QCD, so the question will translate into, {\em how do I choose} 
$\mu_{\ssR}$? The guideline will be {\em set} $\mu_{\ssR}$ {\em to} $s$,
i.e.\ to evolve the coupling to scale $s$ with renormalization
group equations, or,
in other words, make sure that you do not change much by going to the
next order. This is easy in a one-scale process but in any multi-scale
process one will have other additional large logarithms, say of
argument $s/s'$. What to do?
%--
\begin{itemize}
\item Select $\mu_{\ssR}$ and $\mu_{\ssF}$, process by process, in such 
a way that when going from N${}^n$LO to $N^{n+1}$LO you minimize the
effect of the new corrections. 
In many cases a phase-space-dependent choice is needed in order to
achieve this in differential cross sections.
The recipe is the best simulation of a
subtraction at some physical point close to the relevant scale. In
jargon this is called {\em dynamical scale}.
\end{itemize}
%--
\subsection{THU uncertainties}
%--
In this section we will briefly discuss different sources of THU, starting with
QCD scale variation.
%--
\subsubsection{QCD scale variation \label{scale}}
%--
Once the dynamical scale has been selected (process by process) we can
address the second question mentioned in the beginning of
Section~\ref{orprob}. Namely,
how do we understand our approximation in terms of
scale variation, e.g.\ $s/n < \mu_{\ssR,\ssF} < n\,s$? The idea is as
follows: In the {\em full} theory there is no scale dependence
and order by order in perturbation theory we should be able to see this
asymptotic limit. Therefore, variation of the scale(s) is a pragmatic
way of understanding how far we are from controlling the theory. In
practice, this means {\em which value do I choose} for $n$?

The recommendation, in this case, should be as follows: at a given order
look for a plateau in the scale dependence and fix $n$ to be such that
the plateau is included. 
Therefore:
%--
\begin{itemize}
\item Allow each calculation to set the range of scale variation (it is 
a matter of experience), but check that nobody is allowing for too small or
too large variations just to bring the {\em error} in the range 
foreseen by their religion.
\item Check that different calculations and different choices give
consistent results.
\item Drop extreme choices which are too far away from {\em common}
understanding of the problem. 
\end{itemize}
%--
Renormalization scale and factorization scale have different origins and there
is no good argument according to which we should set 
$\mu_{\ssF} = \mu_{\ssR}$. Once again we invoke the {\em minimization
principle}, i.e.\ when going from N${}^n$LO to N$^{n+1}$LO the choice should 
minimize the effect of the new corrections.
Sometimes the estimate of the uncertainty is based on a diagonal scan, sometimes 
anti-diagonal directions are included. There is also another recipe, a 
two-dimensional scan with $1/n < \mu_{\ssR}/\mu_{\ssF} < n$. One should also 
mention that an independent variation of the two scales introduces large 
logarithmic corrections that are cancelled by the next order in perturbation theory.
Our recommendation here is for a one-dimensional scan, monitoring at the same time
large differences induced by the two-dimensional one.

A word of caution is needed at this point: there are examples where one can see
that it is easy to optimize the scale choice, but scale variation becomes a very 
poor way to estimate higher-order corrections (HO) (in fact at LO it misses even 
the order of magnitude).
Being pragmatic we should state that while there may be an optimal scale choice 
(i.e.\ one that minimizes higher-order corrections), one should be careful that 
this does not then bias the results of estimating higher orders by scale variation.
To be more specific, nobody would object to the suggestion that $\mu_{\ssR}$ and
$\mu_{\ssF}$ should be chosen in such a way that higher-order corrections are
minimized, but in practice the recipe is not always meaningful. It remains true that
if we do know the higher order, we will use it, and if we do not know it, we cannot 
estimate the scale which minimizes the difference. Therefore, what we are suggesting
here is to use the last two known orders for the search of stability and for
minimizing corrections (if reasonable), which is -- at best -- a 
{\em rule of thumb}.
Looking for a {\em plateau} simply means looking for a stationary point in the 
dependence of the observable on scale. If there is a stationary point it suggests 
greater reliability. How to trust a calculation if there is no stationary point 
remains an highly questionable point.
%---
% Ansgar
%---
To summarize, one searches the region of the minimum of the
higher-order corrections, and for distributions
a dynamical scale that stays near the minimum in the whole
condsidered range, so that the $K$-factor does not drift away

%To summarize, one searches the region of the minimum, and for distribution
%a dynamical scale that stays near the minimum in the whole condsidered range,
%so that $K$-factor do not drift away.

%--
\subsubsection{PDF}
%--
For PDFs, theoretical uncertainties in the sense defined above are unknown and 
have never been estimated (see section~\ref{pdfsection} and  
Refs.~\cite{forterev,PDF4LHCwebpage,PDF4LHCwiki}). The known PDF uncertainties are
%--
\bei
\item PDF uncertainties, which are propagated data uncertainties (PDF
  uncertainty, henceforth); 
\item parametric uncertainties, of which the one due to propagation of
  the uncertainty on the value of $\alphas$ ($\alphas$ uncertainty henceforth)
has been studied systematically
by several groups, while the uncertainties due to the value of the
heavy-quark masses are being gradually included.
\eei
%--

While we refer to Section~\ref{pdfsection} for a more detailed
discussion, we note that the recommendation given there for the
determination of PDF uncertainties provides a result that already
includes the combination of the 
PDF uncertainty and the $\alphas$ uncertainty.
One option could have been to keep them separated but it was the PDF community 
recommendation to provide only the combination of these two. For
future studies it might be more advantageous to keep them separate in
that this would give more flexibility to the user.
It should
be understood clearly  that other parametric uncertainties are thus 
not included in this prescription.
It is interesting to note that the Gaussian behaviour of PDF
uncertainties has been checked explicitly within the HERAPDF
and NNPDF PDF determinations (see Section~3.2 in Ref.~\cite{Dittmar} and
Ref.~\cite{Ball:2010de}). 
%--
\subsubsection{Other sources of THU}
%--
Other potential sources of THU are:
%--                                                                      
\begin{itemize}
\item Pole masses vs.\ running masses? Whenever we know which mass (including its 
scale) is to be taken the uncertainty should not enter the game. This means that,
in general, we do not recommend inclusion of these effects in THU.

\item In the scheme production$\,\otimes\,$decay the Higgs shape is,
usually, represented by a Breit--Wigner distribution. Differences induced
by using fixed-width scheme vs.\ running width scheme should not enter the THU.
If the difference matters (e.g.\ at large values of the Higgs mass) one should
try to understand the difference and compare results with the complex-mass
scheme.

\item EW uncertainties; we have renormalization scheme dependence, but also an
uncertainty associated to inclusion of EW effect in a QCD NNLO 
calculation~\cite{Actis:2008ug}.
If the QCD $K$-factor is large it will make some difference to multiply
$\delta_{\mathrm{EW}}$ by the full $K$-factor (complete 
factorization~\cite{Anastasiou:2008tj}) or to include it 
only at LO (as the conservative recipe of partial factorization would suggest). 
The most conservative recipe for mixed EW--QCD effects is the vary between 
complete and partial factorization, but an estimate should be given in any case.

\item Full top mass dependence~\cite{Spira:1995rr} vs.\ large 
$\Mt\,$-approximation in the production $\sigma\lpar \PH \to \Pg\Pg\rpar$. 
%It should not enter THU but an estimate of the approximation at NNLO should 
%be included~\cite{Anastasiou:2009kn}.
The correct recipe is as follows: at NLO one should take the full top mass 
dependence and the estimate of the approximation at NNLO should go into the 
THU~\cite{Anastasiou:2009kn}.

\item Inclusion of the bottom-quark loop in gluon--gluon fusion, complete
factorization ($\mid \hbox{top} + \hbox{bottom}\mid^2$ with full NNLO $K$-factor) 
or partial one ($\mid \hbox{bottom}\mid^2$ and top--bottom interference with 
NLO $K$-factor)?

\item
Missing higher-order corrections not related to scale uncertainties.
Sometimes LO predictions lack some scale uncertainty 
that appears only in higher orders (e.g.\ no QCD renormalization scale in the
Drell--Yan process in LO),
sometimes new channels open in higher orders, which is also a systematic
effect that has nothing to do with scales 
(e.g.\ $\Pg\Pg$ channel for $\PW\PW$+jet production).
In particular, the THU resulting from missing higher-order EW corrections
cannot be estimated via scale uncertainties. NNLO EW corrections can be estimated 
to some extent based on the known structure and size of the NLO corrections 
combined with power counting of EW couplings and logarithms.
Here we are discussing mostly SM, at the moment no special recommendation is 
available for MSSM and one should include THU, whenever possible, by scaling the 
corresponding SM THU.
\end{itemize}
%--
\subsection{How to combine THU}
%--
The main question we want to discuss here is: {\em Are THU confidence intervals?} 
And also: {\em Do we have statistical meaning for THU?}
There are different opinions on the subject; some of us think that
THU {\em should} be confidence intervals, though of course being a distribution 
of true values they must be interpreted in a Bayesian sense. Obviously,
given that they refer to a distribution of the values there is no reason to think 
that they are Gaussian, and it might be more reasonable to take them as flat 
distributions. This said, they should be combined using the rules of Bayesian 
inference. The envelope method is then the correct rule to combine probability 
intervals from flat distributions.
%that do not overlap. If the distributions do 
%overlap a lot (or are not flat) then the envelope is just not very good.
%Another way of thinking is that one should not worry too much about THU being 
%confidence intervals and, therefore, THU should be combined according to the 
%envelope method. 

No matter which opinion one has, it seems obvious that if THU come from flat 
distributions, then they should be added linearly, and if from Gaussian, in 
quadrature. It is more reasonable and more conservative to think that THU are 
flat, and thus to add them linearly. As already stated, PDF uncertainties are 
most certainly Gaussian uncertainties, they have been explicitly checked to be 
Gaussian, and should therefore be treated as such.
Therefore, there is enough evidence that the PDF + $\alphas$ uncertainty 
should be added in quadrature to all other PU. The way the total PU is then
combined with the THU comes down to the best way of combining a Gaussian and flat 
distributions (which is less obvious). Of course, whenever a
theoretical uncertainty dominates (typically the QCD
scale uncertainty, e.g.\ for gluon--gluon fusion) the problem becomes less
relevant.

If only one observable is needed each code should provide a set of options 
%--
\bq
\{o_1,\,\dots\,,o_m\}, \qquad \hbox{with values}\quad
 o_i = \{o^1_i,\,\dots\,,o^k_i\}
\eq
%-- 
where, for instance, $o_i$ is QCD $\mu_{\ssR}$ dependence and 
$\{o^1_i,o^2_i,o^3_i\}$ refer to 
$\mu_{\ssR} = \hbox{scale}/2\,,\,\hbox{scale}\,,\,2\,\hbox{scale}$.
After running the observable ${\cal O}$ over all options one determines 
${\cal O}_{\rm min}$, ${\cal O}_{\rm c}$, and ${\cal O}_{\rm max}$, where the 
central value is fixed by the author's taste, defining the {\em preferred} setup.

If several observables have to be combined one has to take into account that,
given $m$ options with multiple values, some of them are correlated, e.g.\ all 
options concerning production via $\Pg\Pg$ fusion should not be varied 
independently in all observables. This means do not compute ${\cal O}_i$ with 
$\hbox{scale}/2$ and ${\cal O}_j$ with $2\,\hbox{scale}$ if both come from 
$\Pg\Pg$ fusion. Even here we have two possibilities:
%--
\begin{itemize}
\item Vary one option at the time and add the effects;
\item vary all options (taking into account correlations) and find the absolute 
minimum and maximum in the allowed range of variation.
\end{itemize}
%--
The first choice has the virtue that experimentalists can decide, later on, on error
combination; the second one is more clean and reflects the true status of THU.
%--

All this said, one has to face the problem of how to combine different
determinations of uncertainties, e.g., from different groups which
provide different uncertainty estimates for the same
observable. Assume for definiteness that
the two groups provide the probability
distribution  
for an observable ${\cal O}$ as $p_1({\cal O})$ and $p_2({\cal O})$,
for example by saying that the distributions are gaussian and
providing their means and standard deviations $m_i$ and $\sigma_i$.
In the case of statistical uncertainties, two attitudes are possible:
%--
\begin{itemize}
\item The different determinations differ due to  statistical
  reasons.
In such case, the best value is found as the weighted
  average. In the above example, the combined determination $\bar p({\cal O})$
is a gaussian, with mean $m$ equal to the weighted average of the means
of the two starting distributions and standard deviation equal to the 
standard deviation of the mean,
$\sigma^2=\frac{\sigma_1^2+\sigma_2^2}{2}$. In this case, the error on
the combined determination is always smaller than the error on each of
the determinations that go into it.
\item The different determinations are exclusive, i.e.\ either one 
  or the other is correct, and they should be
  combined in a Bayesian way by assigning an a priori reliability to
  each of them. In this case  the combined
  probability is 
$\bar p({\cal O})=\frac{p_1({\cal O})+p_2({\cal O})}{2}$, and
  the error on
the combined determination
is not necessarily smaller, and in fact typically larger
  than the error of each of the determinations that go into it.
Indeed,  in practice, unless the
  probability distributions that are being combined are very
  inconsistent (e.g.\ if their respective means differ by many standard
  deviations) this Bayesian combination is very close to the envelope
  of the distributions which are being combined (compare the method
  for the combination of PDF
  uncertainties discussed in Section~\ref{pdfsection}).
\end{itemize}
%--
The case of theoretical uncertainties is rather less obvious and it
will be discussed in the next section.
%--
\subsubsection{Possibilities for option combination}
%--
Consider a given observable ${\cal O}$ whose calculation is characterized by a set
of options $\{o_1,\,\dots\,,o_n\}$. A typical result, showing all components of
THU will be as follows:
%--
\bq
{\cal O} = {\cal O}_{\rm c}\,{}^{+\,o^{\rm max}_1}_{-\,o^{\rm min}_1}\,[ o_1 ]\,
\cdots\,{}^{+\,o^{\rm max}_n}_{-\,o^{\rm min}_n}\,[ o_n ].
\eq
%--
The question is on the combination of different sources. There are three options:
%--
\bei
\item[L)] Linear combination:
%--
\bq
{\cal O} = {\cal O}_{\rm c}\,{}^{+\,\Delta^{\ssL}_+ {\cal O}}_{-\,\Delta^{\ssL}_- {\cal O}},
\qquad
\Delta^{\ssL}_+ {\cal O} = \sum_{i=1}^n o^{\rm max}_i,
\quad
\Delta^{\ssL}_- {\cal O} = \sum_{i=1}^n o^{\rm min}_i,
\eq
%--
\item[Q)] Quadratic combination:
%--
\bq
{\cal O} = {\cal O}_{\rm c}\,{}^{+\,\Delta^{\ssQ}_+ {\cal O}}_{-\,\Delta^{\ssQ}_- {\cal O}},
\qquad
\Delta^{\ssQ}_+ {\cal O} = \Bigl[ \sum_{i=1}^n \bigl( o^{\rm max}_i\bigr)^2\Bigr]^{1/2},
\quad
\Delta^{\ssQ}_- {\cal O} = \Bigl[ \sum_{i=1}^n \bigl( o^{\rm min}_i\bigr)^2\Bigr]^{1/2},
\eq
%--
\item[E)] Envelope combination:
%--
\bq
{\cal O} = {\cal O}_{\rm c}\,{}^{+\,\Delta^{\ssE}_+ {\cal O}}_{-\,\Delta^{\ssE}_- {\cal O}},
\quad
\Delta^{\ssE}_+ {\cal O} = \max_{o_1,\,\dots\,,o_n}\,{\cal O}\lpar \{o\}\rpar -
{\cal O}_{\rm c},
\quad
\Delta^{\ssE}_- {\cal O} = {\cal O}_{\rm c} - 
\min_{o_1,\,\dots\,,o_n}\,{\cal O}\lpar \{o\}\rpar,
\eq
%--
\end{itemize}
%--
where ``c'' refers to the preferred setup for all options. A loop over all options
that are not correlated is indeed needed, all options that are independent should
be varied simultaneously. 
When a specific set of PDFs is used, it should be kept fixed at its
central value when computing the various THU. The PDF uncertainty
(which is not a THU as discussed above) is computed along with other statistical 
and parametric uncertainties.
%--
Let us now assume that all options correspond to uncertainties which
are known to be THU. Clearly addition in quadrature is then not
appropriate. In an ideal case, all sources of THU should be recorded, with
correlated options not be varied independently but rather in the
correlated way discussed previously. The final ensuing uncorrelated
THUs can then be just combined linearly. Moreover, in the future, 
combination can be repeated when some uncertainty is reduced or some improved
strategy is found. To this purpose, all authors should provide 
information on each source of THU.

Unfortunately, this is not usually done, so, lacking 
detailed information, the problem of combining uncertainties arises.
To be more concrete, let us consider an observable ${\cal O}$ and two different 
predictions, ${\cal O}_{\ssA}$ and ${\cal O}_{\ssB}$, both with asymmetric 
error (with for definiteness ${\cal O}_{\ssA}>{\cal O}_{\ssB}$). The precise 
meaning of error here is not obvious; however, we can assume that
an error of $\pm\Delta_{\ssA}$ means that the observable has a constant
probability of being in the range ${\cal O}_{\ssA}-\Delta_{\ssA} < {\cal O} <
{\cal O}_{\ssA}+\Delta_{\ssA}$ (with the obvious generalization when the error is
asymmetric). 
Note that the standard deviation of such a probability
distribution is equal to $\sigma=\frac{\Delta}{\sqrt{3}}$.
The rationale for this choice is that if a calculation is performed by using the 
state-of-art of the present technology, the meaning of the error band
is then: {\em I don't know about higher-order 
effects, I haven't computed them, but I know that it is almost impossible 
that they will change my result more than what I have indicated}. Thus, the true 
result should be within the shown interval.

Given two predictions ${\cal O}_{\ssA,\ssB}$ with asymmetric errors
$\Delta A_{\pm}$ and $\Delta B_{\pm}$ the central value (this result rests on the 
assumption that the upper limit is due to $A$ and the lower due to $B$) can be defined 
as
%--
\bq
\langle{\cal O}\rangle = \frac{1}{2}\,\Bigl( {\cal O}_{\ssA} + {\cal O}_{\ssB} 
+ \Delta B_+ - \Delta A_-\Bigr)
\label{bestcenter}
\eq
%--
i.e.\ at the centre of the overlapping band (or at the centre of the gap in case
of no overlap). If the two uncertainties
are treated as completely independent and they are added linearly, the
width of the band then is 
%--
\begin{equation}
W= \Delta A_+ + \Delta A_- + \Delta B_+ + \Delta B_-.
\label{bigband}
\end{equation}
%--
This is a very conservative estimate, which contradicts
the above philosophy according to which the two intervals 
$\Delta A_+ + \Delta A_-$ and $\Delta B_+ + \Delta B_-$ should already 
be ``maximal'' ranges of variation. Furthermore, it neglects the fact that 
in practice the ranges of variation given by
different authors will include several common effects. 

%One could then alternatively argue in the following way: The two determinations
%provide each a maximal range of variation, however the two different
%estimates of the range of variation include some common effects. The
%total range of variation should then be smaller than $W$,
%Eq.~(\ref{bigband}), to account for these common effects. 
%One possibility is that  $A$ does not include the 
%$\beta\,$-effect and estimates the corresponding uncertainty, but includes the
%$\alpha\,$-effect. The opposite for $B$. In this case, the joint
%determination will include both the $\alpha$- and $\beta$-effects, so
%one should subtract from the uncertainty band the width
%%--
%\bq
%d = \max\{\, 0, \, {\cal O}_{\ssB} - {\cal O}_{\ssA} + \Delta B_+ + \Delta A_- \}
%\label{bigoverlap}
%\eq
%%--
%of the  region of overlap of the two bands, thereby getting an uncertainty band
%with width
%%--
%\bq
%W^\prime= W - d,
%\label{smallband}
%\eq
%%--
%with $W$ and $d$ given by Eqs.~(\ref{bigband}) and~(\ref{bigoverlap}),
%respectively.
%This procedure becomes questionable if it turns out that
%the overlap is maximal, i.e.\ the two predictions have
%congruent central values and similar error bands. In this case, the
%width of the band $W^\prime$ Eq.~(\ref{smallband}) becomes very small.
%If it is thought that this smallness reflects a genuine progress in
%understanding due to having included both effects $\alpha$ and
%$\beta$, then a much better solution exists: could $A$ and $B$, please, include 
%both $\alpha$- and $\beta$-effect since they are documented in the literature? 
%
%If this is not possible (for example if 
%$B$ considers the $\alpha\,$-effect to be wrong or
%questionable), a more conservative approach than that leading to
%Eq.~(\ref{smallband}) seems advisable. Namely, we assume that the
%possible inclusion of common effects is responsible for the fact that
%the two bands overlap, and the difference in central values is due to
%effects not included in both determinations. 
%We then take as the width of the band
%--
%\bq
%W^{\prime\prime}= \Delta A_+ + \Delta B_- + 
%\Bigl( {\cal O}_{\ssA} - {\cal O}_{\ssB}\Bigr),
%\label{envband}
%\eq
%%--
%i.e.\ the envelope of the two bands
%(again making the assumption that the upper limit 
%is due to $A$ and the lower due to $B$). 
%If the bands do overlap, this
%estimate is less conservative than Eq.~(\ref{bigband}) but more
%conservative than Eq.~(\ref{smallband}), and it seems a reasonable
%compromise. 
%------------------
%One could then alternatively argue in the following way: the two determinations
%provide each a maximal range of variation, however the two different
%estimates of the range of variation include some common effects. The
%total range of variation should then be smaller than $W$,
%Eq.~(\ref{bigband}), to account for these common effects. 
%One possibility is that  $A$ does not include the 
%$\beta\,$-effect and estimates the corresponding uncertainty, but includes the
%$\alpha\,$-effect. The opposite for $B$. 
%If it is thought that a smaller $W$ reflects a genuine progress, then a  
% solution exists: could $A$ and $B$, please, include  both $\alpha$- and 
%$\beta$-effect since they are documented in the literature? 
%Sometimes this is not possible, for example if $B$ considers the 
%$\alpha\,$-effect to be wrong or questionable.
%
%A more conservative approach is possible, one should subtract from the 
%uncertainty band the width
%%--
%\bq
%d = \max\{\, 0, \, {\cal O}_{\ssB} - {\cal O}_{\ssA} + \Delta B_+ + \Delta A_- \}
%\label{bigoverlap}
%%\eq
%%--
%of the  region of overlap of the two bands, thereby getting an uncertainty band
%with width
%%--
%\bq
%W^\prime= W - d,
%\label{smallband}
%\eq
%%--
%with $W$ and $d$ given by Eqs.~(\ref{bigband}) and~(\ref{bigoverlap}),
%respectively.
%
%Eq.~(\ref{smallband}) seems advisable. Namely, we assume that the
%possible inclusion of common effects is responsible for the fact that
%the two bands overlap, and the difference in central values is due to
%effects not included in both determinations. Alternatively, we can say that
%the width of the band is
%%--
%\bq
%W^{\prime\prime}= \Delta A_+ + \Delta B_- + 
%\Bigl( {\cal O}_{\ssA} - {\cal O}_{\ssB}\Bigr),
%\label{envband}
%\eq
%%--
%i.e.\ the envelope of the two bands (again making the assumption that the 
%upper limit is due to $A$ and the lower due to $B$). 
%In case the $A$ and $B$ bands overlap or at least touch each other,
%we have $W^\prime= W^{\prime\prime}$.
%--------------------------------------------------------------------
% Stefano version
%--------------------------------------------------------------------
One could then alternatively argue in the following way: the two determinations
provide each a maximal range of variation, however the two different
estimates of the range of variation include some common effects. The
total range of variation should then be smaller than $W$, Eq.~(\ref{bigband}), 
to account for these common effects. 
For example, suppose  $A$ does not include the $\beta\,$-effect and estimates 
the corresponding uncertainty, but includes the $\alpha\,$-effect. The 
opposite for $B$. 
If it is thought that a smaller $W$ reflects a genuine progress, then
an ideal solution would be that $A$ and $B$ include  both the $\alpha$- and 
$\beta$-effects. However, sometimes this is not possible, for example if 
$B$ considers the $\alpha\,$-effect to be wrong or questionable. 
Even so, a less conservative than just using Eq.~(\ref{bigband}) is possible. 
Namely, assuming that the possible inclusion of common effects is responsible 
for the fact that the two bands overlap, and the difference in central values 
is due to effects not included in both determinations, one should subtract 
from the uncertainty band the width
%--
\bq
d = \max\{\, 0, \, {\cal O}_{\ssB} - {\cal O}_{\ssA} 
               + \Delta B_+ + \Delta A_- \}
\label{bigoverlap}
\eq
%--
of the  region of overlap of the two bands, thereby getting an uncertainty band
with width
%--
\bq
W^\prime= W - d.
\label{smallband}
\eq
%--
If the bands have a nonzero overlap, so $d> 0$, $W^\prime$ Eq.~(\ref{smallband}) is 
just the envelope of the two bands, namely $W^\prime= W^{\prime\prime}$, with.
%--
\bq
W^{\prime\prime}= \Delta A_+ + \Delta B_- + 
\Bigl( {\cal O}_{\ssA} - {\cal O}_{\ssB}\Bigr).
\label{envband}
\eq
%--
If the bands do not overlap
%, then $\Bigl( {\cal O}_{\ssA} - {\cal
% O}_{\ssB}\Bigr)> \Bigl(\Delta A_-+ \Delta B_+\Bigr)$, so 
the envelope Eq.~(\ref{envband}) is wider than the linear sum of the 
uncertainties Eq.~(\ref{bigband}): $W^{\prime\prime}>W$. In this case, the 
lack of overlap of the two bands suggests that either or both of
two determinations are missing some source of uncertainty, and the 
envelope prescription, which is now more conservative than the linear
sum, seems more advisable.

Hence we conclude that it is in general advisable to adopt
the envelope uncertainty  estimate Eq.~(\ref{envband}).
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
To formulate our recommendation in full generality we define
%--
\bqa
O_- &=& \max\,\{{\cal O}_{\ssB} - \Delta B_-\,,
\,{\cal O}_{\ssA} - \Delta A_-\},
\nl
O_+ &=& \min\,\{{\cal O}_{\ssB} + \Delta B_+\,,
\,{\cal O}_{\ssA} + \Delta A_+\},
\nl
E_- &=& \min\,\{{\cal O}_{\ssB} - \Delta B_-\,,
\,{\cal O}_{\ssA} - \Delta A_-\},
\nl
E_+ &=& \max\,\{{\cal O}_{\ssB} + \Delta B_+\,,
\,{\cal O}_{\ssA} + \Delta A_+\}.
\label{genDEF}
\eqa
%--
Our recommendation is thus to use as best prediction for the observable the
central value
%--
\bq
\langle{\cal O}\rangle = \frac{1}{2}\,\Bigl( O_+ + O_-\Bigr),
\label{finalCV}
\eq
%--
with an uncertainty band with envelope width
%--
\bq
W^{\prime\prime} = E_+ - E_-
\eq
%--
[generalization of Eq.~(\ref{envband})], namely:
%--
\bq
{\cal O} = \;\langle{\cal O}\rangle^{+\{E_+ - \langle{\cal O}\rangle\}}_
                            {-\{\langle{\cal O}\rangle - E_-\}}.
\label{finalrec}
\eq
%--
%O = \;\langleO\rangle^{+\{O_{\ssA} - \langleO\rangle + \Delta A_+ \}}_{-\{\langleO\rangle -
%O_{\ssB} + \Delta B_- \}}.
%\label{finalrec}
%\eq
%--
Finally, we note that a similar conclusion is reached if we assume
that the two determinations under discussion should be taken as
exclusive and with equal a priori probability. Indeed if $p_{\ssA}({\cal O})$ and
$p_{\ssB}({\cal O})$ are two flat probability distributions for the observable 
${\cal O}$,
then the combined distribution
$\bar p({\cal O})=(p_{\ssA}({\cal O})+p_{\ssB}({\cal O}))/2$ 
has an effect very similar 
%is very close 
to a flat distribution with width equal to the envelope of $p_{\ssA}({\cal O})$ and
$p_{\ssB}({\cal O})$, as long as the two bands at least touch each other. 
%---
% Ansgar
%--
%To see this, note
%that the combined distribution coincides with the envelope in the two limiting 
%cases in which $p_{\ssA}({\cal O})$ and $p_{\ssB}({\cal O})$ coincide, or 
%$p_{\ssA}({\cal O})$ and $p_{\ssB}({\cal O})$ just touch each other (i.e.\ 
%${\cal O}_{\ssB}+\Delta  B_+={\cal O}_{\ssA}-\Delta A_-$).  

If the starting determinations distributions are inconsistent, i.e.\ 
${\cal O}_{\ssB}+\Delta  B_+ \muchless {\cal O}_{\ssA}-\Delta A_-$ none of 
these methods seems adequate, and in such case one should question the 
reliability of the results which are being combined. In all other cases, we 
conclude that the envelope method Eq.~(\ref{finalrec}) provides a conservative 
but not overly conservative way of combining THU, though it could overestimate 
a bit the combined THU.
%--
\subsubsection{Conclusions}
%--
The concept of THU and its use require few basic rules and an agreement within
the community:
%--
\begin{itemize}

\item Sets of options in different calculations should be homogeneous; if one
calculation includes a {\em new} option its physical origin should be
motivated and its inclusion accepted, in which case all codes should include it.
If a calculation is based on options that inflate (or deflate) the THU without
a general consensus or a solid theoretical basis, it should not be included in 
the average. Controversial assumptions should be put on a waiting list and 
included only when the issue is fully clarified. Our recommendation for central 
value is the midpoint of the overlapping region, \eqn{finalCV}.

\item If different calculations include homogeneous sets of options the
difference between central values should be considered with particular care, 
unless the central value itself reflects a specific choice for the 
{\em preferred setup} with different choices in different calculations. If 
all options, including the preferred setup, are congruent then differences 
in the central values cannot be justified by THU.

\end{itemize}

%--
To summarize our recommendations we suggest that:

\begin{itemize}

\item Parametric uncertainties that are distributed according to a 
known (usually Gaussian) distribution should be added in quadrature.

\item For the choice of central QCD scales we are suggesting to use the 
last two known orders for the search of stability and for
minimizing corrections (if reasonable), which is -- at best -- a 
{\em rule of thumb}. 
The corresponding theoretical uncertainty, which of course should be assessed 
by investigating the highest known order, 
is arguably distributed according to a flat distribution.
Problems related to incompatible data are more the rule than an exception
for THU and, in principle, THUs should be considered case by case; this is
this is particularly true whenever the two error bands are far apart,
%--
% Ansgar
%--
%particularly true whenever the error of calculation $A$ is much larger than the
%one of calculation $B$
and also the envelope (the standard method for 
incompatible statistical uncertainties) becomes questionable. 
In order to formulate a global recommendation we suggest that THU should be 
combined according to the envelope method: therefore, define the central value 
according to \eqn{finalCV} with uncertainty given by \eqn{finalrec}.

\item One should keep in mind that there are additional sources of THU, e.g.\ 
the THU resulting from missing higher-order EW corrections, that cannot be 
estimated via scale uncertainties. Therefore scale variation uncertainties
(SU) are a relevant portion of the global THU but do not exhaust the THU.
It is our recommendation that all sources of THU, not only SU, and their origin 
should always be documented.

\item The way the total PU is then combined with the THU comes down to the best 
way of combining a Gaussian and flat distributions.
As general rule that is sufficiently conservative only the linear combination
of those errors can be recommended.

\item To stress our point let us repeat that the PDF + $\alphas$ (plus
other parametric uncertainties, such as heavy-quark masses) are added in
quadrature to each other (i.e.\ if one wants to add heavy-quark mass effects, 
this has to be done in quadrature to PDF + $\alphas$, which is already the sum 
in quadrature of PDF + $\alphas$), but then they are added {\em only once} at 
the end to the THU.
Thus if one has two different estimates of the PDF + $\alphas$ uncertainty, 
$A$ and $B$, the recommendation is on averaging these two estimates (which is the
same as the uncertainty on two fully correlated measurements) before combining with the 
THU.
%As a matter of fact, we prefer a final recipe, more consistent with 
% the point of view of providing a conservative error estimate: instead of 
% averaging the two estimates we suggest to use their maximum.
A remaining source of uncertainty, i.e. the scale dependence of PDF, cannot be
estimated at present.

\end{itemize}





























