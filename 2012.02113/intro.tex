\chapter*{Introduction}


This book was born of research in category theory, brought to life by the
ongoing vigorous debate on how to quantify biological diversity, given
strength by information theory, and fed by the ancient field of functional
equations.  It applies the power of the axiomatic method to a biological
problem of pressing concern, but it also presents new advances in `pure'
mathematics that stand in their own right, independently of any
application.

The starting point is the connection between diversity and entropy.  We
will discover:
% 
\begin{itemize}
\item
how Shannon entropy, originally defined for communications engineering, can
also be understood through biological diversity (Chapter~\ref{ch:shannon});
% (Section~\ref{sec:ent-div});

\item
how deformations of Shannon entropy express
a spectrum of viewpoints on the meaning of biodiversity
(Chapter~\ref{ch:def});

\item
how these deformations \emph{provably} provide the only reasonable
abundance-based measures of diversity (Chapter~\ref{ch:value});
% (Section~\ref{sec:total-hill});

\item
how to derive such results from characterization theorems for the
power means, of which we prove several, some new (Chapters~\ref{ch:mns}
and~\ref{ch:prob}). 
% Section~\ref{sec:mult-means}). 
\end{itemize}
% 
Complementing the classical techniques of these proofs is a large-scale
categorical programme, which has produced both new mathematics and new
measures of diversity now used in scientific applications.  For example, we
will find:
% 
\begin{itemize}
\item
that many invariants of size from across the breadth of mathematics
(including cardinality, volume, surface area, fractional dimension, and
both topological and algebraic notions of Euler characteristic) arise
from one single invariant, defined in the wide generality of enriched
categories (Chapter~\ref{ch:sim});
% (Sections~\ref{sec:mag} and~\ref{sec:mag-geom});

\item
a way of measuring diversity that reflects not only the varying abundances
of species (as is traditional), but also the varying similarilities between
them, or, more generally, any notion of the values of the species
(Chapters~\ref{ch:sim} and~\ref{ch:value});

\item
that these diversity measures belong to the extended family of measures
of size (Chapter~\ref{ch:sim});
% (Sections~\ref{sec:mag} and~\ref{sec:mag-geom});

\item
a `best of all possible worlds': an abundance distribution on any given set
of species that maximizes diversity from an infinite number of viewpoints
simultaneously (Chapter~\ref{ch:sim});
% (Section~\ref{sec:max});

\item
an extension of Shannon entropy from its classical context of finite sets
to distributions on a metric space or a graph (Chapter~\ref{ch:sim}),
obtained by translating the similarity-sensitive diversity measures into
the language of entropy.
\end{itemize}
% 
Shannon entropy is a fundamental concept of information theory, but
information theory contains many riches besides.  We will mine them,
discovering:
% 
\begin{itemize}
\item
how the concept of relative entropy not only touches subjects from
Bayesian inference to coding theory to Riemannian geometry, but also
provides a way of quantifying local diversity within a larger context
(Chapter~\ref{ch:rel});

\item
quantitative methods for identifying particularly unusual or atypical parts of
an ecological community (Chapter~\ref{ch:mm}, drawing on work of Reeve
et al.~\cite{HPD}).
\end{itemize}
% 
The main narrative thread is modest in its mathematical prerequisites.  But
we also take advantage of some more specialized bodies of knowledge (large
deviation theory, the theory of operads, and the theory of finite fields),
establishing:
% 
\begin{itemize}
\item
how probability theory can be used to solve functional equations
(Chapter~\ref{ch:prob}, following work of Aubrun and Nechita~\cite{AuNe});  

\item
a streamlined characterization of information loss, as a natural consequence
of categorical and operadic thinking (Chapters~\ref{ch:loss} and~\ref{ch:cat});

\item
that the concept of entropy is (provably) inescapable even in the
pure-mathematical heartlands of category theory, algebra and topology,
quite separately from its importance in scientific
applications (Chapter~\ref{ch:cat});

\item
the right definition of entropy for probability distributions whose
`probabilities' are elements of the ring $\Zp$ of integers modulo a
prime~$p$ (Chapter~\ref{ch:p}, drawing on work of
Kontsevich~\cite{KontOHL}). 
\end{itemize}
% 
The question of how to quantify diversity is far more mathematically
profound than is generally appreciated.  This book makes the case that the
theory of diversity measurement is fertile soil for new
mathematics, just as much as the neighbouring but far more thoroughly
worked field of information theory.

\introbreak

What \emph{is} the problem of quantifying diversity?%
% 
\index{diversity measure}%
\index{diversity}
% 
Briefly, it is to take a biological community and extract from it a
numerical measure of its `diversity' (whatever that should mean).
% 
This task is certainly beset with practical problems: for instance, field
ecologists recording woodland animals will probably observe the noisy, the
brightly-coloured and the gregarious more frequently than the quiet, the
camouflaged and the shy.  There are also statistical difficulties: if a
survey of one community finds $10$ different species in a sample of $50$
individuals, and a survey of another finds $18$ different species in a
sample of $100$, which is more diverse?

However, we will not be concerned with either the practical or the
statistical difficulties.  Instead, we will focus on a fundamental
conceptual problem: in an ideal world where we have complete, perfect data,
how can we quantify diversity in a meaningful and logical way?

In both the news media and the scientific literature, the most common
meaning given to the word `diversity' (or `biodiversity') is simply the
number of species present.  Certainly this is an important quantity.
However, it is not always very informative.  For instance, the number of
species of great ape\index{apes} on the planet is~$8$
(Example~\ref{eg:hill-apes}), but $99.99$\% of all great apes belong to
just one species: us.  In terms of global ecology, it is arguably more accurate
to say that there is effectively only one species of great ape.

An example illustrates the spectrum of possible interpretations of the
concept of diversity.  Consider two bird\index{birds} communities:
\[
\lbl{p:intro-birds}
\lengths
\begin{picture}(120,55)(0,2)
% 
% In what follows, each bird is 8\unitlength high, and \unitlength=0.9mm,
% so it's 7.2mm high. Now each bird is 214x184 pixels, so that's 184/7.2 =
% 25.555... pixels/mm, i.e. 25.555... x 25.4 = 649.111... pixels/inch. That
% exceeds the 600 dpi requirement.
% 
\cell{10}{8}{bl}{\includegraphics[height=8\unitlength]{birdF_std.png}}
\cell{10}{16.2}{bl}{\includegraphics[height=8\unitlength]{birdF_std.png}}
\cell{10}{24.4}{bl}{\includegraphics[height=8\unitlength]{birdF_std.png}}
\cell{10}{32.6}{bl}{\includegraphics[height=8\unitlength]{birdF_std.png}}
\cell{10}{40.8}{bl}{\includegraphics[height=8\unitlength]{birdF_std.png}}
\cell{10}{49}{bl}{\includegraphics[height=8\unitlength]{birdF_std.png}}
\cell{20.5}{8}{bl}{\includegraphics[height=8\unitlength]{birdA_std.png}}
\cell{31}{8}{bl}{\includegraphics[height=8\unitlength]{birdG_std.png}}
\cell{41.5}{8}{bl}{\includegraphics[height=8\unitlength]{birdB_std.png}}
\cell{30}{2}{b}{A}
\cell{70}{8}{bl}{\includegraphics[height=8\unitlength]{birdF_std.png}}
\cell{70}{16.2}{bl}{\includegraphics[height=8\unitlength]{birdF_std.png}}
\cell{70}{24.4}{bl}{\includegraphics[height=8\unitlength]{birdF_std.png}}
\cell{80.5}{8}{bl}{\includegraphics[height=8\unitlength]{birdA_std.png}}
\cell{80.5}{16.2}{bl}{\includegraphics[height=8\unitlength]{birdA_std.png}}
\cell{80.5}{24.4}{bl}{\includegraphics[height=8\unitlength]{birdA_std.png}}
\cell{91}{8}{bl}{\includegraphics[height=8\unitlength]{birdG_std.png}}
\cell{91}{16.2}{bl}{\includegraphics[height=8\unitlength]{birdG_std.png}}
\cell{91}{24.4}{bl}{\includegraphics[height=8\unitlength]{birdG_std.png}}
\cell{85}{2}{b}{B}
\end{picture}
\]
In community~A, there are four species, but the majority of individuals
belong to a single dominant species.  Community~B contains the first three
species in equal abundance, but the fourth is absent.  Which community,
A~or~B, is more diverse?

One viewpoint%
%
\index{viewpoint!diversity@on diversity} 
% 
is that the presence of \emph{species} is what matters.  Rare
species count for as much as common ones: every species is precious.  From
this viewpoint, community~A is more diverse, simply because more species
are present.  The abundances of species are irrelevant; presence or absence
is all that matters.

But there is an opposing viewpoint that prioritizes the balance of
\emph{communities}.  Common species are important; they are the ones that
exert the most influence on the community.  Community~A has a single very
common species, which has largely outcompeted the others, whereas
community~B has three common species, evenly balanced.  From this
viewpoint, community~B is more diverse.

These two viewpoints are the two ends of a continuum.  More precisely,
there is a continuous one-parameter family $(D_q)_{q \in [0, \infty]}$ of
diversity measures encoding this spectrum of viewpoints.  Low values of $q$
attach high importance to rare species; for example, $D_0$ measures
community~A as more diverse than community~B.  When $q$ is high, $D_q$ is
most strongly influenced by the balance of more common species; thus,
$D_\infty$ judges~B to be more diverse.  No single viewpoint is right or
wrong.  Different scientists adopt different viewpoints (that is, different
values of $q$) for different purposes, as the literature amply attests
(Examples~\ref{egs:hill}).

Long ago, it was realized that the concept of diversity is closely related
to the concept of entropy.  Entropy appears in dozens of guises across
dozens of branches of science, of which thermodynamics is probably the most
famous.  (The introduction to Chapter~\ref{ch:shannon} gives a long but
highly incomplete list.)  The most simple incarnation is Shannon entropy,
which is a real number associated with any probability distribution on a
finite set.  It is, in fact, the logarithm of the diversity measure $D_1$.
Most often, Shannon entropy is explained and understood through the theory
of coding; indeed, we provide such an explanation here.  But the diversity
interpretation provides a new perspective.

For example, the diversity measures $D_q$, known in ecology as the
Hill%
%
\index{Hill number}
%
numbers, are the exponentials of what information theorists know as the
R\'enyi%
%
\index{Renyi entropy@R\'enyi entropy} 
% 
entropies.  From the very beginning of information theory, an
important role has been played by characterization theorems: results
stating that any measure (of information, say) satisfying a list of
desirable properties must be of a particular form (a scalar
multiple of Shannon entropy, say).  But what counts as a desirable
property depends on one's perspective.  We will prove that the Hill numbers
$D_q$ are, in a precise sense, the only measures of diversity with
certain natural properties (Theorem~\ref{thm:total-hill}).  This theorem
translates into a new characterization of the R\'enyi entropies, but it is
not one that necessarily would have been thought of from a purely
information-theoretic perspective.  

However, something is missing.  In the real world, diversity is understood
as involving not only the number and abundances of the species, but also
how \emph{different} they are.  (For example, this affects
conservation\index{conservation} policy; see the OECD quotation on
p.~\pageref{p:oecd-quote}.)  We describe the remedy in
Chapter~\ref{ch:sim}, defining a family of diversity measures that take
account of the varying similarity%
%
\index{similarity!species@of species} 
% 
between species, while still incorporating the
spectrum of viewpoints discussed above.  This definition unifies into one
family a large number of the diversity measures proposed and used in the
ecological and genetics literature.

This family of diversity measures first appeared in a paper in
\emph{Ecology}~\cite{MDISS}, but it can also be understood and motivated
from a purely mathematical perspective.  The classical R\'enyi entropies
are a family of real numbers assigned to any probability distribution on a
finite \emph{set}.  By factoring in the differences or distances
between points (species), we extend this to a family of real numbers
assigned to any probability distribution on a finite
\emph{metric\index{metric!space} space}.
In the extreme case where $d(x, y) = \infty$ for all distinct points $x$
and $y$, we recover the R\'enyi entropies.  In this way, the
similarity-sensitive diversity measures extend the definition of R\'enyi
entropy from sets to metric spaces.

Different values of the viewpoint parameter $q \in [0, \infty]$ produce
different judgements on which of two distributions is the more diverse.
But it turns out that for any metric space (or in biological terms, any
set of species), there is a single distribution that maximizes%
%
\lbl{p:max-intro}
% 
diversity from all viewpoints simultaneously.  For a generic finite metric
space, this maximizing distribution is unique.  Thus, almost every finite
metric space carries a canonical probability distribution (not usually
uniform).  The maximum%
%
\index{maximum diversity} 
% 
diversity itself is also independent of $q$, and is therefore a numerical
invariant of metric spaces.  This invariant has geometric significance in
its own right (Section~\ref{sec:mag-geom}).

We go further.  One might wish to evaluate an ecological community in a way
that takes into account some notion of the values\index{value} of the
species (such as 
phylogenetic distinctiveness).  Again, there is a sensible family of
measures that does this job, extending not only the similarity-sensitive
diversity measures just described, but also further measures already
existing in the ecological literature.  The word `sensible' can be made
precise: as soon as we subject an abstract measure of the value of a
community to some basic logical requirements, it is forced to belong to a
certain one-parameter family $(\sigma_q)$ (Theorem~\ref{thm:val-char}),
which are essentially the R\'enyi \emph{relative} entropies.

Information theory also helps us to analyse the diversity of
metacommunities,\index{metacommunity} that is, ecological communities made
up of a number of smaller communities such as geographical regions.  The
established notions of relative entropy, conditional entropy and mutual
information provide meaningful measures of the structure of a metacommunity
(Chapter~\ref{ch:mm}).  But we will do more than simply translate
information theory into ecological language.  For example, the new
characterization of the R\'enyi entropies mentioned above is a byproduct of
the characterization theorem for measures of ecological value.  In this
way, the theory of diversity gives back to information theory.

\introbreak

The scientific importance of biological diversity goes far beyond the
obvious setting of conservation\index{conservation} of animals and plants.
Certainly such conservation efforts are important, and the need for
meaningful measures of diversity is well-appreciated in that context.  For
example, Vane-Wright%
%
\index{Vane-Wright, Richard} 
% 
et al.~\cite{VWHW} wrote thirty years ago of the `agony of choice'
in conservation of flora and fauna, and emphasized how crucial it is to use
the right diversity measures.

But most life is microscopic.  Nee~\cite{NeeMTM}%
%
\index{Nee, Sean} 
% 
argued in 2004 that
% 
\begin{quote}
{}[w]e are still at the very beginning of a golden age of biodiversity
discovery, driven largely by the advances in molecular biology and a new
open-mindedness about where life might be found,%
%
\index{microbial systems}
\end{quote}
% 
and that
% 
\begin{quote}
all of the marvels in biodiversity's new bestiary are invisible.
\end{quote}
% 
Even excluding exotic new discoveries of microscopic life, two recent lines
of research illustrate important uses of diversity measures at the
microbial level.

First, the extensive use of antimicrobial drugs on animals
unfortunate enough to be born into the modern meat industry is commonly
held to be a cause of antimicrobial resistance in pathogens affecting
humans.  However, a 2012 study of Mather%
%
\index{Mather, Alison} 
% 
et al.~\cite{MMMR} suggests that the causality may be more complex.  By
analysing the diversity of antimicrobial%
%
\index{antimicrobial resistance}%
\index{microbial systems}
% 
resistance in \emph{Salmonella} taken from animal populations on the one
hand, and from human populations on the other, the authors concluded that
the animal population is `unlikely to be the major source of resistance'
for humans, and that `current policy emphasis on restricting antimicrobial
use in domestic animals may be overly simplistic'.  The diversity measures
used in this analysis were the Hill numbers $D_q$ mentioned above and
central to this book.

Second, the increasing problem of obesity in humans has prompted research
into causes and treatments, and there is evidence of a negative
correlation between obesity and diversity of the gut%
%
\index{gut microbiome} 
% 
microbiome (Turnbaugh et al.~\cite{THYC,TQFM}).  Almost all traditional
measures of diversity rely on a division of organisms into species or other
taxonomic groups, but in this case, only a fraction of the microbial
species concerned have been isolated and classified taxonomically.
Researchers in this field therefore use DNA sequence data, applying
sophisticated but somewhat arbitrary clustering algorithms to create
artificial species-like groups (`operational taxonomic units').  On the
other hand, the similarity-sensitive diversity measures mentioned above and
introduced in Chapter~\ref{ch:sim} can be applied directly to the sequence
data, bypassing the clustering step and producing a measure of genetic
diversity.  A test case was carried out in Leinster and
Cobbold~\cite{MDISS} (Example~4), with results that supported the
conclusions of Turnbaugh et al.

Despite the wide variety of uses of diversity measures in biology, none of
the mathematics presented in this text is intrinsically biological.%
% 
\index{diversity measure!applications of}
% 
Indeed, the mathematics of diversity was being developed as early as 1912
by the economist\index{economics} Corrado
Gini~\cite{Gini}%
%
\index{Gini, Corrado} 
% 
(best known for the Gini
coefficient of disparity of wealth), and by the statistician Udny
Yule%
%
\index{Yule, G. Udny} 
% 
in the 1940s for the analysis of lexical%
%
\index{lexical diversity}%
\index{diversity!lexical}
% 
diversity in literature~\cite{Yule}.  Some of the diversity measures most
common in ecology have recently been used to analyse the ethnic and
sociological diversity of judges (Barton and Moran~\cite{BaMo}), and the
similarity-sensitive diversity measures that are the subject of
Chapter~\ref{ch:sim} have been used not only in multiple ecological
contexts (as listed after Example~\ref{eg:devries}), but also in
non-biological applications such as computer%
%
\index{computer network security} 
% 
network security (Wang et al.~\cite{WZJS}).% 
% 
\index{diversity measure!applications of}

In mathematical terms, simple diversity measures such as the Hill numbers
are invariants of a probability distribution on a finite set.  The
similarity-sensitive diversity measures are defined for any probability
distribution on a finite set with an assigned degree of similarity between
each pair of points.  (This includes any finite metric space or graph.)
The value measures are defined for any finite set equipped with a
probability distribution and an assignment of a nonnegative value to each
element.  The metacommunity measures are defined for any probability
distribution on the cartesian product of a pair of finite sets.  Much of
this text is written using ecological terminology, but the mathematics is
entirely general.\lbl{p:general}

\introbreak

This work grew out of a general category-theoretic%
%
\index{category theory} 
%
study of size.\index{size} In many parts of mathematics, there is a
canonical notion of the size of the objects of study: sets have
cardinality, vector spaces have dimension, subsets of Euclidean space have
volume, topological spaces have Euler%
%
\index{Euler characteristic}
%
characteristic, and so on.  Typically, such measures of size satisfy
analogues of the elementary inclusion-exclusion and multiplicativity
formulas for counting finite sets:
% 
\begin{align*}
\mg{X \cup Y}   &= \mg{X} + \mg{Y} - \mg{X \cap Y},     \\
\mg{X \times Y} &= \mg{X} \cdot \mg{Y}.
\end{align*}
% 
(The interpretation of Euler characteristic as the topological analogue of
cardinality is not as well known as it should be; this is an insight of
Schanuel%
%
\index{Schanuel, Stephen} 
% 
on which we elaborate in Section~\ref{sec:mag}.)  From a
categorical perspective, it is natural to seek a single invariant unifying
all of these measures of size.

Some unification is achieved by defining a notion of size for categories
themselves, called \emph{magnitude}\index{magnitude} or Euler
characteristic.  (Finiteness hypotheses are required, but will not be
mentioned in this overview.)  This definition already brings together
several established invariants of size~\cite{ECC}: cardinality of
sets, and the various notions of Euler characteristic for partially ordered
sets, topological spaces, and even orbifolds (whose Euler characteristics
are in general not integers).  The theory of magnitude of categories is
closely related to the theory of M\"obius--Rota inversion for partially
ordered sets~\cite{RotaFCT,NMI}.

But the decisive, unifying step is the generalization of the definition of
magnitude from categories to the wider class of \emph{enriched}%
%
\index{enriched category} 
% 
categories~\cite{MMS}, which includes not only categories
themselves, but also metric spaces, graphs, and the additive categories
that are a staple of homological algebra.

The definition of the magnitude of an enriched category unifies still more
established invariants of size.  For example, in the representation theory
of associative algebras, one frequently considers the indecomposable
projective modules, which form an additive category.  The magnitude of that
additive category turns out to be the Euler form of a certain canonical
module, defined as an alternating sum of dimensions of $\Ext$ groups
(equation~\eqref{eq:ip}).  Magnitude for enriched categories can also be
realized as the Euler characteristic of a certain Hochschild-like homology%
%
\index{magnitude!homology}
% 
theory of enriched categories, in the same sense that the Jones polynomial
for knots is the Euler characteristic of Khovanov homology~\cite{Khov}.
This was established in recent work led by Shulman~\cite{MHECMS}, building
on the case of magnitude homology for graphs previously developed by
Hepworth and Willerton~\cite{HeWi}. 

Since any metric\index{metric!space} space can be regarded as an enriched
category, the general definition of the magnitude of an enriched category
gives, in particular, a definition of the magnitude $\mg{X} \in \R$ of a
metric space $X$.  Unlike the other special cases just mentioned, this
invariant is essentially new.

Recent, increasingly sophisticated, work in analysis has connected
magnitude with classical invariants of geometric measure.  For example, for
a compact subset $X \sub \R^n$ satisfying certain regularity conditions, if
one is given the magnitude of all of the rescalings $tX$ of $X$ (for $t >
0$), then one can recover:
% 
\begin{itemize}
\item 
the Minkowski\index{dimension} dimension of $X$ (one of the principal
notions of fractional dimension), a result proved by Meckes using results
in potential theory (Theorem~\ref{thm:mink});

\item
the volume\index{volume} of $X$, a result proved by Barcel\'o and Carbery
using PDE methods (Theorem~\ref{thm:bc});

\item
the surface%
%
\index{surface area} 
%
area of $X$, a result proved by Gimperlein and Goffeng using
global analysis (or more specifically, tools for computing heat trace
asymptotics; Theorem~\ref{thm:gg}).   
\end{itemize}
% 
Gimperlein and Goffeng also proved an asymptotic
inclusion-exclusion%
%
\index{inclusion-exclusion principle}
%
principle:
\[
\mg{t(X \cup Y)} + \mg{t(X \cap Y)} - \mg{tX} - \mg{tY}
\to 0
\]
as $t \to \infty$, for sufficiently regular $X, Y \sub \R^n$
(Section~\ref{sec:mag-geom}).  This is another manifestation of the
cardinality-like nature of magnitude.

We have seen that every finite metric space $X$ has an unambiguous maximum
diversity $\Dmax{X} \in \R$, defined in terms of the similarity-sensitive
diversity measures (p.~\pageref{p:max-intro}).  We have also seen that $X$
has a magnitude $\mg{X} \in \R$.  These two real numbers are not in general
equal (ultimately because probabilities or species abundances are forbidden
to be negative),%
%
\index{negative!probability}
%
but they are closely related.  Indeed, $\Dmax{X}$ is always equal to the
magnitude of some \emph{subspace} of $X$, and in important families of
cases is equal to the magnitude of $X$ itself.  So, magnitude is closely
related to maximum diversity.  Indeed, this relationship was exploited by
Meckes%
%
\index{Meckes, Mark} 
% 
to prove the result on Minkowski dimension.

There is a historical surprise.  Although this author arrived at the
definition of the magnitude of a metric space by the route of enriched
category theory, it had already arisen in earlier work on the
quantification of biodiversity.  In 1994, the environmental scientists
Andrew Solow%
%
\index{Solow, Andrew} 
%
and Stephen\label{p:sp-mag} Polasky%
%
\index{Polasky, Stephen} 
%
carried out a probabilistic analysis of the benefits of high biodiversity
(\cite{SoPo}, Section~4), and isolated a particular quantity that they
called the `effective%
%
\index{effective number!species@of species}
% 
number of species'.  They did not
investigate it mathematically, merely remarking mildly that it `has
some appealing properties'.  It is exactly our magnitude.

\introbreak

Ecologists began to propose quantitative definitions of biological
diversity in the mid-twentieth century~\cite{SimpMD,WhitVSM}, setting in
motion more than sixty years of heated debate, with dozens of further proposed
diversity measures, hundreds of scholarly papers, at least one book devoted
to the subject~\cite{Magu}, and consequently, for some, despair (expressed
as early as 1971 in a famously-titled paper of Hurlbert~\cite{Hurl}).%
%
\index{Hurlbert, Stuart}  
% 
Meanwhile, parallel debates were taking place in genetics and other
disciplines.

The connections between diversity measurement on the one hand, and
information theory and category theory on the other, are fruitful for both
mathematics and biology.  But any measure of biological diversity must be
justifiable in purely biological terms, rather than by borrowing authority
from information theory, category theory, or any other field.  The
ecologist E.~C.~Pielou%
% 
\index{Pielou, Evelyn Chrystalla} 
% 
warned against attaching ecological significance to diversity measures for
anything other than ecological reasons:
% 
\begin{quote}
It should not be (but it is) necessary to emphasize that the object of
calculating indices of diversity is to solve, not to create, problems.  The
indices are merely numbers, useful in some circumstances but not in all.
[\ldots] Indices should be calculated for the light (not the shadow) they
cast on genuine ecological problems.
\end{quote}
% 
(\cite{PielME}, p.~293).

In a series of incisive papers beginning in 2006, the
conservationist and botanist Lou Jost%
%
\index{Jost, Lou}
%
insisted that whatever diversity measures one uses, they must exhibit
\emph{logical behaviour}%
%
\index{diversity measure!logical behaviour of} 
%
\cite{JostED,JostPDI,JostGST,JostMBD}.  For
example, Shannon entropy is commonly used as a diversity measure by
practising ecologists, and it does behave logically if one is only using it
to ask whether one community is more or less diverse than another.  But as Jost
observed, any attempt to reason about percentage changes in diversity using
Shannon entropy runs into logical absurdities: Examples~\ref{eg:plague}
and~\ref{eg:oil} describe the plague that exterminates $90\%$ of species
but only causes a $17\%$ drop in `diversity', and the oil drilling that
simultaneously destroys \emph{and} preserves $83\%$ of the `diversity' of
an ecosystem.  It is, in fact, the \emph{exponential} of Shannon entropy
that should be used for this purpose.

In this sense, origin stories are irrelevant.  Inventing new diversity
measures is easy, and it is nearly as easy to tell a story of how a new
measure fits with some intuitive idea of diversity, or to justify it in
terms of its importance in some related discipline.  But if a measure does
not pass basic logical tests (as in Section~\ref{sec:prop-hill}), it is
useless or worse.

Jost noted that all of the Hill numbers $D_q$ do behave logically.  Again,
we go further: Theorem~\ref{thm:total-hill} states that the Hill numbers
are in fact the \emph{only} measures of diversity satisfying certain
logically fundamental properties.  (At least, this is so for the simple
model of a community in terms of species abundances only.)  This is the
ideal of the axiomatic approach: to prove results stating that if one
wishes to have a measure with such-and-such properties, then it can only be
one of \emph{these} measures.

Mathematically, such results belong to the field of functional equations.  We
review a small corner of this vast and classical theory, beginning with the
fact that the only measurable functions $f \from \R \to \R$ satisfying the
Cauchy functional equation $f(x + y) = f(x) + f(y)$ are the linear mappings
$x \mapsto cx$.  Building on classical results, we obtain new axiomatic
characterizations of a variety of measures of diversity, entropy and value.
We also explain a new method, pioneered by Aubrun%
%
\index{Aubrun, Guillaume}
%
and Nechita%
%
\index{Nechita, Ion} 
%
in
2011~\cite{AuNe}, for solving functional equations by harnessing the power
of probability theory.  This produces new characterizations of the $\ell^p$
norms and the power means.

Characterization theorems for the power%
%
\index{power mean} 
%
means are, in fact, the engine of
this book (Chapter~\ref{ch:mns}).  By definition, the power mean of order
$t$ of real numbers $x_1, \ldots, x_n$, weighted by a probability
distribution $(p_1, \ldots, p_n)$, is
\[
M_t(\vc{p}, \vc{x}) 
=
\Biggl( \sum_{i = 1}^n p_i x_i^t \Biggr)^{1/t}.
\]
The power means $(M_t)_{t \in \R}$ form a one-parameter family of
operations, and the central place that they occupy in this text is explained
by their relationship with several other important one-parameter families:
the Hill numbers, the R\'enyi entropies, the $q$-logarithms, the
$q$-logarithmic entropies (also known as Tsallis entropies), the value
measures of Chapter~\ref{ch:value}, and the $\ell^p$-norms.  We will prove
characterization theorems for all of these families, in each case finding a
short list of properties that determines them uniquely.

\introbreak

Much of this text can be described as `mathematical%
% 
\index{mathematical anthropology} 
% 
anthropology'.  The mathematical anthropologist begins by observing that
some group of scientists attaches great importance to a particular object
or concept: homotopy theorists talk a lot about simplicial sets, harmonic
analysts constantly use the Fourier transform, ecologists often count the
number of species present in a community, and so on.  The next step is to
ask: why do they attach such importance to that particular thing, not
something slightly different?  Is it the \emph{only} object that enjoys the
useful properties that it enjoys?  If not, why do they use the object they
use, and not some other object with those properties?  And if it \emph{is}
the only object with those properties, can we prove it?  For
example, 2008 work of Alesker, Artstein-Avidan and Milman~\cite{AAAM}
proved that the Fourier transform is, in fact, the only transform that enjoys
its familiar properties.

This is the animating spirit of the field of functional equations.
But there is another field that has been
enormously successful in mathematical anthropology: category%
\index{category theory} 
% 
theory.  There, objects of mathematical interest are typically
characterized by universal%
%
\index{universal property} 
% 
properties.  For instance, the tensor product $M \otimes N$ of modules $M$
and $N$ is the universal module equipped with a bilinear map $M \times N
\to M \otimes N$; the Hilbert space completion $\hat{X}$ of an inner
product space $X$ is the universal Hilbert space equipped with an isometry
$X \to \hat{X}$; the real interval $[0, 1]$ is the universal bipointed
topological space equipped with a map $[0, 1] \to [0, 1] \vee [0, 1]$
(Theorem~2.2 of Leinster~\cite{GTSS} and Theorem~2.5 of
Leinster~\cite{GSSO}, building on results of Freyd~\cite{FreARA}).  Any
universal property involves uniqueness at two levels: the literal
uniqueness of a connecting \emph{map}, and the fact that the universal
property characterizes the \emph{object} possessing it uniquely up to
isomorphism.  Thus, category theory is a potent tool for proving
characterization theorems.

We demonstrate this with a categorically-motivated characterization theorem
for entropy (Baez, Fritz and Leinster~\cite{CETIL}).  Briefly put, the
probability distributions on finite sets form an operad\index{operad}, we
construct a certain universal category acted on by that operad, and this
leads naturally to the concept of Shannon entropy.  The categorical
approach amounts to a shift of emphasis from the entropy of a probability
space (an object) to the amount of information lost by a deterministic
process (a map).

The moral of this result is that entropy is not just something for applied
scientists.  It emerges inevitably from a general categorical machine,
given as its inputs nothing more obscure than the real line and the
standard topological simplices.  In other words, even in algebra and
topology, entropy is inescapable.

To demonstrate the strength of the axiomatic approach, we finish by
applying it to an entity of purely mathematical interest: entropy modulo a
prime number.  The topic was first introduced as a curiosity by
Kontsevich,%
%
\index{Kontsevich, Maxim} 
% 
as a byproduct of work on polylogarithms~\cite{KontOHL}.  Just as any real
probability distribution $\ppi = (\pi_1, \ldots, \pi_n)$ has a Shannon
entropy $H_\R(\ppi) \in \R$, one can define, for any
prime%
%
\index{entropy!modulo a prime} 
%
$p$ and `probabilities' $\pi_1, \ldots, \pi_n \in \Zp$, a kind of entropy
$H_p(\ppi) \in \Zp$.  The functional forms are quite different:
\[
\begin{array}{rcll}
H_\R(\pi_1, \ldots, \pi_n)      &
= &
\displaystyle
-\sum_{1 \leq i \leq n} \pi_i \log \pi_i
&\in \R, \\[1.5ex]
H_p(\pi_1, \ldots, \pi_n)       &
=&
\displaystyle
-\sum_{\substack{0 \leq r_1, \ldots, r_n < p\\ r_1 + \cdots + r_n = p}}
\frac{\pi_1^{r_1} \cdots \pi_n^{r_n}}{r_1! \cdots r_n!}
&\in \Zp.
\end{array}
\]
One would probably not guess that the second formula is the correct mod~$p$
analogue of the first.  However, the definition is fully justified by a
characterization theorem strictly analogous to the one that characterizes
real Shannon entropy.  And from the categorical perspective, there is a
strictly analogous characterization of information loss mod~$p$.  In short,
the apparatus developed for the real field can be successfully applied to
the field of integers modulo a prime.

\newpage
\introbreak

Finally, this book aims to challenge outdated conceptions of what
applied%
%
\index{applied mathematics} 
% 
mathematics can look like.  Too often, `applied mathematics' is
subconsciously understood to mean `methods of analysis applied to problems
of physics'.  (Or, worse, `applied' is taken to be a euphemism for
`unrigorous'.)  Those applications are certainly enormously important.
However, this excessively narrow interpretation ignores the glittering
array of applications of other parts of mathematics to other kinds of
problem.  It is mere historical accident that a researcher using PDEs in
the study of fluids is usually called an applied mathematician, but one
applying category theory to the design of programming languages is not.

Mathematicians are coming to appreciate that applications of their subject
to biology are enormously fruitful and, with the revolution in the
availability of genetic data, will only grow.  Mackey and Maini asked and
answered the question `What has mathematics done for
biology?'~\cite{MaMa},%
% 
\index{Mackey, Michael}%
\index{Maini, Philip}
% 
quoting the evolutionary biologist and slime mould specialist
John%
%
\index{Bonner, John} 
% 
Bonner on the `rocking back and forth between the reality of experimental
facts and the dream world of hypotheses'.  They reviewed some major
contributions, including striking success stories in ecology, epidemiology,
developmental biology, physiology, and neuro-oncology.  But still, most of
the work cited there (and most of mathematical biology as a whole) uses
parts of mathematics traditionally thought of as `applied', such as
differential equations, dynamical systems, and stochastic analysis.

The reality is that many parts of mathematics conventionally called `pure'
are now being successfully applied in diverse contexts, both biological and
otherwise.  Knot theory has solved longstanding problems in genetic
recombination (Buck and Flapan~\cite{BuFlPKC,BuFlTCK}).  Group theory has
illuminated virus structure (Twarock, Valiunas and Zappa~\cite{TVZ}).
Topological data analysis, founded on the theory of persistent%
%
\index{persistent homology} 
%
homology and
calling on the power of algebraic topology, succeeded in identifying a
hitherto unknown subtype of breast cancer with a 100\% survival rate
(Nicolau, Levine and Carlsson~\cite{NLC}; see Lesnick~\cite{Lesn} for an
expository account).  Order theory, topos theory and classical logic have
all been employed in the quest for improved ways of specifying, modelling
and designing concurrent systems (Nygaard and Winskel~\cite{NyWi}; Joyal,
Nielsen and Winskel~\cite{JNW}; Hennessy and Milner~\cite{HeMi}).  And,
famously, number theory is used to both provide and undermine security of
communications on the internet (Hales~\cite{HaleNBD}).  All of these are
real applications of mathematics.  None is `applied mathematics' as
traditionally construed.

But applications are not the only product of applied mathematics.  It also
\emph{nourishes} the core of mathematics, providing new questions, answers,
and perspectives.  Mathematics applied to physics has done this from
Archimedes to Newton to Witten.  Reed~\cite{Reed} lists dozens of ways in
which mathematics applied to biology is doing it now.  The developments
surveyed in this book provide further evidence that a body of mathematics
can simultaneously be entirely rigorous, be applied effectively to another
branch of science, use parts of mathematics that do not fit the narrow
stereotype of `applied mathematics', and produce new results that are
significant and satisfying from a purely mathematical aesthetic.




