\chapter{Entropy modulo a prime}
\lbl{ch:p}
\index{entropy!modulo a prime}


\begin{quote}
\emph{Conclusion:} If we have a random variable $\xi$ which takes
finitely many values with all probabilities in $\Q$ then we can define
not only the transcendental number $H(\xi)$ but also its `residues%
% 
\index{residue class}
% 
modulo $p$' for almost all primes $p$\,!  \quad%
%
\index{Kontsevich, Maxim}
%
\hfill -- Maxim Kontsevich~\cite{KontOHL}.
\end{quote}

\noindent
In this chapter, we define the entropy of any probability distribution
whose `probabilities' are not real numbers, but integers modulo a prime $p$.
Its entropy, too, is an integer mod~$p$.  We justify the definition by
proving a characterization theorem very similar to Faddeev's theorem on
real entropy (Theorem~\ref{thm:faddeev}), and by a characterization theorem
for information loss mod~$p$ that is also closely analogous to the real
case.

In earlier chapters, we reached our axiomatic characterization of real
information loss in three steps:
% 
\begin{list}{}{}
\item[\phantom{(III)}]\makebox[0em][r]{(I)} 
characterize the sequence $(\log n)_{n \geq 1}$
(Theorem~\ref{thm:erdos-liminf});  

\item[\phantom{(III)}]\makebox[0em][r]{(II)} 
using~(I), characterize entropy (Theorem~\ref{thm:faddeev});

\item[\phantom{(III)}]\makebox[0em][r]{(III)} 
using~(II), characterize information loss (Theorem~\ref{thm:cetil}).  
\end{list}
% 
Here, we follow three analogous steps to characterize entropy and
information loss modulo~$p$ (Sections~\ref{sec:p-defn}
and~\ref{sec:p-char}).  The analytic subtleties disappear, but instead we
encounter a number-theoretic obstacle.  

With the definition of entropy mod~$p$ in place, we implement the idea
proposed by Kontsevich in the quotation above.  That is, we define a sense
in which certain real numbers can be said to have residues mod~$p$
(Section~\ref{sec:p-res}).  The residue map establishes a direct
relationship between entropy over $\R$ and entropy over $\Zp$,
supplementing the analogy between the Faddeev-type theorems over $\R$ and
$\Zp$. 

We finish by developing an alternative but equivalent approach to entropy
modulo a prime (Section~\ref{sec:p-poly}).  It takes place in the ring of
polynomials over the field of $p$ elements.  It is related more closely
than the rest of this chapter to the subject of
polylogarithms\index{polylogarithm}, which formed the context of
Kontsevich's note~\cite{KontOHL} and of subsequent related work such as
that of Elbaz-Vincent and Gangl~\cite{EVGOPI,EVGFPM}.

The results of this chapter first appeared in~\cite{EMP}.  While~\cite{EMP}
seems to have been the first place where the theory of entropy mod~$p$ was
developed in detail, many of the ideas had been sketched or at least hinted
at in Kontsevich's note~\cite{KontOHL}, which itself was preceded by
related work of Cathelineau~\cite{CathSHS,CathRDP}.%
%
\index{Cathelineau, Jean-Louis}
% 
The introduction to Elbaz-Vincent and Gangl~\cite{EVGOPI} relates some of
the history, including the connection with polylogarithms; see also
Remark~\ref{rmk:kont-comp} below.


\section{Fermat quotients and the definition of entropy}
\lbl{sec:p-defn}


For the whole of this chapter, fix a prime $p$.  To avoid confusion between
the prime $p$ and a probability distribution $\p$, we now denote a typical
probability distribution by $\ppi = (\pi_1, \ldots, \pi_n)$.

Our first task is to formulate the correct definition of the entropy of a
probability%
%
\index{probability distribution!modulo a prime} 
% 
distribution $\ppi$ in which $\pi_1, \ldots, \pi_n$ are not real numbers,
but elements of the field $\Zp$ of integers modulo~$p$.

A problem arises immediately.  Real probabilities are ordinarily required to
be nonnegative, and the logarithms in the definition of entropy over $\R$
would be undefined if any probability were negative.  So in the familiar
real setting, the notion of positivity seems to be needed in order to state
a definition of entropy.  But in $\Zp$, there is no sense of positive or
negative.  How, then, are we to imitate the definition of entropy in
$\Zp$?

This problem is solved by a simple observation.  Although Shannon entropy%
%
\index{entropy!negative probabilities@with negative probabilities}
% 
is usually only defined for sequences $\ppi = (\pi_1, \ldots, \pi_n)$ of
\emph{nonnegative} reals summing to $1$, it can just as easily be defined
for sequences $\ppi$ of \emph{arbitrary} reals summing to $1$.  One simply
puts
% 
\begin{equation}
\lbl{eq:defn-ent-hyp}
H(\ppi) = - \sum_{i \in \supp(\ppi)} \pi_i \log \,\mg{\pi_i},
\end{equation}
% 
where \ntn{suppall}$\supp(\ppi) 
%
\index{support}
%
= \{ i \such \pi_i \neq 0\}$.  (See Kontsevich~\cite{KontOHL},%
%
\index{Kontsevich, Maxim} 
% 
for instance.)  This extended entropy is still continuous and symmetric,
and still satisfies the chain rule.  So, real entropy can in fact be
defined without reference to the notion of positivity.  (And generally
speaking, negative%
%
\index{negative!probability}
%
probabilities are not as outlandish as they might seem; see
Feynman~\cite{Feyn} and Blass and Gurevich~\cite{BlGuNP1,BlGuNP2}.)

Thus, writing
\[
\Pi_n = \{ \ppi \in (\Zp)^n \such \pi_1 + \cdots + \pi_n = 1 \},
\ntn{Pin}
\]
it is reasonable to attempt to define the entropy of any element of
$\Pi_n$.  We will refer to elements $\ppi = (\pi_1, \ldots, \pi_n)$ of
$\Pi_n$ as \demph{probability%
%
\index{probability distribution!modulo a prime}
% 
distributions mod $p$}, or simply \demph{distributions}.% 
%
\index{distribution}
%
Geometrically, the set $\Pi_n$ of
distributions on $n$ elements is a hyperplane in the $n$-dimensional vector
space $(\Zp)^n$ over the field $\Zp$.

The function $x \mapsto \log\mg{x}$ is a homomorphism from the
multiplicative group $\R^\times$ of nonzero reals to the additive
group $\R$.  But when we look for an analogue over $\Zp$, we run into
an obstacle:

\begin{lemma}
\lbl{lemma:no-log}
\index{logarithm modulo a prime}
% 
There is no nontrivial homomorphism from the multiplicative group
$(\Zp)^\times$ of nonzero integers modulo~$p$ to the additive group $\Zp$.
\end{lemma}

\begin{proof}
Let $\phi\from (\Zp)^\times \to \Zp$ be a homomorphism.  The image of
$\phi$ is a subgroup of $\Zp$, which by Lagrange's theorem has order
$1$ or $p$.  Since $(\Zp)^\times$ has order $p - 1$, the image of
$\phi$ has order at most $p - 1$.  It therefore has order $1$; that is,
$\phi = 0$.
\end{proof}

In this sense, there is no logarithm for the integers modulo~$p$.
Nevertheless, there is an acceptable substitute.  For integers $n$ not
divisible by $p$, Fermat's little theorem implies that $p$ divides $n^{p -
  1} - 1$.  The \demph{Fermat%
%
\index{Fermat quotient} 
% 
quotient} of $n$ modulo~$p$ is defined as   
\[
\fq{p}(n) = \frac{n^{p - 1} - 1}{p} \in \Zp.
\ntn{fq}
\]
The resemblance between the formulas for the Fermat quotient and the
$q$-logarithm (equation~\eqref{eq:q-log-general}) hints that the
Fermat quotient might function as some kind of logarithm, and
part~\bref{part:fq-elem-log} of the following lemma confirms that this is
so.

\begin{lemma}
\lbl{lemma:fq-elem}
The map $\fq{p} \from \{ n \in \Z \such p \ndvd n \} \to \Zp$ has the
following properties:
% 
\begin{enumerate}
\item 
\lbl{part:fq-elem-log} 
$\fq{p}(mn) = \fq{p}(m) + \fq{p}(n)$ for all $m, n \in
\Z$ not divisible by~$p$, and $\fq{p}(1) = 0$;

\item
\lbl{part:fq-elem-shift}
$\fq{p}(n + rp) = \fq{p}(n) - r/n$ for all $n, r \in \Z$ such that $n$ is not
divisible by $p$;

\item
\lbl{part:fq-elem-per}
$\fq{p}(n + p^2) = \fq{p}(n)$ for all $n \in \Z$ not divisible by $p$.
% 
% \item
% \lbl{part:fq-elem-surj}
% for all $a \in \Z$, there exists $n \in \Z$ not divisible by $p$ such that
% $\fq{p}(n) \equiv a \pmod{p}$.
\end{enumerate}
\end{lemma}

\begin{proof}
For~\bref{part:fq-elem-log}, certainly $\fq{p}(1) = 0$.  We now have to
show that
\[
m^{p - 1}n^{p - 1} - 1 
\equiv
\bigl( m^{p - 1} - 1 \bigr) + \bigl( n^{p - 1} - 1 \bigr)
\pmod{p^2},
\]
or equivalently,
\[
\bigl( m^{p - 1} - 1 \bigr)\bigl( n^{p - 1} - 1 \bigr)
\equiv
0
\pmod{p^2}.
\]
Since both $m^{p - 1} - 1$ and $n^{p - 1} - 1$ are integer multiples of
$p$, this is true.

For~\bref{part:fq-elem-shift}, we have
% 
\begin{align*}
(n + rp)^{p - 1}        &
=
n^{p - 1} + (p - 1)n^{p - 2}rp
+ \sum_{i = 2}^{p - 1} \binom{p - 1}{i} n^{p - i - 1} r^i p^i       \\
&
\equiv
n^{p - 1} + p(p - 1)r n^{p - 2}
\pmod{p^2}.
\end{align*}
% 
Subtracting $1$ from each side and dividing by $p$ gives 
\[
\fq{p}(n + rp)
\equiv
\fq{p}(n) + (p - 1)rn^{p - 2}
% \equiv
% \fq{p}(n) - r/n
\pmod{p},
\]
and~\bref{part:fq-elem-shift} then follows from the fact that $n^{p - 1}
\equiv 1 \pmod{p}$.  Taking $r = p$ in~\bref{part:fq-elem-shift}
gives~\bref{part:fq-elem-per}. 
\end{proof}

It follows that $\fq{p}$ defines a group homomorphism
\[
\fq{p} \from (\Zps)^\times \to \Zp,
\]
where $(\Zps)^\times$\ntn{Zpsm} is the multiplicative group of integers
modulo~$p^2$.  (The elements of $(\Zps)^\times$ are the congruence
classes modulo~$p^2$ of the integers not divisible by $p$.)  Moreover, the
homomorphism $\fq{p}$ is surjective, since the lemma implies that
\[
\fq{p}(1 - rp) = \fq{p}(1) + r \equiv r \pmod{p}
\]
for all integers $r$.

Lemma~\ref{lemma:no-log} states that there is no logarithm mod~$p$, in the
sense that there is no nontrivial group homomorphism $(\Zp)^\times \to
\Zp$.  But the Fermat quotient is the next best thing, being a homomorphism
$(\Zps)^\times \to \Zp$.  It is essentially the only such homomorphism:

\begin{propn}
\lbl{propn:fq-mult}
Every group homomorphism $(\Zps)^\times \to \Zp$ is a scalar multiple of
the Fermat quotient.
\end{propn}

\begin{proof}
It is a standard fact that the group $(\Zps)^\times$ is cyclic
(Theorem~10.6 of Apostol~\cite{AposIAN}, for instance).  Choose a generator
$e$.  Since $\fq{p}$ is surjective, it is not identically zero, so
$\fq{p}(e) \neq 0$.

Let $\phi\from (\Zps)^\times \to \Zp$ be a group homomorphism.  Put $c =
\phi(e)/\fq{p}(e) \in \Zp$.  Then for all $n \in \Z$,
\[
\phi(e^n) = n\phi(e) = nc\fq{p}(e) = c\fq{p}(e^n).
\]
Since $e$ is a generator, it follows that $\phi = c\fq{p}$.
\end{proof}

In Section~\ref{sec:log-seqs}, we proved characterization theorems for the
sequence $(\log(n))_{n \geq 1}$.  The next result plays a similar role for
$(\fq{p}(n))_{n \geq 1,\:p \ndvd n}$.  

\begin{thm}
\lbl{thm:fq-char}
\index{Fermat quotient!characterization of}
% 
Let $f \from \{ n \in \nat \such p \ndvd n \} \to \Zp$ be a function.  The
following are equivalent:
% 
\begin{enumerate}
\item 
\lbl{part:fq-char-condns}
$f(mn) = f(m) + f(n)$ and $f(n + p^2) = f(n)$ for all $m, n \in \nat$ not
divisible by $p$;

\item
\lbl{part:fq-char-form}
$f = c\fq{p}$ for some $c \in \Zp$.
\end{enumerate}
\end{thm}

\begin{proof}
We have already shown that $\fq{p}$ satisfies the conditions
in~\bref{part:fq-char-condns}, so~\bref{part:fq-char-form}
implies~\bref{part:fq-char-condns}.  For the converse, suppose that $f$
satisfies the conditions in~\bref{part:fq-char-condns}.  Then $f$ induces a
group homomorphism $(\Zps)^\times \to \Zp$, which by
Proposition~\ref{propn:fq-mult} is a scalar multiple of $\fq{p}$.  The
result follows.
\end{proof}

In terms of the three-step plan in the introduction to this chapter, we
have now completed step~(I): defining and characterizing the appropriate
notion of logarithm.  We now begin step~(II): defining and characterizing
the appropriate notion of entropy.

To state the definition, we will need an elementary lemma.

\begin{lemma}
\lbl{lemma:pps}
Let $a, b \in \Z$.  If $a \equiv b \pmod{p}$ then $a^p \equiv b^p
\pmod{p^2}$. 
\end{lemma}

\begin{proof}
If $b = a + rp$ with $r \in \Z$, then
% 
\begin{align*}
b^p     &
= 
(a + rp)^p      \\
&
=
a^p + p a^{p - 1} rp + \sum_{i = 2}^p \binom{p}{i} a^{p - i} r^i p^i    \\
&
\equiv
a^p
\pmod{p^2}.
\end{align*}
% 
% as required.
\end{proof}

% This lemma enables us to state the definition of entropy modulo $p$.
% Explanation and justification follow.

\begin{defn}
\lbl{defn:ent-p}
Let $n \geq 1$ and $\ppi \in \Pi_n$.  The
\demph{entropy}%
%
\index{entropy!modulo a prime} 
% 
of $\ppi$ is
\[
H(\ppi) 
= 
\frac{1}{p} \Biggl( 1 - \sum_{i = 1}^n a_i^p \Biggr)
\in \Zp,
\ntn{Hp}
\]
where $a_i \in \Z$ represents $\pi_i \in \Zp$ for each $i \in \{1,
\ldots, n\}$. 
\end{defn}

Lemma~\ref{lemma:pps} guarantees that the definition is independent of the
choice of $a_1, \ldots, a_n$.  

We now explain and justify the definition of entropy mod~$p$.  In
particular, we will prove a theorem characterizing the sequence of
functions $\bigl( H \from \Pi_n \to \Zp \bigr)$ uniquely up to a scalar
multiple.  This result is plainly analogous to Faddeev's theorem for real
entropy, and as such, is the strongest justification for the definition.
But the analogy with the real case can also be seen in terms of
derivations, as follows.

The entropy of a real probability distribution $\ppi$ is equal
to $\sum_i \partial(\pi_i)$, where
% 
\begin{equation}
\lbl{eq:real-deriv}
\partial(x) = 
\begin{cases}
-x\log x        &\text{if } x > 0,   \\
0               &\text{if } x = 0
\end{cases}
\end{equation}
% 
(as in Section~\ref{sec:ent-defn}).  What is the analogue of $\partial$ over
$\Zp$?  Given the analogy between the logarithm and the Fermat quotient, it
is natural to consider $-n\fq{p}(n)$ as a candidate.  For integers $n$ not
divisible by $p$, 
\[
-n \fq{p}(n) 
=
\frac{n - n^p}{p}.
\]
The right-hand side is a well-defined integer even if $n$ is divisible by
$p$.  We therefore define a map $\partial \from \Z \to \Zp$ by
% 
\begin{equation}
\lbl{eq:p-deriv}
\partial(n) = \frac{n - n^p}{p} \in \Zp.
\end{equation}
% 
Thus,
\[
\partial(n)
=
\begin{cases}
-n\fq{p}(n)        &\text{if } p \ndvd n,  \\
n/p             &\text{if } p \dvd n.
\end{cases}
\]
If $n \equiv m \pmod{p^2}$ then $\partial(n) = \partial(m)$, so $\partial$
can also be regarded as a map $\Zps \to \Zp$.  Like its real counterpart,
$\partial$ satisfies a version of the Leibniz rule (and for this reason, is
essentially what is called a
\demph{$p$-derivation}%
%
\index{derivation!p-@$p$-}%
\index{pderivation@$p$-derivation}%
%
~\cite{BuiuDCA,BuiuAAD}):

\begin{lemma}
\lbl{lemma:p-deriv-elem}
$\partial(mn) = m\partial(n) + \partial(m)n$ for all $m, n \in \Z$.
\end{lemma}

\begin{proof}
The proof is similar to that of
Lemma~\ref{lemma:fq-elem}\bref{part:fq-elem-log}.  The statement to be
proved is equivalent to
\[
mn - m^p n^p \equiv m(n - n^p) + (m - m^p)n \pmod{p^2}.
\]
Rearranging, this in turn is equivalent to
\[
0 \equiv (m - m^p)(n - n^p) \pmod{p^2},
\]
which is true since $m \equiv m^p \pmod{p}$ and $n \equiv n^p \pmod{p}$.
\end{proof}

Using this lemma, we derive an equivalent expression for entropy mod~$p$:

\begin{lemma}
\lbl{lemma:ent-partial}
For all $n \geq 1$ and $\ppi \in \Pi_n$,
\[
H(\ppi)
=
\sum_{i = 1}^n \partial(a_i) 
- \partial\Biggl( \sum_{i = 1}^n a_i \Biggr),
\]
where $a_i \in \Z$ represents $\pi_i \in \Zp$ for each $i \in \{1, \ldots,
n\}$. 
\end{lemma}

\begin{proof}
An equivalent statement is that
\[
1 - \sum a_i^p
\equiv
\sum (a_i - a_i^p) - \biggl\{ \sum a_i - \Bigl( \sum a_i \Bigr)^p \biggr\}
\pmod{p^2}.
\]
Cancelling, this reduces to
\[
1 \equiv \Bigl( \sum a_i \Bigr)^p \pmod{p^2}.
\]
But $\sum \pi_i = 1$ in $\Zp$ by definition of $\Pi_n$, so $\sum a_i \equiv
1 \pmod{p}$, so $\bigl( \sum a_i \bigr)^p \equiv 1 \pmod{p^2}$ by
Lemma~\ref{lemma:pps}. 
\end{proof}

Thus, $H(\ppi)$ measures the extent to which the nonlinear derivation
$\partial$ fails to preserve the sum $\sum a_i$.

The analogy with entropy over $\R$ is now evident.  For a real probability
distribution $\ppi$, and defining $\partial \from [0, \infty) \to \R$ as
  in equation~\eqref{eq:real-deriv}, we also have
\[
H(\ppi) = \sum \partial(\pi_i) - \partial\Bigl(\sum \pi_i\Bigr).
\]
In the real case, since $\sum \pi_i = 1$, the second term on the right-hand
side vanishes.  But over $\Zp$,
% (and with notation as in Lemma~\ref{lemma:ent-partial}), 
it is not true in general that $\partial(\sum a_i) = 0$, so it is not true
either that $H(\ppi) = \sum \partial(a_i)$.  (Indeed, $\sum \partial(a_i)$,
unlike $H(\ppi)$, depends on the choice of representatives $a_i$.)
So in the formula
\[
H(\ppi) = \sum \partial(a_i) - \partial\Bigl(\sum a_i\Bigr)
\]
for entropy mod~$p$, the second summand is indispensable.

\begin{example}
\lbl{eg:ent-p-ufm}
Let $n \geq 1$ with $p \ndvd n$.  Since $n$ is invertible mod~$p$, there is
a \demph{uniform%
%
\index{uniform distribution!modulo a prime} 
% 
distribution}
\[
\vc{u}_n = (\underbrace{1/n, \ldots, 1/n}_n) \in \Pi_n.
\ntn{unp}
\]
Choose $a \in \Z$ representing $1/n \in \Zp$.  By
Lemma~\ref{lemma:ent-partial} and then the derivation property of
$\partial$, 
\[
H(\vc{u}_n)
=
n\partial(a) - \partial(na)
=
-a\partial(n).
\]
But $\partial(n) = -n\fq{p}(n)$, so $H(\vc{u}_n) = \fq{p}(n)$.  This result over
$\Zp$ is analogous to the formula $H(\vc{u}_n) = \log n$ for the real entropy
of a uniform distribution.
\end{example}

\begin{example}
\lbl{eg:ent-p-2}
Let $p = 2$.  Any distribution $\ppi \in \Pi_n$ has an odd
number of elements in its support, since $\sum \pi_i = 1$.  Directly from
the definition of entropy, $H(\ppi) \in \Z/2\Z$ is given by 
\[
H(\ppi) 
= 
\hlf\bigl(\mg{\supp(\ppi)} - 1\bigr) 
=
\begin{cases}
0       &\text{if } \mg{\supp(\ppi)} \equiv 1 \!\!\!\!\pmod{4}, \\
1       &\text{if } \mg{\supp(\ppi)} \equiv 3 \!\!\!\!\pmod{4}.
\end{cases}
\]
\end{example}

In preparation for the next example, we record a useful standard lemma:

\begin{lemma}
\lbl{lemma:p-binom}
$\binom{p - 1}{s} \equiv (-1)^s \pmod{p}$ for all $s \in \{0, \ldots, p -
  1\}$. 
\end{lemma}

\begin{proof}
In $\Zp$, we have equalities
% 
\begin{align*}
\binom{p - 1}{s}        &
=
\frac{(p - 1)(p - 2) \cdots (p - s)}{s!}    \\
&
=
\frac{(-1)(-2) \cdots (-s)}{s!}   \\
&
=
(-1)^s.
\end{align*}
% 
% as required.
\end{proof}

\begin{example}
\lbl{eg:p-bin} Here we find the entropy of a distribution $(\pi, 1 - \pi)$
on two elements.  Choose an integer $a$ representing $\pi \in \Zp$.  From the
definition of entropy, assuming that $p \neq 2$,
\[
H(\pi, 1 - \pi)
=
\frac{1}{p} \bigl( 1 - a^p - (1 - a)^p \bigr)
=
\sum_{r = 1}^{p - 1} (-1)^{r + 1} \frac{1}{p} \binom{p}{r} a^r.
\]
But $\tfrac{1}{p} \binom{p}{r} = \tfrac{1}{r} \binom{p - 1}{r - 1}$, so by
Lemma~\ref{lemma:p-binom}, the coefficient of $a^r$ in the sum is simply
$\tfrac{1}{r}$.  We can now replace $a$ by $\pi$, giving
\[
H(\pi, 1 - \pi) = \sum_{r = 1}^{p - 1} \frac{\pi^r}{r}.
\]
The function on the right-hand side was the starting point of Kontsevich's
note~\cite{KontOHL}, and we return to it in Section~\ref{sec:p-poly}.  

In the case $p = 2$, we have $H(\pi, 1 - \pi) = 0$ for both values of $\pi
\in \Ztwo$.
\end{example}

\begin{example}
Appending zero probabilities to a distribution does not change its entropy:
\[
H(\pi_1, \ldots, \pi_n, 0, \ldots, 0) = H(\pi_1, \ldots, \pi_n).
\]
This is immediate from the definition.  But a subtlety of distributions
mod~$p$, absent in the standard real setting, is that nonzero
probabilities can sum to zero.  So, one might ask whether
\[
H(\pi_1, \ldots, \pi_n, \tau_1, \ldots, \tau_m) = H(\pi_1, \ldots, \pi_n)
\]
whenever $\tau_1, \ldots, \tau_m \in \Zp$ with $\sum \tau_j = 0$.  The
answer is trivially yes for $m = 0$ and $m = 1$, and it is also yes for $m
= 2$ as long as $p \neq 2$.  (For if we choose an integer $a$ to represent
$\tau_1$ then $-a$ represents $\tau_2$, and $a^p + (-a)^p = 0$.)  But the
answer is no for $m \geq 3$.  For instance, when $p = 3$, we have
\[
H(1, 1, 1, 1) = H(\vc{u}_4) = \fq{3}(4) = \tfrac{1}{3}(4^2 - 1) = -1
\]
by Example~\ref{eg:ent-p-ufm}, which is not equal to $H(1) = 0$, even
though $1 + 1 + 1 = 0$.
\end{example}

Distributions over $\Zp$ can be composed, using the same formula as in the
real case (Definition~\ref{defn:comp-dist}).  As in the real case, entropy
mod~$p$ satisfies the chain rule:

\begin{propn}[Chain rule]
\lbl{propn:chn-p}%
\index{chain rule!modulo a prime}%
%
We have
\[
H\bigl( \ggamma \of (\ppi^1, \ldots, \ppi^n) \bigr)
=
H(\ggamma) + \sum_{i = 1}^n \gamma_i H(\ppi^i)
\]
for all $n, k_1, \ldots, k_n \geq 1$, all $\ggamma = (\gamma_1, \ldots,
\gamma_n) \in \Pi_n$, and all $\ppi^i \in \Pi_{k_i}$.
\end{propn}

\begin{proof}
Write $\ppi^i = \bigl(\pi^i_1, \ldots, \pi^i_{k_i}\bigr)$.  Choose $b_i \in
\Z$ representing $\gamma_i \in \Zp$ and $a^i_j \in \Z$ representing
$\pi^i_j \in \Zp$, for each $i$ and $j$.  Write $A^i = a^i_1 + \cdots +
a^i_{k_i}$.

We evaluate in turn the three terms $H\bigl( \ggamma \of (\ppi^1, \ldots,
\ppi^n) \bigr)$, $H(\ggamma)$, and $\sum \gamma_i H(\ppi^i)$.
First, by Lemma~\ref{lemma:ent-partial} and the derivation property of
$\partial$ (Lemma~\ref{lemma:p-deriv-elem}),
% 
\begin{align*}
H\bigl( \ggamma \of (\ppi^1, \ldots, \ppi^n) \bigr)     &
=
\sum_{i = 1}^n \sum_{j = 1}^{k_i} \partial\bigl(b_i a^i_j\bigr)
-
\partial \Biggl( \sum_{i = 1}^n \sum_{j = 1}^{k_i} b_i a^i_j \Biggr)    \\
&
=
\sum_{i = 1}^n \sum_{j = 1}^{k_i}
\bigl( \partial(b_i) a^i_j + b_i \partial\bigl(a^i_j\bigr) \bigr)
-
\partial \Biggl( \sum_{i = 1}^n b_i A^i \Biggr) \\
&
=
\sum_{i = 1}^n \partial(b_i) A^i 
+ \sum_{i = 1}^n b_i \sum_{j = 1}^{k_i} \partial\bigl(a^i_j\bigr)
-
\partial \Biggl( \sum_{i = 1}^n b_i A^i \Biggr). 
\end{align*}
% 
Second, $A^i \equiv 1 \pmod{p}$ since $\ppi^i \in \Pi_{k_i}$, so
$b_i A^i \in \Z$ represents $\gamma_i \in \Zp$.  Hence
% 
\begin{align*}
H(\ggamma)      &
=
\sum_{i = 1}^n \partial\bigl(b_i A^i\bigr) 
- \partial\Biggl( \sum_{i = 1}^n b_i A^i \Biggr)        \\
&
=
\sum_{i = 1}^n \partial(b_i) A^i
+ \sum_{i = 1}^n b_i \partial(A^i)
- \partial\Biggl( \sum_{i = 1}^n b_i A^i \Biggr).        %\\
\end{align*}
% 
Third, 
% 
\[
\sum_{i = 1}^n \gamma_i H(\ppi^i)       
=
\sum_{i = 1}^n b_i \sum_{j = 1}^{k_i} \partial\bigl(a^i_j\bigr)
- \sum_{i = 1}^n b_i \partial(A^i).
\]
The result follows.
\end{proof}

There is a tensor%
%
\index{tensor product} 
% 
product for distributions mod~$p$, defined as in the real case
(p.~\pageref{p:tensor}), and entropy mod~$p$ has the familiar logarithmic%
%
\index{entropy!modulo a prime!logarithmic property}%
\index{logarithmic sequence} 
% 
property:

\begin{cor}
\lbl{cor:ent-p-loglike}
$H(\ggamma \otimes \ppi) = H(\ggamma) + H(\ppi)$ for all $\ggamma \in
  \Pi_n$ and $\ppi \in \Pi_m$.
\qed
\end{cor}


\section{Characterizations of entropy and information loss}
\lbl{sec:p-char}


We now state our characterization theorem for entropy mod~$p$, whose close
resemblance to the characterization theorem for real entropy
(Theorem~\ref{thm:faddeev}) is the main justification for the definition.

\begin{thm}
\lbl{thm:fad-p}%
\index{entropy!modulo a prime!characterization of}%
\index{Faddeev, Dmitry!mod p analogue of entropy theorem@mod $p$ analogue of entropy theorem}%
%
Let $\bigl( I \from \Pi_n \to \Zp \bigr)_{n \geq 1}$ be a sequence of
functions.  The following are equivalent:
% 
\begin{enumerate}
\item 
\lbl{part:fad-p-condns}
$I$ satisfies the chain rule (that is, satisfies the conclusion of
Proposition~\ref{propn:chn-p} with $I$ in place of $H$);

\item
\lbl{part:fad-p-form}
$I = cH$ for some $c \in \Zp$.
\end{enumerate}
\end{thm}

As in our sharper version of Faddeev's theorem over $\R$
(Theorem~\ref{thm:faddeev}), no symmetry condition is needed.

Since $H$ satisfies the chain rule, so does any constant multiple of $H$.
Hence~\bref{part:fad-p-form} implies~\bref{part:fad-p-condns}.
% 
We now begin the proof of the converse.  \femph{For the rest of the proof},
let $\bigl( I \from \Pi_n \to \Zp \bigr)_{n \geq 1}$ be a sequence of
functions satisfying the chain rule.

\begin{lemma}
\lbl{lemma:ent-p-elem}
\begin{enumerate}
\item
\lbl{part:epe-bin}
$I(\vc{u}_{mn}) = I(\vc{u}_m) + I(\vc{u}_n)$ for all $m, n \in \nat$ not
  divisible by $p$;

\item 
\lbl{part:epe-null}
$I(\vc{u}_1) = 0$.
\end{enumerate}
\end{lemma}

\begin{proof}
Both parts are proved exactly as in the real case
(Lemma~\ref{lemma:fad-log}).  
\end{proof}

\begin{lemma}
\lbl{lemma:p-10}
$I(1, 0) = I(0, 1) = 0$.
\end{lemma}

\begin{proof}
The proof that $I(1, 0) = 0$ is identical to the proof in the real case
(Lemma~\ref{lemma:fad-10}), and $I(0, 1) = 0$ is proved similarly.
\end{proof}

\begin{lemma}
\lbl{lemma:ent-p-abs}
For all $\ppi \in \Pi_n$ and $i \in \{0, \ldots, n\}$, 
\[
I(\pi_1, \ldots, \pi_n)
=
I(\pi_1, \ldots, \pi_i, 0, \pi_{i + 1}, \ldots, \pi_n).
\]
\end{lemma}

\begin{proof}
First suppose that $i \neq 0$.  Then
\[
(\pi_1, \ldots, \pi_i, 0, \pi_{i + 1}, \ldots, \pi_n)
=
\ppi \of 
\bigl(\underbrace{\vc{u}_1, \ldots, \vc{u}_1}_{i - 1}, (1, 0), 
\underbrace{\vc{u}_1, \ldots, \vc{u}_1}_{n - i}\bigr).
\]
Applying $I$ to both sides, then using the chain rule and that $I(\vc{u}_1)
= I(1, 0) = 0$, gives the result.  The case $i = 0$ is proved similarly,
now using $I(0, 1) = 0$.
\end{proof}

As in the real case, we will prove the characterization theorem by
analysing $I(\vc{u}_n)$ as $n$ varies.  And as in the real case, the chain
rule will allow us to deduce the value of $I(\ppi)$ for more general
distributions $\ppi$:

\begin{lemma}
\lbl{lemma:p-ufm-to-gen}
Let $\ppi \in \Pi_n$ with $\pi_i \neq 0$ for all $i$.  For each $i$, let
$k_i \geq 1$ be an integer representing $\pi_i \in \Zp$, and write $k =
\sum_{i = 1}^n k_i$.  Then 
\[
I(\ppi) = I(\vc{u}_k) - \sum_{i = 1}^n k_i I(\vc{u}_{k_i}).
\]
\end{lemma}

\begin{proof}
First note that none of $k_1, \ldots, k_n$ is a multiple of $p$, and since
$k$ represents $\sum \pi_i = 1 \in \Zp$, neither is $k$.  Hence
$\vc{u}_{k_i}$ and $\vc{u}_k$ are well-defined.  By definition of
composition,
\[
\ppi \of (\vc{u}_{k_1}, \ldots, \vc{u}_{k_n}) 
= 
(\underbrace{1, \ldots, 1}_k)
=
\vc{u}_k.
\]
Applying $I$ and using the chain rule gives the result.
\end{proof}

We now come to the most delicate part of the argument.  Since $H(\vc{u}_n)
= \fq{p}(n)$, and since $\fq{p}(n)$ is $p^2$-periodic in $n$, if $I$ is to
be a constant multiple of $H$ then $I(\vc{u}_n)$ must also be
$p^2$-periodic in $n$.  We show this directly.

\begin{lemma}
\lbl{lemma:p-per}
$I(\vc{u}_{n + p^2}) = I(\vc{u}_n)$ for all natural numbers $n$ not
  divisible by $p$.
\end{lemma}

\begin{proof}
First we prove the existence of a constant $c \in \Zp$ such that for all $n
\in \nat$ not divisible by $p$,
% 
\begin{equation}
\lbl{eq:per-one}
I(\vc{u}_{n + p}) = I(\vc{u}_n) - c/n.
\end{equation}
% 
(Compare Lemma~\ref{lemma:fq-elem}\bref{part:fq-elem-shift}.)  An
equivalent statement is that $n(I(\vc{u}_{n + p}) - I(\vc{u}_n))$ is
independent of $n$.  For any $n_1$ and $n_2$, we can
choose some $m \geq \max\{n_1, n_2\}$ with $m \equiv 1 \pmod{p}$, so it is
enough to show that whenever $0 \leq n \leq m$ with $n \not\equiv 0
\pmod{p}$ and $m \equiv 1 \pmod{p}$,
% 
\begin{equation}
\lbl{eq:per-nm}
n \bigl( I(\vc{u}_{n + p}) - I(\vc{u}_{n}) \bigr)
=
I(\vc{u}_{m + p}) - I(\vc{u}_{m}).
\end{equation}
% 
To prove this, consider the distribution
\[
\ppi = (n, \underbrace{1, \ldots, 1}_{m - n}).
\]
By Lemma~\ref{lemma:p-ufm-to-gen} and the fact that $I(\vc{u}_1) = 0$,
\[
I(\ppi) = I(\vc{u}_m) - nI(\vc{u}_n).
\]
But also
\[
\ppi = (n + p, \underbrace{1, \ldots, 1}_{m - n}),
\]
so by the same argument,
% 
\begin{align*}
I(\ppi) &
=
I(\vc{u}_{m + p}) - (n + p)I(\vc{u}_{n + p})    \\
&
= 
I(\vc{u}_{m + p}) - nI(\vc{u}_{n + p}).
\end{align*}
% 
Comparing the two expressions for $I(\ppi)$ gives
equation~\eqref{eq:per-nm}, thus proving the initial claim.

By induction on equation~\eqref{eq:per-one},
\[
I(\vc{u}_{n + rp}) = I(\vc{u}_n) - cr/n
\]
for all $n, r \in \nat$ with $n$ not divisible by $p$.  The result 
follows by setting $r = p$.
\end{proof}

We can now prove the characterization theorem for entropy modulo~$p$.

\begin{pfof}{Theorem~\ref{thm:fad-p}}
Define $f \from \{ n \in \nat \such p \ndvd n\} \to \Zp$ by $f(n) =
I(\vc{u}_n)$.  By Lemma~\ref{lemma:ent-p-elem}, $f(mn) = f(m) + f(n)$ for
all $m, n$ not divisible by $p$.  By Lemma~\ref{lemma:p-per}, $f(n + p^2) =
f(n)$ for all $n$ not divisible by $p$.  Hence by
Theorem~\ref{thm:fq-char}, $f = c\fq{p}$ for some $c \in \Zp$.  It follows
from Example~\ref{eg:ent-p-ufm} that $I(\vc{u}_n) = cH(\vc{u}_n)$ for all
$n$ not divisible by $p$.

Since both $I$ and $cH$ satisfy the chain rule,
Lemma~\ref{lemma:p-ufm-to-gen} applies to both; and since $I$ and $cH$ are
equal on uniform distributions, they are also equal on all distributions
$\ppi$ such that $\pi_i \neq 0$ for all $i$.  Finally, applying
Lemma~\ref{lemma:ent-p-abs} to both $I$ and $cH$, we deduce by 
induction that $I(\ppi) = cH(\ppi)$ for all $\ppi \in \Pi_n$.
\end{pfof}

In the real case, the characterization theorem for entropy leads to a
characterization of information loss involving only linear conditions
(Theorem~\ref{thm:cetil}).  The same holds for entropy mod~$p$, and the
argument can be copied over from the real case nearly verbatim.

Thus, given a finite set $\XX$, we write $\Pi_\XX$ for the set of families
$\ppi = (\pi_i)_{i \in \XX}$ of elements of $\Zp$ such that $\sum_{i \in \XX}
\pi_i = 1$.  A \demph{finite probability space mod~$p$}%
%
\index{probability space!modulo a prime}
%
is a finite set $\XX$ together with an element $\ppi \in \Pi_\XX$.  A
\demph{measure-preserving%
%
\index{measure-preserving map!modulo a prime} 
% 
map} $f \from (\YY, \ssigma) \to (\XX, \ppi)$
between such spaces is a function $f \from \YY \to \XX$ such that
\[
\pi_i = \sum_{j \in f^{-1}(i)} s_j
\]
for all $i \in \XX$.  

As in the real case, we can take convex combinations of both
probability spaces and maps between them.  Given two finite probability
spaces mod~$p$, say $(\XX, \ppi)$ and $(\XX', \ppi')$, and given also a
scalar $\lambda \in \Zp$, we obtain another such space, $\bigl(\XX \cpd
\XX', \lambda\ppi \cpd (1 - \lambda)\ppi'\bigr)$\ntn{smallccdistsp}.  Given
two measure-preserving maps
% \[
% \xymatrix{
% f \from         (\YY, \ssigma)   \ar[r]    &(\XX, \ppi) \\
% f' \from        (\YY', \ssigma') \ar[r]    &(\XX', \ppi')
% }
\begin{eqnarray*}
f \from         (\YY, \ssigma)   &\to&    (\XX, \ppi), \\
f' \from        (\YY', \ssigma') &\to&    (\XX', \ppi')
\end{eqnarray*}
% \]
and an element $\lambda \in \Zp$, we obtain a new measure-preserving map
\[
\lambda f \cpd (1 - \lambda) f'
\from
\bigl(\YY \cpd \YY', \lambda\ssigma \cpd (1 - \lambda)\ssigma'\bigr)
\to
\bigl(\XX \cpd \XX', \lambda\ppi \cpd (1 - \lambda)\ppi'\bigr),
\ntn{smallccmapsp}
\]
exactly as in Section~\ref{sec:meas-pres}.

The \demph{entropy}%
%
\index{entropy!modulo a prime} 
% 
of $\ppi \in \Pi_\XX$ is, naturally, 
\[
H(\ppi) = \frac{1}{p} \Biggl( 1 - \sum_{i \in \XX} a_i^p \Biggr),
\ntn{entgenp}
\]
where $a_i \in \Z$ represents $\pi_i \in \Zp$ for each $i \in \XX$.  The
\demph{information loss}%
%
\index{information loss!modulo a prime} 
% 
of a measure-preserving map $f \from (\YY, \ssigma)
\to (\XX, \ppi)$ between finite probability spaces mod~$p$ is
\[
L(f) = H(\ssigma) - H(\ppi) \in \Zp.
\ntn{Lp}
\]

\begin{thm}
\lbl{thm:cetil-p}
\index{information loss!modulo a prime!characterization of}
% 
Let $K$ be a function assigning an element $K(f) \in \Zp$ to each
measure-preserving map $f$ of finite probability spaces mod~$p$.  The
following are equivalent:
% 
\begin{enumerate}
\item 
\lbl{part:cetil-p-condns}
$K$ has these three properties:
% 
\begin{enumerate}
\item 
$K(f) = 0$ for all isomorphisms $f$;

\item
$K(g \of f) = K(g) + K(f)$ for all composable pairs $(f, g)$ of
  measure-preserving maps;

\item
$K\bigl(\lambda f \cpd (1 - \lambda) f'\bigr) = \lambda K(f) + (1 -
  \lambda) K(f')$ for all measure-preserving maps $f$ and $f'$ and all
  $\lambda \in \Zp$;
\end{enumerate}

\item
\lbl{part:cetil-p-form}
$K = cL$ for some $c \in \Zp$.
\end{enumerate}
\end{thm}

\begin{proof}
The proof is identical to that of the real case, Theorem~\ref{thm:cetil},
but with $\Zp$ in place of $\R$, Theorem~\ref{thm:fad-p} in place of
Faddeev's theorem, and all mention of continuity removed.
\end{proof}


\section{The residues of real entropy}
\lbl{sec:p-res}
\index{entropy!residue modulo a prime}


Having found a satisfactory definition of the entropy of a probability
distribution mod~$p$, we are now in a position to develop 
Kontsevich's%
%
\index{Kontsevich, Maxim} 
%
suggestion about the residues mod~$p$ of real entropy, quoted at the start of
this chapter.  (That quotation was the sum total of what he wrote on the
subject.)

Let $\ppi \in \Delta_n$ be a probability distribution with
rational probabilities, say $\ppi = (a_1/b_1, \ldots, a_n/b_n)$ with $a_i,
b_i \in \Z$.  There are only finitely many primes that divide one or more
of the denominators $b_i$.  If $p$ is not in that exceptional set then
$\ppi$ defines an element of $\Pi_n$, and therefore has a mod~$p$ entropy
$H(\ppi) \in \Zp$.  Kontsevich invites us to think of this as the residue%
%
\index{residue class} 
% 
class modulo~$p$ of the real entropy $H(\ppi) \in \R$.

Kontsevich phrased his suggestion playfully, but there is more to it than
meets the eye.  To explain it, let us write $H_\R$\ntn{HR} for
entropy over the reals, $H_p$\ntn{Hpp} for entropy mod~$p$, and
$\Rat{n}{p}$\ntn{Deltanp} for the set of real probability distributions
$\ppi \in \Delta_n$ such that each $\pi_i$ can be expressed as a rational
number with denominator not divisible by $p$.  The proposal is that given
$\ppi \in \Rat{n}{p}$, we regard $H_p(\ppi) \in \Zp$ as the residue mod~$p$
of $H_\R(\ppi) \in \R$.

Now, different distributions can have the same entropy over $\R$.  For
instance, 
\[
H_\R\bigl(\tfrac{1}{2}, 
\tfrac{1}{8}, \tfrac{1}{8}, \tfrac{1}{8}, \tfrac{1}{8}\bigr)
=
H_\R\bigl(\tfrac{1}{4}, \tfrac{1}{4}, \tfrac{1}{4}, \tfrac{1}{4}\bigr).
\]
There is, therefore, a question of consistency: Kontsevich's proposal only
makes sense if
\[
H_\R(\ppi) = H_\R(\ggamma) \implies H_p(\ppi) = H_p(\ggamma)
\]
for all $\ppi \in \Rat{n}{p}$ and $\ggamma \in \Rat{m}{p}$.  We now show
that this is true.

\begin{lemma}
\lbl{lemma:p-prod-par}
Let $n, m \geq 1$ and let $a_1, \ldots, a_n, b_1, \ldots, b_m \geq 0$ be
integers.  Then
\[
\prod_{i = 1}^n a_i^{a_i} = \prod_{j = 1}^m b_j^{b_j}
\implies
\sum_{i = 1}^n \partial(a_i) = \sum_{j = 1}^m \partial(b_j),
\]
where the first equality is in $\Z$, the second is in $\Zp$, and we use the
convention that $0^0 = 1$.
\end{lemma}

Here $\partial$ is the map $\Z \to \Zp$ defined in
equation~\eqref{eq:p-deriv}.  The analogue of this lemma for the
real-valued map $\partial$ of equation~\eqref{eq:real-deriv} is trivial:
simply discard the factors in the products for which $a_i$ or $b_j$ is $0$,
then take logarithms.  But over $\Zp$, it is not so simple.  The subtlety
arises from the possibility that some $a_i$ or $b_j$ is not zero but is
divisible by $p$.  In that case, $\prod a_i^{a_i} = \prod b_j^{b_j}$ is
divisible by $p$, so its Fermat quotient~-- the analogue of the
logarithm~-- is undefined.  A more detailed analysis is therefore
required. 

\begin{proof}
Since $0^0 = 1$ and $\partial(0) = 0$, it is enough to prove the result in
the case where each of the integers $a_i$ and $b_j$ is strictly positive.
We may then write $a_i = p^{\alpha_i} A_i$ with $\alpha_i \geq 0$ and $p
\ndvd A_i$, and similarly $b_j = p^{\beta_j} B_j$.  We adopt the convention
that unless mentioned otherwise, the index $i$ ranges over $1, \ldots, n$
and the index $j$ over $1, \ldots, m$.

Assume that $\prod a_i^{a_i} = \prod b_j^{b_j}$.  We have
\[
\prod a_i^{a_i}
=
p^{\sum \alpha_i a_i} \prod A_i^{a_i}
\]
with $p \ndvd \prod A_i^{a_i}$, and similarly for $\prod b_j^{b_j}$.
It follows that
% 
\begin{align}
\prod A_i^{a_i}       &
=
\prod B_j^{b_j},       
\lbl{eq:ppp-prod}       \\
\sum \alpha_i a_i &
=
\sum \beta_j b_j
\lbl{eq:ppp-sum}        
\end{align}
% 
in $\Z$. We consider each of these equations in turn.

First, since $p \ndvd \prod A_i^{a_i}$, the Fermat quotient
$\fq{p}\bigl(\prod A_i^{a_i}\bigr)$ is well-defined, and the logarithmic
property of $\fq{p}$ (Lemma~\ref{lemma:fq-elem}\bref{part:fq-elem-log})
gives 
\[
-\fq{p}\Bigl( \prod A_i^{a_i} \Bigr)
=
\sum - a_i \fq{p}(A_i).
\]
Consider the right-hand side as an element of $\Zp$.  When $p \dvd a_i$,
the $i$-summand vanishes.  When $p \ndvd a_i$, the $i$-summand is $-a_i
\fq{p}(a_i) = \partial(a_i)$.  Hence
\[
-\fq{p}\Bigl( \prod A_i^{a_i} \Bigr)
=
\sum_{i \csuch \alpha_i = 0} \partial(a_i)
\]
in $\Zp$.  A similar result holds for $\prod B_j^{b_j}$, so
equation~\eqref{eq:ppp-prod} gives
% 
\begin{equation}
\lbl{eq:ppp-zero}
\sum_{i \csuch \alpha_i = 0} \partial(a_i)
=
\sum_{j \csuch \beta_j = 0} \partial(b_j).
\end{equation}

Second,
\[
\sum_{i = 1}^n \alpha_i a_i 
= 
\sum_{i \csuch \alpha_i \geq 1} \alpha_i a_i,
\]
so $p \dvd \sum \alpha_i a_i$.  Now
\[
\frac{1}{p} \sum \alpha_i a_i 
=
\sum_{i \csuch \alpha_i \geq 1} \alpha_i p^{\alpha_i - 1} A_i
\equiv
\sum_{i \csuch \alpha_i = 1} A_i \pmod{p},
\]
and if $\alpha_i = 1$ then $A_i = a_i/p = \partial(a_i)$.  
A similar result holds for $\sum \beta_j b_j$, so
equation~\eqref{eq:ppp-sum} gives
% 
\begin{equation}
\lbl{eq:ppp-one}
\sum_{i \csuch \alpha_i = 1} \partial(a_i)
=
\sum_{j \csuch \beta_j = 1} \partial(b_j)
\end{equation}
% 
in $\Zp$.

Finally, for each $i$ such that $\alpha_i \geq 2$, we have $p^2 \dvd a_i$
and so $\partial(a_i) = 0$ in $\Zp$.  The same holds for $b_j$, so 
% 
\begin{equation}
\lbl{eq:ppp-geqtwo}
\sum_{i \csuch \alpha_i \geq 2} \partial(a_i)
=
\sum_{j \csuch \beta_j \geq 2} \partial(b_j), 
\end{equation}
% 
both sides being $0$.  Adding equations~\eqref{eq:ppp-zero},
\eqref{eq:ppp-one} and~\eqref{eq:ppp-geqtwo} gives the result.
\end{proof}

We deduce that the real entropy of a rational distribution determines its
entropy mod $p$:

\begin{thm}
\lbl{thm:R-p}
\index{entropy!residue modulo a prime}
\index{residue class}
% 
Let $n, m \geq 1$, $\ppi \in \Rat{n}{p}$, and $\ggamma \in \Rat{m}{p}$.
Then
\[
H_\R(\ppi) = H_\R(\ggamma) \implies H_p(\ppi) = H_p(\ggamma).
\]
\end{thm}

\begin{proof}
We can write
\[
\ppi = (r_1/t, \ldots, r_n/t),  
\qquad
\ggamma = (s_1/t, \ldots, s_m/t),
\]
where $r_i$, $s_j$ and $t$ are nonnegative integers such that $p \ndvd t$
and
\[
r_1 + \cdots + r_n = t = s_1 + \cdots + s_m.
\]
By multiplying all of these integers by a constant if necessary, we may
assume that $t \equiv 1 \pmod{p}$.  

By definition,
\[
e^{-H_\R(\ppi)} = \prod_i (r_i/t)^{r_i/t},
\]
with the convention that $0^0 = 1$.  Multiplying both sides by $t$ and then
raising to the power of $t$ gives
\[
t^t e^{-t H_\R(\ppi)} = \prod_i r_i^{r_i}.
\]
By the analogous equation for $\ggamma$ and the assumption that $H_\R(\ppi)
= H_\R(\ggamma)$, it follows that
\[
\prod_i r_i^{r_i} = \prod_j s_j^{s_j}.
\]
Lemma~\ref{lemma:p-prod-par} now gives
\[
\sum_i \partial(r_i) = \sum_j \partial (s_j)
\]
in $\Zp$.  Since $\sum r_i = t = \sum s_j$, it follows that
\[
\sum_i \partial(r_i) - \partial \Biggl( \sum_i r_i \Biggr)
=
\sum_j \partial(s_j) - \partial \Biggl( \sum_j s_j \Biggr).
\]
But $t \equiv 1 \pmod{p}$, so $r_i$ represents the element $r_i/t = \pi_i$
of $\Zp$, so by Lemma~\ref{lemma:ent-partial}, the left-hand side of this
equation is $H_p(\ppi)$.  Similarly, the right-hand side is
$H_p(\ggamma)$. Hence $H_p(\ppi) = H_p(\ggamma)$.
\end{proof}


It follows that Kontsevich's residue classes of real entropies are
well-defined.  That is, writing
\[
\Rentp
=
\bigcup_{n = 1}^\infty
\Bigl\{ H_\R(\ppi) \such \ppi \in \Rat{n}{p} \Bigr\}
\sub \R,
\]
there is a unique map of sets
\[
[\,\cdot\,] \from \Rentp \to \Zp
\]
such that $[H_\R(\ppi)] = H_p(\ppi)$ for all $\ppi \in
\Rat{n}{p}$ and $n \geq 1$.  We now show that this map is additive, as the
word `residue' leads one to expect.

\begin{propn}
\index{residue class!additivity of}
% 
The set $\Rentp$ is closed under addition, and the residue map
\[
\begin{array}{cccc}
{}[\,\cdot\,] \from     &     
\Rentp  &
\to     &
\Zp     \\
&
H_\R(\ppi)      &
\mapsto &
H_p(\ppi)
\end{array}
\]
preserves addition.
\end{propn}

\begin{proof}
Let $\ppi \in \Rat{n}{p}$ and $\ggamma \in \Rat{m}{p}$.  We must
show that $H_\R(\ppi) + H_\R(\ggamma) \in \Rentp$ and 
\[
[H_\R(\ppi) + H_\R(\ggamma)] 
=
[H_\R(\ppi)] + [H_\R(\ggamma)].
\]
Evidently $\ppi \otimes \ggamma \in \Rat{nm}{p}$, so by the logarithmic
property of $H_\R$,
\[
H_\R(\ppi) + H_\R(\ggamma)
=
H_\R(\ppi \otimes \ggamma)
\in \Rentp. 
\]
Now also using the logarithmic property of $H_p$
(Corollary~\ref{cor:ent-p-loglike}),
% 
\begin{align*}
[H_\R(\ppi) + H_\R(\ggamma)]    
&
= 
[H_\R(\ppi \otimes \ggamma)]    \\
&
=
H_p(\ppi \otimes \ggamma)       \\
&
=
H_p(\ppi) + H_p(\ggamma)        \\
&
=
[H_\R(\ppi)] + [H_\R(\ggamma)],
\end{align*}
% 
as required.
\end{proof}



\section{Polynomial approach}
\lbl{sec:p-poly}

There is an alternative approach to entropy modulo a prime.  It repairs a
defect of the approach above: that in order to define the entropy of a
distribution $\ppi$ over $\Zp$, we had to step outside $\Zp$ to make
arbitrary choices of integers representing the probabilities $\pi_i$, then
show that the definition was independent of those choices.  We now show how
to define $H(\ppi)$ directly as a function of $\pi_1, \ldots, \pi_n$.

Inevitably, that function is a polynomial, by the classical fact that every
function $\fld^n \to \fld$ on a finite field $\fld$ is induced by some
polynomial in $n$ variables.  Indeed, there is a \emph{unique} such
polynomial whose degree in each variable is strictly less than the order of
the field:

\begin{lemma}
\lbl{lemma:fn-fin-fld}%
\index{finite field, functions on}%
\index{polynomial!finite field@over finite field}%
% 
Let $\fld$ be a finite field with $q$ elements, let $n \geq 0$, and let $F
\from \fld^n \to \fld$ be a function.  Then there is a unique polynomial
$f$ of the form
% 
\begin{equation*}
% \lbl{eq:poly-small-deg}
f(x_1, \ldots, x_n)
=
\sum_{0 \leq r_1, \ldots, r_n < q} c_{r_1, \ldots, r_n} 
x_1^{r_1} \cdots x_n^{r_n}
\end{equation*}
% 
($c_{r_1, \ldots, r_n} \in \fld$) such that
\[
f(\pi_1, \ldots, \pi_n) = F(\pi_1, \ldots, \pi_n)
\]
% 
for all $\pi_1, \ldots, \pi_n \in \fld$.
\end{lemma}

\begin{proof}
See Appendix~\ref{sec:fff}.
\end{proof}

In particular, taking $\fld = \Zp$, entropy modulo~$p$ can be expressed as
a polynomial of degree less than $p$ in each variable.  We now 
identify such a polynomial.

For each $n \geq 1$, define $h(x_1, \ldots, x_n) \in (\Zp)[x_1, \ldots,
  x_n]$ by 
\[
\ntn{hpoly}
h(x_1, \ldots, x_n)
=
-\sum_{\substack{0 \leq r_1, \ldots, r_n < p\\ r_1 + \cdots + r_n = p}}
\frac{x_1^{r_1} \cdots x_n^{r_n}}{r_1! \cdots r_n!}.
\]

\begin{propn}
\lbl{propn:ent-p-eqv}%
\index{entropy!modulo a prime!polynomial form}%
% 
For all $n \geq 1$ and $(\pi_1, \ldots, \pi_n) \in \Pi_n$,
\[
H(\pi_1, \ldots, \pi_n) 
=
h(\pi_1, \ldots, \pi_n).
\]
\end{propn}

\begin{proof}
Let $\pi_1, \ldots, \pi_n \in \Zp$.  We will show that whenever $a_1,
\ldots, a_n$ are integers representing $\pi_1, \ldots, \pi_n$, then
% 
\begin{equation}
\lbl{eq:Hh}
\frac{1}{p} \Biggl( 
\Biggl( \sum_{i = 1}^n a_i \Biggr)^p - \sum_{i = 1}^n a_i^p
\Biggr)
\end{equation}
% 
is an integer representing $h(\pi_1, \ldots, \pi_n)$.  The result will
follow, since if $\ppi \in \Pi_n$ then $\sum \pi_i = 1$, so $(\sum a_i)^p
\equiv 1 \pmod{p^2}$ by Lemma~\ref{lemma:pps}.

We have to prove that
\[
\Biggl( \sum_{i = 1}^n a_i \Biggr)^p - \sum_{i = 1}^n a_i^p 
\equiv
-p
\sum_{\substack{0 \leq r_1, \ldots, r_n < p\\ r_1 + \cdots + r_n = p}}
\frac{a_1^{r_1} \cdots a_n^{r_n}}{r_1! \cdots r_n!}
\pmod{p^2}.
\]
Since $(p - 1)!$ is invertible in $\Zps$, an equivalent statement is that
% 
\begin{equation}
\lbl{eq:ent-p-eqv-1}
(p - 1)! \Biggl( \sum_{i = 1}^n a_i^p  
- \Biggl( \sum_{i = 1}^n a_i \Biggr)^p \Biggr)
\equiv
\sum_{\substack{0 \leq r_1, \ldots, r_n < p\\ r_1 + \cdots + r_n = p}}
\frac{p!}{r_1! \cdots r_n!}
a_1^{r_1} \cdots a_n^{r_n}
\pmod{p^2}.
\end{equation}
% 
The right-hand side is $\bigl(\sum a_i\bigr)^p - \sum a_i^p$, so
equation~\eqref{eq:ent-p-eqv-1} reduces to
\[
\bigl( (p - 1)! + 1 \bigr)
\Biggl( \sum_{i = 1}^n a_i^p - \Biggl( \sum_{i = 1}^n a_i \Biggr)^p \Biggr)
\equiv
0
\pmod{p^2}.
\]
And since $(p - 1)! \equiv - 1 \pmod{p}$ and $\sum a_i^p \equiv \sum a_i
\equiv (\sum a_i)^p \pmod{p}$, this is true.
\end{proof}

% The polynomial $h$ is homogeneous of degree $p$, so the induced function on
% $(\Zp)^n$ is a degree~$1$ homogeneous extension $\bar{H} \from (\Zp)^n \to
% \Zp$ of the entropy function $H \from \Pi_n \to \Zp$.  

We have shown that $h(\pi_1, \ldots, \pi_n) = H(\pi_1, \ldots, \pi_n)$
whenever $\sum \pi_i = 1$.  Lemma~\ref{lemma:fn-fin-fld} does not imply
that $h$ is the unique such polynomial of degree less than $p$ in each
variable, since this equation is only stated (and $H$ is only defined) in
the case where the arguments sum to $1$.  However, $h$ has further good
properties.  It is homogeneous of degree $p$, which implies that the
polynomial function $\bar{H} \from (\Zp)^n \to \Zp$ induced by $h$ is
homogeneous of degree $1$.  In fact,
\[
\bar{H}(\ppi) 
= 
\sum_{i = 1}^n \partial(a_i) 
- \partial\Biggl( \sum_{i = 1} a_i \Biggr).
\]
(This follows from the fact that the integer~\eqref{eq:Hh} represents
$h(\pi_1, \ldots, \pi_n)$.)  So in the light of
Lemma~\ref{lemma:ent-partial} and the explanation that follows it, $h$ is
the natural choice of polynomial representing entropy mod~$p$.

We now establish several polynomial identities satisfied by $h$, which are
stronger than the functional equations previously proved for $H$.  The
first is closely related to the chain rule, as we shall see.

\begin{thm}
\lbl{thm:p-grouping}
Let $n, k_1, \ldots, k_n \geq 0$.  Then $h$ satisfies the following
identity of polynomials in commuting variables $x_{ij}$ over $\Zp$:
% 
\begin{multline*}
h(x_{11}, \ldots, x_{1k_1}, 
\ \ldots, \ 
x_{n1}, \ldots, x_{nk_n})
\\
=
h\bigl( x_{11} + \cdots + x_{1k_1}, \ \ldots, \
x_{n1} + \cdots + x_{nk_n} \bigr)
+
\sum_{i = 1}^n h(x_{i1}, \ldots, x_{ik_i}).
\end{multline*}
\end{thm}

\begin{proof}
The left-hand side of this equation is equal to
% 
\begin{equation}
\lbl{eq:ch-poly-1}
-\sum_{\substack{0 \leq s_1, \ldots, s_n \leq p\\ s_1 + \cdots + s_n = p}}
\sum
\frac{x_{11}^{r_{11}} \cdots x_{1k_1}^{r_{1k_1}} \ \cdots \ 
x_{n1}^{r_{n1}} \cdots x_{nk_n}^{r_{nk_n}}}%
{r_{11}! \cdots r_{1k_1}! \ \cdots \ r_{n1}! \cdots r_{nk_n}!},
\end{equation}
% 
where the inner sum is over all $r_{11}, \ldots, r_{nk_n}$ such
that $0 \leq r_{ij} < p$ and 
\[
r_{11} + \cdots + r_{1k_1} = s_1,
\ \ldots,\ 
r_{n1} + \cdots + r_{nk_n} = s_n.
% r_{i1} + \cdots + r_{ik_i} = s_i
\]
% for each $i \in \{1, \ldots, n\}$.  
Split the outer sum into two parts, the
first consisting of the summands in which none of $s_1, \ldots, s_n$ is
equal to $p$, and the second consisting of the summands in which one $s_i$
is equal to $p$ and the others are zero.  Then the
polynomial~\eqref{eq:ch-poly-1} is equal to $A + B$, where
% 
\begin{align*}
A       &
=
-\sum_{\substack{0 \leq s_1, \ldots, s_n < p\\ s_1 + \cdots + s_n = p}}
\prod_{i = 1}^n
\sum_{\substack{r_{i1}, \ldots, r_{ik_i} \geq 0\\ 
r_{i1} + \cdots + r_{ik_i} = s_i}}
\frac{x_{i1}^{r_{i1}} \cdots x_{ik_i}^{r_{ik_i}}}%
{r_{i1}! \cdots r_{ik_i}!},     \\
B       &
=
-\sum_{i = 1}^n 
\sum_{\substack{0 \leq r_{i1}, \ldots, r_{ik_i} < p\\ 
r_{i1} + \cdots + r_{ik_i} = p}}
\frac{x_{i1}^{r_{i1}} \cdots x_{ik_i}^{r_{ik_i}}}%
{r_{i1}! \cdots r_{ik_i}!}.
\end{align*}
% 
We have
% 
\begin{align*}
A       &
=
-\sum_{\substack{0 \leq s_1, \ldots, s_n < p\\ s_1 + \cdots + s_n = p}}
\frac{1}{s_1! \cdots s_n!}
\prod_{i = 1}^n
\sum_{\substack{r_{i1}, \ldots, r_{ik_i} \geq 0\\ 
r_{i1} + \cdots + r_{ik_i} = s_i}}
\frac{s_i!}{r_{i1}! \cdots r_{ik_i}!}      
x_{i1}^{r_{i1}} \cdots x_{ik_i}^{r_{ik_i}}  \\
&
=
-\sum_{\substack{0 \leq s_1, \ldots, s_n < p\\ s_1 + \cdots + s_n = p}}
\frac{1}{s_1! \cdots s_n!}
\prod_{i = 1}^n
(x_{i1} + \cdots + x_{ik_i})^{s_i}      \\
&
=
h(x_{11} + \cdots + x_{1k_1}, \ \ldots,\ 
x_{n1} + \cdots + x_{nk_n})
\end{align*}
% 
and
\[
B = 
\sum_{i = 1}^n h(x_{i1}, \ldots, x_{ik_i}).
\]
The result follows.
\end{proof}

We easily deduce the polynomial form of the chain rule.

\begin{cor}[Chain rule]
\lbl{cor:p-chn-poly}%
\index{chain rule!modulo a prime}%
%
Let $n, k_1, \ldots, k_n \geq 0$.  Then $h$ satisfies the following
identity of polynomials in commuting variables $y_i$, $x_{ij}$ over $\Zp$:
% 
\begin{multline*}
h(y_1 x_{11}, \ldots, y_1 x_{1k_1}, 
\ \ldots, \ 
y_n x_{n1}, \ldots, y_n x_{nk_n})
\\
=
h\bigl( y_1(x_{11} + \cdots + x_{1k_1}), \ \ldots, \
y_n(x_{n1} + \cdots + x_{nk_n}) \bigr)
+
\sum_{i = 1}^n y_i^p h(x_{i1}, \ldots, x_{ik_i}).
\end{multline*}
\end{cor}

\begin{proof}
This follows from Theorem~\ref{thm:p-grouping} by substituting $y_i x_{ij}$
for $x_{ij}$ then using the degree $p$ homogeneity of $h$.
\end{proof}

This polynomial identity provides another proof of the chain rule for
entropy mod~$p$: given $\ggamma \in \Pi_n$ and $\ppi^i \in \Pi_{k_i}$ as in
Proposition~\ref{propn:chn-p}, substitute $y_i = \gamma_i$ and $x_{ij} =
\pi^i_j$, then use the facts that $\sum_j \pi^i_j = 1$ and $\gamma_i^p =
\gamma_i$ for each $i$.  (Here $i$ is a superscript and $p$ is a power.)

The entropy polynomial $h(x)$ in a single variable is $0$, by definition.
But the entropy polynomial in two variables has important properties:

\begin{cor}
\lbl{cor:cocycle}
The two-variable entropy polynomial $h$ satisfies the cocycle%
%
\index{cocycle identity}
% 
condition
\[
h(x, y) - h(x, y + z) + h(x + y, z) - h(y, z) = 0
\]
as a polynomial identity.
\end{cor}

Similar results appear in Cathelineau~\cite{CathSHS} (p.~58--59),%
%
\index{Cathelineau, Jean-Louis}
% 
Kontsevich~\cite{KontOHL},%
%
\index{Kontsevich, Maxim} 
% 
and Elbaz--Vincent%
%
\index{Elbaz-Vincent, Philippe} 
% 
and Gangl~\cite{EVGFPM}%
%
\index{Gangl, Herbert}
% 
(Section~2.3), and can be understood through the information
cohomology of Baudot, Bennequin and Vigneaux~\cite{BaBe,VignTSS}.

\begin{proof}
Theorem~\ref{thm:p-grouping} with $n = 2$ and $(k_1, k_2) = (2, 1)$ gives
\[
h(x, y, z) = h(x + y, z) + h(x, y)
\]
(since $h(z)$ is the zero polynomial), and similarly,
\[
h(x, y, z) = h(x, y + z) + h(y, z).
\]
The result follows.
\end{proof}

We are especially interested in the case where the arguments of the entropy
function sum to $1$, and under that restriction, $h(x, y)$ reduces to a
simple form:

\begin{lemma}
\lbl{lemma:p-to-two}
If $p \neq 2$, there is an identity of polynomials
\[
h(x, 1 - x) = \sum_{r = 1}^{p - 1} \frac{x^r}{r},
\]
and if $p = 2$, there is an identity of polynomials 
\[
h(x, 1 - x) = x + x^2.
\]
\end{lemma}

\begin{proof}
The case $p = 2$ is trivial.  Suppose, then, that $p > 2$.  
% 
The result can be proved by direct calculation, but we shorten the proof 
using Example~\ref{eg:p-bin}, which implies that
\[
h(\pi, 1 - \pi) = \sum_{r = 1}^{p - 1} \frac{\pi^r}{r}
\]
for all $\pi \in \Zp$.  We now want to prove that this is a
\emph{polynomial} identity, not just an equality of functions.  By
Lemma~\ref{lemma:fn-fin-fld}, it suffices to show that the polynomial
\[
h(x, 1 - x) 
= 
-\sum_{r = 1}^{p - 1} \frac{x^r(1 - x)^{p - r}}{r!(p - r)!}
\]
has degree strictly less than $p$.  Since it plainly has degree at most
$p$, we need only show that the coefficient of $x^p$ vanishes.  That
coefficient is 
\[
-\sum_{r = 1}^{p - 1} \frac{(-1)^{p - r}}{r! (p - r)!}.
\]
For $1 \leq r \leq p - 1$,
\[
- \frac{(-1)^{p - r}}{r! (p - r)!}
=
(-1)^{p - r} \frac{(p - 1)!}{r! (p - r)!}
=
(-1)^{p - r} \frac{1}{r} \binom{p - 1}{r - 1}
=
(-1)^{p - 1} \frac{1}{r}
\]
in $\Zp$, using first the fact that $(p - 1)! = -1$ and then
Lemma~\ref{lemma:p-binom}.  Hence the coefficient of $x^p$ in $h(x, 1 - x)$
is
\[
(-1)^{p - 1} \sum_{r \in (\Zp)^\times} \frac{1}{r}.
\]
But $r \mapsto 1/r$ defines a permutation of $(\Zp)^\times$, so the sum
here is equal to $\sum_{r = 1}^{p - 1} r$, which is $0$ since $p$ is odd.
\end{proof}

Following Elbaz-Vincent%
%
\index{Elbaz-Vincent, Philippe} 
% 
and Gangl~\cite{EVGOPI},%
%
\index{Gangl, Herbert}
% 
we write
% 
\begin{equation}
% \label{eq:sterling}
\ntn{sterling}
\pounds_1(x) 
= 
h(x, 1 - x)
=
\begin{cases}
\sum_{r = 1}^{p - 1} x^r/r      &\text{if } p \neq 2,   \\
x + x^2                         &\text{if } p = 2.
\end{cases}
\end{equation}
% 
(Elbaz-Vincent and Gangl omitted the case $p = 2$.)  The
function $\pounds_1$ is the mod~$p$ analogue of the real function
% 
\begin{equation}
\lbl{eq:real-two}
x 
\mapsto 
H_\R(x, 1 - x)
=
- x \log x - (1 - x) \log(1 - x).
\end{equation}
% 
This may be a surprise, given the lack of formal resemblance between the
expressions~\eqref{notn:sterling} and~\eqref{eq:real-two}.  Indeed, the
polynomial $\sum_{r = 1}^{p - 1} x^r/r$ is the truncation of the power
series of $-\log(1 - x)$, not~\eqref{eq:real-two}.  Nevertheless, the
Faddeev theorem and its mod~$p$ counterpart (Theorems~\ref{thm:faddeev}
and~\ref{thm:fad-p}) establish a tight analogy between the entropy
functions over $\R$ and $\Zp$.

It is immediate from the definition of $h$ that it is a symmetric
polynomial, so there is a polynomial identity
% 
\begin{equation}
\lbl{eq:pounds-sym}
\pounds_1(x) = \pounds_1(1 - x).
\end{equation}
% 
The polynomial $\pounds_1$ also satisfies a more complicated identity,
whose significance will be explained shortly.  Following
Kontsevich~\cite{KontOHL}, Elbaz-Vincent and Gangl proved: 
  
\begin{propn}[Elbaz-Vincent and Gangl]
\lbl{propn:p-feith}%
\index{Elbaz-Vincent, Philippe}%
\index{Gangl, Herbert}%
%
There is a polynomial identity
\[
\pounds_1(x) + (1 - x)^p \pounds_1\biggl( \frac{y}{1 - x} \biggr)
=
\pounds_1(y) + (1 - y)^p \pounds_1\biggl( \frac{x}{1 - y} \biggr).
\]
\end{propn}

Both sides of this equation are indeed polynomials, since $\pounds_1$ has
degree at most $p$.  Elbaz-Vincent and Gangl proved it using differential
equations (Proposition~5.9(2) of~\cite{EVGOPI}), but it also follows easily
from the cocycle identity:

\begin{proof}
We work in the field of rational expressions over $\Zp$ in commuting
variables $x$ and $y$.  Since $h$ is homogeneous of degree $p$,
\[
h(x, y) = (x + y)^p \pounds_1\biggl( \frac{x}{x + y} \biggr).
\]
The identity to be proved is, therefore, equivalent to
\[
h(x, 1 - x) + h(y, 1 - x - y)
=
h(y, 1 - y) + h(x, 1 - x - y).
\]
Since $h$ is symmetric, this in turn is equivalent to
\[
h(x, 1 - x - y) - h(x, 1 - x) + h(1 - y, y) - h(1 - x - y, y) = 0,
\]
which is an instance of the cocycle identity
of Corollary~\ref{cor:cocycle}. 
\end{proof}

Proposition~\ref{propn:p-feith} can be understood as follows.  Any
probability distribution mod~$p$ can be expressed as an iterated composite
of distributions on two elements.  Hence, the entropy of any distribution
can be computed in terms of entropies $H(\pi, 1 - \pi)$ of distributions on
two elements, using the chain rule.  In this sense, the sequence of
functions
\[
\bigl( H \from \Pi_n \to \Zp \bigr)_{n \geq 1}
\]
reduces to the single function $H \from \Pi_2 \to \Zp$, which is
effectively a function in one variable:
\[
\begin{array}{cccc}
F\from  &\Zp    &\to            &\Zp    \\
        &\pi    &\mapsto        &H(\pi, 1 - \pi).
\end{array}
\]
The same is true over $\R$: the sequence of functions $(H \from \Delta_n
\to \R)_{n \geq 1}$ reduces to the single function $F \from [0, 1] \to \R$
defined by $F(\pi) = H(\pi, 1 - \pi)$.

On the other hand, given an arbitrary function $F \from \Zp \to \Zp$, one
cannot generally extend it to a sequence of functions $(\Pi_n \to \Zp)_{n
  \geq 1}$ satisfying the chain rule (nor, similarly, in the real case).
Indeed, by expressing a distribution $(\pi, 1 - \pi - \tau, \tau)$ as a
composite in two different ways, we obtain an equation that $F$ must
satisfy if such an extension is to exist.  Assuming the symmetry property
$F(\pi) = F(1 - \pi)$, that equation is
% 
\begin{equation}
\lbl{eq:feith}
F(\pi) + (1 - \pi) F\biggl(\frac{\tau}{1 - \pi}\biggr)
=
F(\tau) + (1 - \tau) F\biggl(\frac{\pi}{1 - \tau}\biggr)
\end{equation}
% 
($\pi, \tau \neq 1$).  When the function $F$ is $\pi \mapsto H(\pi, 1 -
\pi)$, equation~\eqref{eq:feith} also follows from
Proposition~\ref{propn:p-feith}.

Equation~\eqref{eq:feith} is sometimes called the \demph{fundamental%
%
\index{fundamental equation of information theory} 
%
equation of information theory}.  (Over $\R$, this functional equation has
been studied since at least the 1958 work of Tverberg~\cite{Tver}. The name
seems to have come later, and appears in Acz\'el and Dar\'oczy's 1975
book~\cite{AcDa}.)  Assuming that $F$ is symmetric, it is the only obstacle
to the extension problem, in the sense that if $F$ satisfies the
fundamental equation then the extension can be performed.

In the real case, the function~\eqref{eq:real-two} is a solution of the
fundamental equation.  In fact, up to a scalar multiple, it is the
\emph{only} measurable solution $F$ of the fundamental equation such that
$F(0) = F(1)$ (Corollary~3.4.22 of Acz\'el and Dar\'oczy~\cite{AcDa}).  It
can be deduced that up to a constant factor, Shannon entropy for finite
real probability distributions is characterized uniquely by measurability,
symmetry and the chain rule.  This is the 1964 theorem of Lee%
%
\index{Lee, Pan-Mook} 
%
mentioned in Remark~\ref{rmks:faddeev}\bref{rmk:faddeev-lee},
proofs of which can be found in Lee~\cite{Lee} and Acz\'el and
Dar\'oczy~\cite{AcDa} (Corollary~3.4.23).

In the mod~$p$ case, we know that the function $F = \pounds_1$ is symmetric
and satisfies the fundamental equation.  Since any such function $F$ can be
extended to a sequence of functions $\Pi_n \to \Zp$ satisfying the chain
rule, it follows from Theorem~\ref{thm:fad-p} that up to a constant factor,
$\pounds_1$ is unique with these properties.

Kontsevich~\cite{KontOHL}%
%
\index{Kontsevich, Maxim} 
% 
proposed calling $\pounds_1$ the \demph{$1\hlf$-logarithm},%
%
% \index{1-logarithm@$1\hlf$-logarithm}%
\index{one-logarithm@$1\hlf$-logarithm} 
%
because the ordinary logarithm satisfies a three-term functional equation
($\log(xy) = \log x + \log y$), the dilogarithm satisfies a five-term
functional equation (as in Section~2 of Zagier~\cite{Zagi} or
Proposition~3.5 of Elbaz-Vincent and Gangl~\cite{EVGOPI}), and the
$1\hlf$-logarithm satisfies the four-term functional
equation~\eqref{eq:feith}.
%  (Proposition~\ref{propn:p-feith}).

\begin{remark}
\lbl{rmk:kont-comp}
In his seminal note~\cite{KontOHL}, Kontsevich%
%
\index{Kontsevich, Maxim} 
% 
unified the real and mod~$p$ cases with a homological argument, using a
cocycle%
%
\index{cocycle identity}
% 
identity equivalent to the one in Corollary~\ref{cor:cocycle}.  In doing so,
he established that $\sum_{0 < r < p} \pi^r/r$ is the correct formula for
the entropy of a distribution $(\pi, 1 - \pi)$ mod $p$ on two
elements (assuming, as he did, that $p \neq 2$).  Although he gave no
definition of the entropy of a distribution mod $p$ on an
arbitrary number of elements, his arguments showed that a unique reasonable
such definition must exist.

In this chapter, we have developed the framework hinted at by Kontsevich,
and also provided the definition and characterization of information
loss mod~$p$.  Two further features of this theory are apparent. The first
is the streamlined inclusion of the case $p = 2$.  The second is the
dropping of all symmetry%
%
\index{Faddeev, Dmitry!mod p analogue of entropy theorem@mod $p$ analogue of entropy theorem!role of symmetry in}%
\index{symmetry in Faddeev-type theorems}
%
requirements.  In axiomatic approaches to entropy based on the fundamental
equation of information theory~\eqref{eq:feith}, such as those of
Lee~\cite{Lee} and Kontsevich, the symmetry axiom $F(\pi) = F(1 - \pi)$ is
essential.  Indeed, $F(\pi) = \pi$ is also a solution of~\eqref{eq:feith},
and the polynomial identity of Proposition~\ref{propn:p-feith} is also
satisfied by $x^p$ in place of $\pounds_1(x)$.  The symmetry axiom is used
to rule out these and other undesired solutions.  This is why Lee's
characterization of real entropy $H$ needed the assumption that $H$ is a
symmetric function.  In contrast, symmetry is needed nowhere in the
approach taken here.
\end{remark}





