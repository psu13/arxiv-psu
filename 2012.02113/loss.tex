\chapter{Information loss}
\lbl{ch:loss}
\index{information loss}


\begin{quote}%
\index{Grothendieck, Alexander}%
\index{Katz, Nicholas}
% 
Grothendieck came along and said, `No, the Riemann--Roch%
%
\index{Riemann, Bernhard!Roch theorem@--Roch theorem} 
% 
theorem is \emph{not} a theorem about varieties, it's a theorem about
morphisms between varieties.'  
% 
\hfill 
% 
-- Nicholas Katz (quoted in \cite{JackCAN}, p.~1046).
\end{quote}

\noindent
This short chapter tells the following story.
% 
A measure-preserving map between finite probability spaces can be regarded
as a deterministic%
%
\index{deterministic process} 
% 
process.  As such, it loses information.  We can attempt to quantify how
much information is lost.  It turns out that as soon as we impose a few
reasonable requirements on this quantity, it is highly constrained: up to a
constant factor, it must be the difference between the entropies of the
domain and the codomain.  That is our main theorem.

This result is essentially another characterization of Shannon entropy, and
first appeared in a 2011 paper of Baez, Fritz and Leinster~\cite{CETIL}.
The broad idea is to shift the focus from \emph{objects}%
%
\index{objects vs.\ maps} 
% 
(finite probability spaces) to \emph{maps}%
%
\index{maps vs.\ objects} 
% 
between objects (measure-preserving maps).  Entropy is an invariant of
finite probability spaces; information loss is an invariant of
measure-preserving maps.  The shift of emphasis from objects to maps is
integral to category theory, and has borne fruit such as the
Grothendieck--Riemann--Roch theorem alluded to in the opening quotation, as
well as the considerably more humble characterization of information loss
described here.

In full categorical generality, a map $\Xx \toby{f} \Yy$ of any kind can be
viewed as an object $\Xx$ parametrized by another object $\Yy$.
An object $\Xx$ can be viewed as a map of a special kind, namely, the
unique map $\Xx \toby{!_\Xx} 1$ to the terminal object $1$ of the category
concerned.  In the case at hand, we associate with any probability
space $\Xx$ the unique measure-preserving map $\Xx \toby{!_\Xx} 1$ to the
one-point space $1$, and the information loss of the map $!_\Xx$ is equal
to the entropy of the space $\Xx$.  Thus, entropy is a special case of
information loss.

An advantage of working with information loss rather than entropy (that is,
maps rather than objects) is that the characterization theorems take on a
new simplicity.  For instance, the conditions in our main result
(Theorem~\ref{thm:cetil}) look just like the linearity or homomorphism
conditions that appear throughout mathematics.  In contrast, the
chain%
%
\index{chain rule!complicated nature of} 
% 
rule for entropy, while justifiable in many other ways, has a more
complicated algebraic form.

We begin with a review of measure-preserving maps, then define information
loss (Section~\ref{sec:meas-pres}).  After recording a few simple
properties of information loss, we prove that they characterize it uniquely
(Section~\ref{sec:cil}).  An analogous and even simpler result is then
proved for $q$-logarithmic information loss ($q \neq 1$).  Both of these
theorems first appeared in the 2011 paper of Baez, Fritz and
Leinster~\cite{CETIL}.


\section{Measure-preserving maps}
\lbl{sec:meas-pres}


So far in this text, we have focused on probability distributions on finite
sets of the special form $\{1, \ldots, n\}$.  Here, it is convenient to use
arbitrary finite sets.  The difference is cosmetic, but does cause some
shifts in notation, as follows.

\begin{defn}
\begin{enumerate}
\item 
Let $\Xx$ be a finite set.  A \demph{probability%
% 
\index{probability distribution} 
% 
distribution} $\p$ on $\Xx$ is a family $(p_i)_{i \in \Xx}$ of nonnegative
real numbers such that $\sum_{i \in \Xx} p_i = 1$.  We write
$\Delta_\Xx$\ntn{DeltaX} for the set of probability distributions on $\Xx$.

\item
A \demph{finite probability%
%
\index{probability space} 
%
space} is a pair $(\Xx, \p)$ where $\Xx$ is a finite set and $\p \in
\Delta_\Xx$.
\end{enumerate}
\end{defn}

The set $\Delta_\Xx$ is topologized as a subspace of the product space
$\R^\Xx$. 

\begin{defn}
\lbl{defn:meas-pres}
Let $(\Yy, \vc{s})$ and $(\Xx, \p)$ be finite probability spaces.  A
\demph{measure-preserving%
% 
\index{measure-preserving map} 
% 
map} $(\Yy, \vc{s}) \to (\Xx, \p)$ is a function $f \from \Yy \to \Xx$ such
that
% 
\begin{equation}
\lbl{eq:meas-pres}
p_i = \sum_{j \in f^{-1}(i)} s_j
\end{equation}
% 
for all $i \in \Xx$.  
\end{defn}

An equivalent statement is that $f \from (\Yy, \vc{s}) \to (\Xx, \p)$ is
measure-preserving if and only if
% 
\begin{equation}
\lbl{eq:meas-pres-full}
\sum_{i \in \VV} p_i = \sum_{j \in f^{-1}\VV} s_j
\end{equation}
% 
for all $\VV \sub \Xx$.  Indeed, \eqref{eq:meas-pres}~is the case
of~\eqref{eq:meas-pres-full} where $\VV = \{i\}$,
and~\eqref{eq:meas-pres-full} follows from~\eqref{eq:meas-pres} by summing
over all $i \in \VV$.  

\begin{remarks}
\lbl{rmks:finprob}
\begin{enumerate}
\item 
\lbl{rmk:finprob-pf} 
For any finite probability space $(\Yy, \vc{s})$ and function $f$ from
$\Yy$ to another finite set $\Xx$, there is an induced probability
distribution $f \vc{s}$ on $\Xx$, the \dmph{pushforward} of $\vc{s}$ along
$f$.  It is defined by the obvious generalization of
Definition~\ref{defn:pfwd}:
\[
(f \vc{s})_i = \sum_{j \in f^{-1}(i)} s_j
\ntn{pfwdgen}
\]
($i \in \Xx$).  In these terms, a function $f \from (\Yy, \vc{s}) \to (\Xx, \p)$
is measure-preserving if and only if $f \vc{s} = \p$.

\item
Finite probability spaces and measure-preserving maps form a category
$\FinProb$\ntn{FinProb}.  We note in passing that by~\bref{rmk:finprob-pf},
the forgetful functor $\FinProb \to \FinSet$ is a discrete opfibration.  In
fact, $\FinProb$ is the category of elements of the functor $\FinSet \to
\Set$ defined on objects by $\Xx \mapsto \Delta_\Xx$ and on maps by
pushforward. (For the categorical terminology used here, see for instance 
Riehl~\cite{RiehCTC}, Definition~2.4.1 and Exercise~2.4.viii.)
\end{enumerate}
\end{remarks}

Although a measure-preserving map need not be literally surjective, it is
essentially so, in the sense that all elements not in the image have
probability zero.

\begin{example}
\lbl{eg:meas-pres-surj}
Let $\Yy = \{ \as{a}, \as{\`a}, \as{\^a}, \as{b}, \as{c}, \as{\c{c}},
\ldots \}$ be the set of symbols in the French%
% 
\index{French language}
%  
language, and let $\vc{s} \in \Delta_\Yy$ be their frequency distribution
(as in Example~\ref{eg:comp-french}).  Let $\Xx = \{ \as{a}, \as{b},
\as{c}, \ldots \}$ be the 26-element set of letters, and $\p \in
\Delta_\Xx$ their frequency distribution.  There is a function $f \from \Yy
\to \Xx$ that forgets accents; for instance, $f(\as{a}) = f(\as{\`a}) =
f(\as{\^a}) = \as{a}$.  Then $f \from (\Yy, \vc{s}) \to (\Xx, \p)$ is
measure-preserving and surjective.
\end{example}

\begin{example}
\lbl{eg:meas-pres-incl}
Let $\ell$ be the inclusion function $\{1\} \incl \{1, 2\}$.  Give $\{1\}$
its unique probability distribution $(1) = \vc{u}_1$, and give $\{1, 2\}$
the distribution $(1, 0)$.  Then $\ell$ is measure-preserving but not
surjective. 
\end{example}

Any measure-preserving map between finite probability spaces can be
factorized%
%
\index{measure-preserving map!factorization of} 
%  
canonically into maps of the two types in these two examples: a
surjection followed by a subset inclusion, where the subset concerned has
total probability~$1$.  Specifically, $f \from (\Yy, \vc{s}) \to (\Xx, \p)$
factorizes as
\[
(\Yy, \vc{s}) \toby{f'} (f\Yy, \p') \toby{\ell} (\Xx, \p),
\]
where $\p'$ is the probability distribution on $f\Yy$ defined by
$p'_i = p_i$ for all $i \in f\Yy$, the surjection $f'$ is defined by $f'(j)
= f(j)$ for all $j \in \Yy$, and $\ell$ is inclusion.  

A measure-preserving surjection simply discards information (such as the
accents in Example~\ref{eg:meas-pres-surj}).  It is a coarse-graining, in
the sense of taking finely-grained information (such as letters with
accents) and converting it into more coarsely-grained information (such as
mere letters).
% 
A measure-preserving inclusion is essentially trivial, simply
appending some events of probability zero.

For any measure-preserving bijection $f \from (\Yy, \vc{s}) \to (\Xx, \p)$
between finite probability spaces, the inverse $f^{-1}$ is also
measure-preserving.  We call such an $f$ an \demph{isomorphism},%
%
\index{isomorphism of probability spaces}%
\index{probability space!isomorphism of}
% 
and write $(\Yy, \vc{s}) \iso (\Xx, \p)$.

An important feature of probability spaces is that we can take convex%
%
\index{convex!combination}%
\index{probability space!convex combination of}
% 
combinations of them.  Given $\vc{w} \in \Delta_n$ and finite probability
spaces $(\Xx_1, \p^1), \ldots, (\Xx_n, \p^n)$, we obtain a new probability
space
% 
% \begin{equation}
% \lbl{eq:g-comp}
\[
\Biggl(
\coprod_{i = 1}^n \Xx_i, 
\,
\coprod_{i = 1}^n w_i \p^i
\Biggr),
\ntn{bigccdists}
\]
% \end{equation}
% 
where $\coprod \Xx_i$\ntn{disjtu} is the disjoint union of sets $\Xx_1 \cpd
\cdots \cpd \Xx_n$ and $\coprod w_i \p^i$ is the probability distribution
on $\coprod \Xx_i$ that gives probability $w_i p^i_j$ to an element $j \in
\Xx_i$.

Convex combination of probability spaces is just composition of probability
distributions, translated into different notation.  More exactly, if $\Xx_i
= \{1, \ldots, k_i\}$ then $\coprod \Xx_i$ is in canonical bijection with
$\{1, \ldots, k_1 + \cdots + k_n\}$, and under this bijection, $\coprod w_i
\p^i$ corresponds to the composite distribution $\vc{w} \of (\p^1, \ldots,
\p^n)$.

The construction of convex combinations is functorial,%
%
\lbl{p:conv-comb-func} 
% 
that is, applies not only to probability spaces but also to maps between
them.  Indeed, take measure-preserving maps
% 
\begin{eqnarray*}
(\Yy_1, \vc{s}^1) &\toby{f_1}& (\Xx_1, \p^1)   \\
\vdots& &\vdots \\
(\Yy_n, \vc{s}^n) &\toby{f_n}& (\Xx_n, \p^n)
\end{eqnarray*}
% 
between finite probability spaces, and a probability distribution $\vc{w}
\in \Delta_n$.  There is a function
\[
\xymatrix@C+1em{
\displaystyle
\coprod_{i = 1}^n \Yy_i 
\ar[r]^{\coprod\limits_{i = 1}^n f_i} &
\displaystyle
\coprod_{i = 1}^n \Xx_i
}
\]
that maps $j \in \Yy_i$ to $f_i(j) \in \Xx_i$, and it is easily checked
that $\coprod f_i$ is a measure-preserving map
% 
\begin{equation}
\lbl{eq:conv-comb-map}
\xymatrix@C+1em{
\displaystyle
\Biggl( \coprod_{i = 1}^n \Yy_i, \, \coprod_{i = 1}^n w_i \vc{s}^i \Biggr)
\ar[r]^{\coprod\limits_{i = 1}^n f_i} &
\displaystyle
\Biggl( \coprod_{i = 1}^n \Xx_i, \, \coprod_{i = 1}^n w_i \vc{p}^i \Biggr).
}
\end{equation}
% 
It will be convenient to use the alternative notation
\[
\coprod_{i = 1}^n w_i f_i
\qquad
\text{or} 
\qquad
w_1 f_1 \cpd \cdots \cpd w_n f_n
\ntn{ccmaps}
\]
for the measure-preserving map $\coprod_{i = 1}^n f_i$
of~\bref{eq:conv-comb-map}. 

We defined Shannon% 
%
\index{Shannon, Claude!entropy}%
\index{entropy!Shannon}
%
entropy only for probability distributions on sets of
the form $\{1, \ldots, n\}$, but, of course, the definition for general
finite probability spaces $(\Xx, \p)$ is
\[
H(\p) 
=
- \sum_{i \in \supp(\p)} p_i \log p_i,
\ntn{Hgen}
\]
where \ntn{suppgen}$\supp(\p) = \{ i \in \Xx \such p_i > 0\}$.  Shannon
entropy is
\demph{isomorphism-invariant},%
%
\index{isomorphism invariant@isomorphism-invariant}
% 
meaning that $H(\p) = H(\vc{s})$ whenever $(\Xx, \vc{p})$ and $(\Yy,
\vc{s})$ are isomorphic finite probability spaces.

Translated into this notation, the chain rule for Shannon entropy
states that
% 
\begin{equation}
\lbl{eq:g-ent-chain}
H\Biggl( \coprod_{i = 1}^n w_i \p^i \Biggr)
=
H(\vc{w}) + \sum_{i = 1}^n w_i H(\p^i)
\end{equation}
% 
for all $\vc{w} \in \Delta_n$ and finite probability spaces $(\Xx_1, \p^1),
\ldots, (\Xx_n, \p^n)$. The continuity property of entropy is that for
each finite set $\Xx$, the function
% 
\begin{equation}
\lbl{eq:g-cts}
\begin{array}{ccc}
\Delta_\Xx        &\to            &\R     \\
\p              &\mapsto        &H(\p)
\end{array}
\end{equation}
% 
is continuous.

We now set out to quantify the information lost by a measure-preserving map
$f$, first exploring through examples how a reasonable definition of
information loss ought to behave.

\begin{example}
If $f$ is an isomorphism then $f$ should lose no information at all.
More generally, the same should be true if $f$ is injective. 
\end{example}

\begin{example}
\lbl{eg:info-loss-toss}
The unique measure-preserving map $(\{1, 2\}, \vc{u}_2) \to (\{1\},
\vc{u}_1)$ forgets the result of a fair coin%
%
\index{coin!toss} 
% 
toss.  Intuitively, then, it loses one bit of information.
\end{example}

\begin{example}
\lbl{eg:info-loss-bang}
More generally, for any finite probability space $(\Xx, \p)$, consider the
unique measure-preserving map
\[
f \from (\Xx, \p)
\to 
\bigl(\{1\}, \vc{u}_1\bigr),
\]
which forgets the result of an observation drawn from the distribution $\p$.
Such an observation contains $\Hi(\p)$ bits of information (in the sense
of Section~\ref{sec:ent-coding}), so the information lost by $f$ should be
$\Hi(\p)$ bits.
\end{example}

\begin{example}
Suppose that I draw fairly from a pack of playing cards,%
%
\index{cards, playing} 
% 
and tell you only the rank (number) of the card chosen.  The
information that I am withholding is the suit, which needs $\log_2 4 = 2$
bits to encode.  Thus, if $f \from \Yy \to \Xx$ is a four-to-one map from a
$52$-element set $\Yy$ to a $13$-element set $\Xx$, and if we equip $\Yy$
and $\Xx$ with their uniform distributions $\vc{u}_\Yy$ and $\vc{u}_\Xx$,
then the information loss of the measure-preserving map $f \from (\Yy,
\vc{u}_\Yy) \to (\Xx, \vc{u}_\Xx)$ should be $2$~bits.
\end{example}

\begin{example}
\lbl{eg:info-loss-french}
Take the measure-preserving map
\[
f \from
\bigl(
\{ \as{a}, \as{\`a}, \as{\^a}, \as{b}, \ldots \}, \vc{s}
\bigr)
\to
\bigl(
\{ \as{a}, \as{b}, \ldots \},
\p
\bigr)
\]
of Example~\ref{eg:meas-pres-surj}, representing the process of forgetting
the accent on a letter in the French%
%
\index{French language} 
% 
language.  There are two quantities that we could reasonably call the
`amount of information lost' by the process $f$.

First, we could condition on the underlying letter.  To do this, we go
through the $26$ letters, we take for each letter the amount of information
lost by forgetting the accent on that letter, and we form the weighted
mean.  Write
\[
\vc{r}^1 \in \Delta_3, \ 
\vc{r}^2 \in \Delta_1, \ 
\ldots, \ 
\vc{r}^{26} \in \Delta_1
\]
for the accent distributions on each letter, so that $\vc{s} = \coprod_{i =
  1}^{26} p_i \vc{r}^i$.  As in Example~\ref{eg:info-loss-bang}, the
amount of information lost by forgetting the accent on an $\as{a}$ (for
instance) should be $\Hi(\vc{r}^1)$ bits.  So, the expected amount of
information lost by forgetting the accent on a random letter should be
% 
\begin{equation}
\lbl{eq:french-loss}
\sum_{i = 1}^{26} p_i \Hi(\vc{r}^i).
\end{equation}
% 
This is one possible definition of the amount of information lost by $f$. 

Alternatively, we could define the information loss to be
the amount of information we had at the start of the process minus the
amount of information that remains at the end.  This is
% 
\begin{equation}
\lbl{eq:french-diff}
\Hi(\vc{s}) - \Hi(\vc{p}).
\end{equation}
% 
But since $\vc{s} = \coprod p_i \vc{r}^i$, the chain
rule~\eqref{eq:g-ent-chain} tells us that the two
quantities~\eqref{eq:french-loss} and~\eqref{eq:french-diff} are equal.
So, our two ways of quantifying information loss are equivalent.
\end{example}

Motivated by these examples, we make the following definition.

\begin{defn}
\lbl{defn:loss}
Let 
\[
f \from (\Yy, \vc{s}) \to (\Xx, \p)
\]
be a measure-preserving map of finite probability spaces.  The
\demph{information%
%
\index{information loss}
% 
loss} of $f$ is
\[
L(f) = H(\vc{s}) - H(\vc{p}).
\ntn{loss}
\]
\end{defn}

As with other entropic quantities that we have encountered, the definition
of information loss depends on a choice of logarithmic base, and
changing that base scales the quantity by a constant factor.

A deterministic%
%
\index{deterministic process}
% 
process cannot create new information, and correspondingly,
information loss is always nonnegative:%
%
\index{information loss!nonnegative@is nonnegative}

\begin{lemma}
\lbl{lemma:loss}
Let $f \from (\Yy, \vc{s}) \to (\Xx, \p)$ be a measure-preserving map of finite
probability spaces.  Then:
% 
\begin{enumerate}
\item
\lbl{part:loss-formula}
% 
$\displaystyle 
L(f)
=
\sum_{j \in \supp(\vc{s})} s_j \log \frac{p_{f(j)}}{s_j}$;

\item
\lbl{part:loss-ineq}
$L(f) \geq 0$.
\end{enumerate}
\end{lemma}

\begin{proof}
By definition of measure-preserving map
(Definition~\ref{defn:meas-pres}), $p_{f(j)} \geq s_j$ for all $j \in \Yy$.
It follows that 
% 
\begin{equation}
\lbl{eq:loss-supp}
j \in \supp(\vc{s}) \implies f(j) \in \supp(\p).
\end{equation}
% 
It also follows that $\log(p_{f(j)}/s_j) \geq 0$ for all $j \in
\supp(\vc{s})$, so part~\bref{part:loss-ineq} will follow once
we have proved~\bref{part:loss-formula}.

To prove~\bref{part:loss-formula}, first note that by definition of
measure-preserving map,
% 
\begin{align*}
H(\p)   &
=
\sum_{i \in \supp(\p)} p_i \log \frac{1}{p_i}   \\
&
=
% \sum_{\substack{i \in \supp(\p), \ j \in \Yy\csuch\\ f(j) = i}}
\sum_{i \in \supp(\p), \ j \in \Yy\csuch f(j) = i}
s_j \log \frac{1}{p_i}  \\
&
=
\sum_{j \csuch f(j) \in \supp(\p)} s_j \log \frac{1}{p_{f(j)}}.
\end{align*}
% 
By~\eqref{eq:loss-supp}, this sum is unchanged if we take $j$ to range over
$\supp(\vc{s})$ instead.  Hence
% 
\begin{align*}
L(f)    &
=
H(\vc{s}) - H(\p)       \\
&
=
\sum_{j \in \supp(\vc{s})} s_j \log \frac{1}{s_j}
- \sum_{j \in \supp(\vc{s})} s_j \log \frac{1}{p_{f(j)}}        \\
&
=
\sum_{j \in \supp(\vc{s})} s_j \log \frac{p_{f(j)}}{s_j},
\end{align*}
% 
as claimed.
\end{proof}

\begin{remark}
This result is also an instance of
Lemma~\ref{lemma:cond-alts}\bref{part:cond-alts-exp} on conditional%
%
\index{conditional entropy!information loss@and information loss}%
\index{information loss!conditional entropy@and conditional entropy}
% 
entropy, as follows.  Let $V$ be a random variable taking values in $\Yy$,
with distribution $\vc{s}$.  Put $U = f(V)$, which is a random variable
taking values in $\Xx$, with distribution $f\vc{s} = \vc{p}$.  Then $U$ is
determined by $V$, so by Example~\ref{egs:cond}\bref{eg:cond-det},
\[
0 
\leq 
\condent{V}{U} 
= 
H(V) - H(U) 
=
H(\vc{s}) - H(\p)
=
L(f).
\]
On the other hand, by Lemma~\ref{lemma:cond-alts}\bref{part:cond-alts-exp},
\[
\condent{V}{U}
=
\sum_{j, i \csuch \Pr(j, i) > 0} \Pr(j, i) \log \frac{\Pr(i)}{\Pr(j, i)}
=
\sum_{j \csuch s_j > 0} s_j \log \frac{p_{f(j)}}{s_j}.
\]
Comparing the two expressions for $\condent{V}{U}$ gives another proof of
Lemma~\ref{lemma:loss}.

This argument shows that information loss is a special case of conditional
entropy.  But conditional entropy is also a special case of information
loss.  Indeed, let $U$ and $V$ be random variables with the same sample
space, taking values in finite sets $\Xx$ and $\Yy$ respectively.  Equip
$\Xx \times \Yy$ with the distribution of $(U, V)$ and $\Xx$ with the
distribution of $U$.  Then the projection map
\[
\begin{array}{cccc}
\pr_1 \from     &\Xx \times \Yy &\to            &\Xx    \\
                &(i, j)         &\mapsto        &i
\end{array}
\]
is measure-preserving.  By definition, its information loss is 
\[
L(\pr_1) = H(U, V) - H(U) = \condent{V}{U}.
\]
Hence $\condent{V}{U} = L(\pr_1)$,
expressing conditional entropy in terms of information loss.
\end{remark}


\section{Characterization of information loss}
\lbl{sec:cil}
\index{information loss!characterization of}


In this section, we prove that information loss is uniquely characterized
(up to a constant factor) by four basic properties.  

First, a reversible process%
%
\index{deterministic process!reversible} 
% 
loses no information: $L(f) = 0$ for all isomorphisms $f$.  This follows
from the definition of $L$ and the isomorphism-invariance of $H$.

Second, the amount of information lost by two processes%
%
\index{deterministic process}
%  
in series is the sum of the amounts of information lost by each
individually.  Formally,
% 
\begin{equation}
\lbl{eq:series-sum}
L(g \of f) = L(g) + L(f)
\end{equation}
% 
whenever
% 
% \begin{equation}
% \lbl{eq:series}
\[
(\Yy, \vc{s}) \toby{f} (\Xx, \p) \toby{g} (W, \vc{t})
\]
% \end{equation}
% 
are measure-preserving maps of finite probability spaces.  This is
immediate from the definition of information loss.

Third, given $n$ measure-preserving maps
% 
\begin{eqnarray*}
(\Yy_1, \vc{s}^1) &\toby{f_1}& (\Xx_1, \p^1)   \\
\vdots& &\vdots \\
(\Yy_n, \vc{s}^n) &\toby{f_n}& (\Xx_n, \p^n)
\end{eqnarray*}
% 
and a distribution $\vc{w} \in \Delta_n$, the amount of information lost
by the convex combination $\coprod w_i f_i$ is given by
% 
\begin{equation}
\lbl{eq:parallel-sum}
L\Biggl( \coprod_{i = 1}^n w_i f_i \Biggr)
=
\sum_{i = 1}^n w_i L(f_i).
\end{equation}
% 
This follows from the chain rule~\eqref{eq:g-ent-chain}:
% 
\begin{align*}
L\Bigl( \coprod w_i f_i \Bigr)  &
=
H\Bigl( \coprod w_i \vc{s}^i \Bigr) 
- H\Bigl( \coprod w_i \vc{p}^i \Bigr)   \\
&
=
\biggl\{ H(\vc{w}) + \sum w_i H(\vc{s}^i) \biggr\}
-
\biggl\{ H(\vc{w}) + \sum w_i H(\vc{p}^i) \biggr\}        \\
&
=
\sum w_i L(f_i).
\end{align*}
% 
In particular, given measure-preserving maps
% 
\begin{eqnarray*}
(\Yy, \vc{s}) &\toby{f}& (\Xx, \p),   \\
(\Yy', \vc{s}') &\toby{f'}& (\Xx', \p')
\end{eqnarray*}
% 
and a constant $\lambda \in [0, 1]$,
\[
L\bigl( \lambda f \cpd (1 - \lambda) f' \bigr)
=
\lambda L(f) + (1 - \lambda) L(f').
\]
Intuitively, this means that if we flip a probability-$\lambda$ coin%
%
\index{coin!toss} 
% 
and, depending on the outcome, do either the process $f$ or the process
$f'$, then the expected information loss is $\lambda$ times the information
loss of $f$ plus $1 - \lambda$ times the information loss of $f'$.  So,
while the previous property of $L$ (equation~\eqref{eq:series-sum})
concerned the information lost by two processes in \emph{series}, this
property (equation~\eqref{eq:parallel-sum}) concerns the information lost
by two or more processes in \emph{parallel}.

Fourth and finally, information loss is continuous,%
%
\index{continuous!measure of information loss}%
\index{information loss!continuity of}
% 
in the following sense.  Let $f \from \Yy \to \Xx$ be a map of finite sets.
For each probability distribution $\vc{s}$ on $\Yy$, we have the
pushforward distribution $f \vc{s}$ on $\Xx$, and $f$ defines a
measure-preserving map
\[
f \from (\Yy, \vc{s}) \to (\Xx, f \vc{s})
\]
(Remark~\ref{rmks:finprob}\bref{rmk:finprob-pf}).  The statement is
that the map 
\[
\begin{array}{ccc}
\Delta_\Yy      &\to            &\R     \\
\vc{s}          &\mapsto        &
L \Bigl( (\Yy, \vc{s}) \toby{f} (\Xx, f\vc{s}) \Bigr)
\end{array}
\]
is continuous.  This follows from the fact that all the maps in the
(noncommutative) triangle
\[
\xymatrix{
\Delta_\Yy \ar[rr]^{\vc{s} \mapsto f\vc{s}} \ar[rd]_H        &
&
\Delta_\Xx \ar[ld]^H                      \\
&
\R
} 
\]
are continuous.

An equivalent way to state continuity is as follows.  Let us say that an
infinite sequence
\[
\Bigl(
(\Yy_m, \vc{s}^m) \toby{f_m} (\Xx_m, \vc{p}^m)
\Bigr)_{m \geq 1}
\]
of measure-preserving maps of finite probability spaces
\demph{converges}\index{convergence} to a map
\[
(\Yy, \vc{s}) \toby{f} (\Xx, \p)
\]
if 
\[
\Bigl( \Yy_m \toby{f_m} \Xx_m \Bigr) = \Bigl( \Yy \toby{f} \Xx \Bigr)
\]
for all sufficiently large $m$, and $\vc{s}^m \to \vc{s}$ and $\p^m \to \p$
as $m \to \infty$.  Then continuity of $L$ is equivalent to the statement
that for any such convergent sequence,
\[
L
\Bigl(
(\Yy_m, \vc{s}^m) \toby{f_m} (\Xx_m, \vc{p}^m)
\Bigr)
\to
L
\Bigl(
(\Yy, \vc{s}) \toby{f} (\Xx, \vc{p})
\Bigr)
% L(f_m) \to L(f) 
\text{ as } m \to \infty.
\]
The equivalence between these two formulations of continuity follows from
the elementary fact that a map of metrizable spaces is continuous if and
only if it preserves convergence of sequences.

We now state the main theorem, which first appeared as Theorem~2 of
Baez, Fritz and Leinster~\cite{CETIL}.

\begin{thm}[Baez, Fritz and Leinster]
\lbl{thm:cetil}%
\index{Baez, John}%
\index{Fritz, Tobias}%
\index{information loss!characterization of}%
%
Let $K$ be a function assigning a real number $K(f)$ to each
measure-preserving map $f$ of finite probability spaces.  The following
are equivalent:
% 
\begin{enumerate}
\item 
\lbl{part:cetil-condns}
$K$ has these four properties:
% 
\begin{enumerate}
\item 
$K(f) = 0$ for all isomorphisms $f$;

\item
$K(g \of f) = K(g) + K(f)$ for all composable pairs $(f, g)$ of
  measure-preserving maps;

\item
$K\bigl(\lambda f \cpd (1 - \lambda) f'\bigr) = \lambda K(f) + (1 -
  \lambda) K(f')$ for all measure-preserving maps $f$ and $f'$ and all
  $\lambda \in [0, 1]$;

\item
$K$ is continuous;
\end{enumerate}

\item
\lbl{part:cetil-form}
$K = cL$ for some $c \in \R$.
\end{enumerate}
\end{thm}

The proof, given below, will use a version of Faddeev's theorem:

\begin{thm}[Faddeev, version~2]
\lbl{thm:g-faddeev}%
\index{Faddeev, Dmitry!entropy theorem}%
%
Let $I$ be a function assigning a real number $I(\p)$ to each finite
probability space $(\Xx, \p)$.  The following are equivalent:
% 
\begin{enumerate}
\item
\lbl{part:g-faddeev-condns}
$I$ is isomorphism-invariant, satisfies the chain
rule~\eqref{eq:g-ent-chain}, and is continuous in the sense
of~\eqref{eq:g-cts} (with $I$ in place of $H$); 

\item
\lbl{part:g-faddeev-form}
$I = cH$ for some $c \in \R$.
\end{enumerate}
\end{thm}

\begin{proof}
% This is essentially a restatement of Theorem~\ref{thm:faddeev} (or in fact
% a weaker version of it, as discussed in Remark~\ref{rmk:g-faddeev-sym}
% below).
% 
We have already observed that $H$ satisfies the conditions
in~\bref{part:g-faddeev-condns}, and it follows
that~\bref{part:g-faddeev-form} implies~\bref{part:g-faddeev-condns}.

Conversely, take a function $I$ satisfying~\bref{part:g-faddeev-condns}.
Restricting $I$ to finite sets of the form $\{1, \ldots, n\}$ defines, for
each $n \geq 1$, a continuous function $I \from \Delta_n \to \R$ satisfying
the chain rule.  Hence by Faddeev's Theorem~\ref{thm:faddeev}, there is
some constant $c \in \R$ such that $I(\p) = cH(\p)$ for all $n \geq 1$ and
$\p \in \Delta_n$.  Next, take any finite probability space $(\Yy,
\vc{s})$.  We have
\[
(\Yy, \vc{s}) 
\iso
\bigl( \{1, \ldots, n\}, \p \bigr)
\]
for some $n \geq 1$ and $\p \in \Delta_n$, and then by
isomorphism-invariance of both $I$ and $H$,
\[
I(\vc{s})
=
I(\vc{p})
=
cH(\vc{p})
=
cH(\vc{s}),
\]
as required.
\end{proof}

\begin{remark}
\lbl{rmk:g-faddeev-sym}
The version of Faddeev's theorem just stated is slightly weaker than the
earlier version, Theorem~\ref{thm:faddeev}.  To see this, take $\p \in
\Delta_n$ and a permutation $\sigma$ of $\{1, \ldots, n\}$.  Then $\sigma$
defines a measure-preserving bijection 
\[
\sigma \from
\bigl( \{1, \ldots, n\}, \p\sigma \bigr) 
\to
\bigl( \{1, \ldots, n\}, \p \bigr).
\]
In Theorem~\ref{thm:g-faddeev}, therefore, the isomorphism-invariance axiom
on $I$ includes as a special case that $I(\p\sigma) = I(\p)$ for all $\p
\in \Delta_n$ and permutations $\sigma$.  This is the symmetry%
%
\index{symmetry in Faddeev-type theorems}%
% \index{Faddeev, Dmitry!entropy theorem!role of symmetry in}
% 
axiom that is traditionally included in statements of Faddeev's theorem,
but is not in fact necessary, as observed in
Remark~\ref{rmks:faddeev}\bref{rmk:faddeev-sym}.  So,
Theorem~\ref{thm:g-faddeev} is a restatement of that traditional, weaker
form of Faddeev's theorem.  The analogous restatement of the stronger
Theorem~\ref{thm:faddeev} would involve \emph{ordered}%
%
\index{order!probability space@on probability space}%
\index{probability space!ordered}
%
probability spaces.
\end{remark}

We can now prove the characterization theorem for information loss.

\begin{pfof}{Theorem~\ref{thm:cetil}}
We have already shown that information loss $L$ satisfies the four
conditions of~\bref{part:cetil-condns}, and it follows
that~\bref{part:cetil-form} implies~\bref{part:cetil-condns}.

For the converse, suppose that $K$ satisfies~\bref{part:cetil-condns}.
Given a finite probability space $(\Xx, \p)$, write $!_{\p}$ for the unique
measure-preserving map
\[
!_{\p} \from (\Xx, \p) \to (\{1\}, \vc{u}_1),
\]
and define $I(\p) = K(!_{\p})$.  For any measure-preserving map $f \from
(\Yy, \vc{s}) \to (\Xx, \p)$, the triangle
\[
\xymatrix{
(\Yy, \vc{s}) \ar[rr]^f \ar[rd]_{!_{\vc{s}}}      &
&
(\Xx, \p) \ar[ld]^{!_{\p}}        \\
&
(\{1\}, \vc{u}_1)
}
\]
commutes, so by the composition condition on $K$,
\[
K(!_{\vc{s}}) = K(!_{\p}) + K(f).
\]
Equivalently,
% 
\begin{equation}
\lbl{eq:K-I-diff}
K(f) = I(\vc{s}) - I(\vc{p}).
\end{equation}
% 
So in order to prove the theorem, it suffices to show that $I = cH$ for
some constant $c$; and for this, it is enough to prove that $I$ satisfies
the hypotheses of Theorem~\ref{thm:g-faddeev}.

First, $I$ is isomorphism-invariant, since if $f\from (\Yy, \vc{s}) \to (\Xx,
\p)$ is an isomorphism then $K(f) = 0$, so $I(\vc{s}) = I(\p)$
by~\eqref{eq:K-I-diff}.  

Second, $I$ satisfies the chain rule~\eqref{eq:g-ent-chain}; that is,
% 
\begin{equation}
\lbl{eq:cetil-ch}
I\Biggl( \coprod_{i = 1}^n w_i \p^i \Biggr)
=
I(\vc{w}) + \sum_{i = 1}^n w_i I(\p^i)
\end{equation}
% 
for all $\vc{w} \in \Delta_n$ and finite probability spaces $(\Xx_1, \p^1),
\ldots, (\Xx_n, \p^n)$.  To see this, write
\[
f \from
\coprod_{i = 1}^n \Xx_i \to \{1, \ldots, n\}
\]
for the function defined by $f(j) = i$ whenever $j \in \Xx_i$.  Then $f$
defines a measure-preserving map
\[
f \from
\Bigl( \coprod \Xx_i, \coprod w_i \p^i \Bigr)
\to
\bigl( \{1, \ldots, n\}, \vc{w} \bigr).
\]
We now evaluate $K(f)$ in two ways.  On the one hand, by
equation~\eqref{eq:K-I-diff},
\[
K(f) =
I\Bigl( \coprod w_i \p^i \Bigr) - I(\vc{w}).
\]
On the other, 
\[
f = \coprod w_i \, !_{\p^i},
\]
so by hypothesis on $K$ and induction,
\[
K(f)
=
\sum w_i K(!_{\p^i})
=
\sum w_i I(\p^i).
\]
Comparing the two expressions for $K(f)$ gives the chain
rule~\eqref{eq:cetil-ch} for $I$.

Third and finally, for each finite set $\Xx$, the function $I \from
\Delta_\Xx \to \R$ is continuous, by continuity of $K$.

Theorem~\ref{thm:g-faddeev} can therefore be applied, giving $I = cH$ for some
$c \in \R$. It follows from equation~\eqref{eq:K-I-diff} that $K = cL$.
\end{pfof}

As observed in~\cite{CETIL} (p.~1947), the charm of Theorem~\ref{thm:cetil}
is that the axioms on the information loss function $K$ are entirely
linear.  They give no hint of any special role for the function
\[
p \mapsto -p \log p.
\]
And yet, this function emerges in the conclusion.

Another striking feature of Theorem~\ref{thm:cetil} is that the natural
conditions imposed on $K$ force $K(f)$ to depend only on the domain and
codomain of $f$.  This is a consequence of condition~\hardref{(b)} alone
(on the information lost by a composite process), as can be seen from the
argument leading up to equation~\eqref{eq:K-I-diff}.  It is an instance of
a general categorical fact: for any functor $K$ from a category $\cat{P}$
with a terminal object to a groupoid, $K(f) = K(f')$ whenever $f$
and $f'$ are maps in $\cat{P}$ with the same domain and the same codomain.

Theorem~\ref{thm:cetil} has several variants.  We can drop the condition
that $K(f) = 0$ for isomorphisms $f$ if we instead require that $K(f) \geq
0$ for all $f$.  (This was the version stated in Baez, Fritz and
Leinster~\cite{CETIL}.)  There is another version of
Theorem~\ref{thm:cetil} for finite sets equipped with arbitrary finite
measures instead of probability measures (Corollary~4 of~\cite{CETIL}).
And there is a further variant for the $q$-logarithmic entropies $S_q$,
which we give now.

For a measure-preserving map
\[
f \from (\Yy, \vc{s}) \to (\Xx, \p)
\]
between finite probability spaces, define the \demph{$q$-logarithmic%
%
\index{information loss!q-logarithmic@$q$-logarithmic}%
\index{q-logarithmic entropy@$q$-logarithmic entropy!information loss@and information loss}
%
information loss} of $f$ as
\[
L_q(f) = S_q(\vc{s}) - S_q(\p).
\ntn{lossq}
\]
The following characterization of $L_q$ 
is identical to Theorem~\ref{thm:cetil} except for a change in the rule for
the information lost by two processes in parallel (condition~\hardref{(c)}
below) and the absence of a continuity condition.  With some minor
differences, it first appeared as Theorem~7 of Baez, Fritz and
Leinster~\cite{CETIL}.

\begin{thm}[Baez, Fritz and Leinster]
\lbl{thm:cetil-q}%
\index{information loss!characterization of}%
\index{Baez, John}%
\index{Fritz, Tobias}%
%
Let $1 \neq q \in \R$.  Let $K$ be a function assigning a real number
$K(f)$ to each measure-preserving map $f$ of finite probability spaces.
The following are equivalent:
% 
\begin{enumerate}
\item 
\lbl{part:cetil-q-condns}
$K$ has these three properties:
% 
\begin{enumerate}
\item 
$K(f) = 0$ for all isomorphisms $f$;

\item
$K(g \of f) = K(g) + K(f)$ for all composable pairs $(f, g)$ of
  measure-preserving maps;

\item
$K\bigl(\lambda f \cpd (1 - \lambda) f'\bigr) = \lambda^q K(f) + (1 -
  \lambda)^q K(f')$ for all measure-preserving maps $f$ and $f'$ and all
  $\lambda \in (0, 1)$;
\end{enumerate}

\item
\lbl{part:cetil-q-form}
$K = cL_q$ for some $c \in \R$.
\end{enumerate}
\end{thm}

No continuity or other regularity condition is needed, in contrast to
Theorem~\ref{thm:cetil}. 

\begin{proof}
As for the proof of Theorem~\ref{thm:cetil}, but using the characterization
theorem for $S_q$ (Theorem~\ref{thm:q-ent-char}) instead of Faddeev's
characterization of $H$ (Theorem~\ref{thm:faddeev}).
\end{proof}


