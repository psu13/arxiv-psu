\chapter{Fundamental functional equations}
\lbl{ch:ffe}


Throughout this book, we will make contact with the venerable
subject of functional equations.  A functional%
%
\index{functional equation}
%
equation is an equation in an unknown function satisfied at all values of
its arguments; or more generally, it is an equation relating several
functions to each other in this way.

To set the scene, we give some brief indicative examples.  Viewing
sequences as functions on the set of positive integers, the
Fibonacci%
%
\index{Fibonacci sequence} 
% 
sequence $(F_n)_{n \geq 1}$ satisfies the functional equation
\[
F_{n + 2} = F_n + F_{n + 1} 
\]
($n \geq 1$).  Together with the boundary conditions $F_1 = F_2 = 1$, this
functional equation uniquely characterizes the sequence.  But more
typically, one is concerned with functions of \emph{continuous} variables.
For instance, one might notice that the function
\[
\begin{array}{cccc}
f\from  &\R \cup \{\infty\}     &\to            &\R\cup\{\infty\}\\[1ex]
        &x                      &\mapsto        &
\displaystyle\frac{1}{1 - x}
\end{array}
\]
satisfies the functional equation
% 
\begin{equation}
\lbl{eq:fe-triple}
f(f(f(x))) = x
\end{equation}
% 
($x \in \R \cup \{\infty\}$).  The natural question, then, is whether $f$
is the \emph{only} function satisfying equation~\eqref{eq:fe-triple} for
all $x$.  In this case, it is not.  (This can be shown by constructing an
explicit counterexample or via the theory of M\"obius
transformations.)  So, it is then natural to seek the whole set of
solutions $f$, perhaps restricting the search to just those functions that
are continuous, differentiable, etc.

A more sophisticated example is the functional equation
% 
% \begin{equation}
% \lbl{eq:fe-zeta}
\[
\zeta(1 - s) 
=
\frac{2^{1 - s}}{\pi^s} 
\cos\biggl(\frac{\pi s}{2}\biggr)\,\Gamma(s)\,\zeta(s)
\]
% \end{equation}
% 
($s \in \C$) satisfied by the Riemann%
%
\index{Riemann, Bernhard!zeta function} 
% 
zeta function $\zeta$ (Theorem~12.7 of Apostol~\cite{AposIAN}, for
instance).  Here $\Gamma$ is Euler's gamma function.  This functional
equation, proved by Riemann himself, is a fundamental property of the zeta
function.

In this chapter, we solve three classical, fundamental, functional
equations.  The first is Cauchy's equation on a function $f \from \R \to
\R$:
\[
f(x + y) = f(x) + f(y)
\]
($x, y \in \R$) (Section~\ref{sec:cauchy}).  Once we have solved this, we
will easily be able to deduce the solutions of related equations such as
% 
\begin{equation}
\lbl{eq:fe-cauchy-log-intro}
f(xy) = f(x) + f(y)
\end{equation}
% 
($x, y \in (0, \infty)$).  

The second is the functional equation
\[
f(mn) = f(m) + f(n)
\]
($m, n \geq 1$) on a \emph{sequence} $(f(n))_{n \geq 1}$.  Despite the
resemblance to equation~\eqref{eq:fe-cauchy-log-intro}, the shift from
continuous to discrete makes it necessary to develop quite different
techniques (Section~\ref{sec:log-seqs}).

Third and finally, we solve the functional equation
\[
f(xy) = f(x) + g(x)f(y)
\]
in two unknown functions $f, g\from (0, \infty) \to \R$.  The nontrivial,
measurable solutions $f$ turn out to be the constant multiples of the
so-called $q$-logarithms (Section~\ref{sec:q-log}), a one-parameter family
of functions of which the ordinary logarithm is just the best-known member.


\section{Cauchy's equation}
\lbl{sec:cauchy}


A function $f \from \R \to \R$ is \demph{additive}%
%
\index{additive function} 
% 
if
% 
\begin{equation}
\lbl{eq:additivity}
f(x + y) = f(x) + f(y)
\end{equation}
% 
for all $x, y \in \R$.  This is 
\demph{Cauchy's%
%
\index{Cauchy, Augustin!functional equation} 
% 
functional equation}, some of whose long history is recounted in
Section~2.1 of Acz\'el~\cite{AczeLFE}.  Let us say that $f$ is
\demph{linear}%
%
\index{linear function} 
% 
if there exists $c \in \R$ such that
\[
f(x) = cx
\]
for all $x \in \R$.  Putting $x = 1$ shows that if such a constant $c$
exists then it must be equal to $f(1)$.

Evidently any linear function is additive.  The question is to what extent
the converse holds.  If we are willing to assume that $f$ is differentiable
then the converse is very easy:

\begin{propn}
\lbl{propn:add-diff}
Every differentiable additive function $\R \to \R$ is linear.
\end{propn}

\begin{proof}
Let $f \from \R \to \R$ be a differentiable additive function.
Differentiating equation~\eqref{eq:additivity} with respect to $y$ gives
\[
f'(x + y) = f'(y)
\]
for all $x, y \in \R$.  Taking $y = 0$ then shows that $f'$ is constant.
Hence there are constants $c, d \in \R$ such that $f(x) = cx + d$ for all
$x \in \R$.  Substituting this expression back into
equation~\eqref{eq:additivity} gives $d = 0$.
\end{proof}

However, differentiability is a stronger condition than we will want to
assume for our later purposes.  It is, in fact, unnecessarily strong.  In
the rest of this section, we prove that additivity implies linearity under
a succession of ever-weaker regularity conditions, starting with continuity
and finishing with mere measurability.

We begin with a lemma that needs no regularity conditions at all.

\begin{lemma}
\lbl{lemma:add-rat}
Let $f \from \R \to \R$ be an additive function.  Then $f(qx) = qf(x)$ for
all $q \in \Q$ and $x \in \R$.  
\end{lemma}

\begin{proof}
First, $f(0 + 0) = f(0) + f(0)$, so $f(0) = 0$.  Then, for all $x \in \R$,
\[
0 = f(0) = f(-x + x) = f(-x) + f(x),
\]
so $f(-x) = -f(x)$.

Let $x \in \R$.  By induction,
% 
\begin{equation}
\lbl{eq:int-scalar}
f(nx) = nf(x)
\end{equation}
% 
for all integers $n > 0$, and we have just shown that
equation~\eqref{eq:int-scalar} also holds when $n = 0$.  Moreover, when $n
< 0$,
\[
f(nx) 
=
f\bigl(-(-n)x\bigr)
=
-f\bigl((-n)x\bigr)
=
-(-n)f(x)
=
nf(x),
\]
using equation~\eqref{eq:int-scalar} for positive integers.
Hence~\eqref{eq:int-scalar} holds for all integers $n$.

Now let $x \in \R$ and $q \in \Q$.  Write $q = m/n$, where $m, n \in \Z$
with $n \neq 0$.  Then by two applications of
equation~\eqref{eq:int-scalar}, 
\[
f(qx)
=
\tfrac{1}{n} f(nqx)
=
\tfrac{1}{n} f(mx)
=
\tfrac{m}{n} f(x)
=
qf(x),
\]
as required.
\end{proof}

\begin{remark}
The same argument proves that any additive function between vector spaces
over $\Q$ is linear over $\Q$.  In the case of functions $\R \to \R$, our
question is whether (or under what conditions) $\Q$-linearity implies
$\R$-linearity, which here we are just calling `linearity'.
\end{remark}

Lemma~\ref{lemma:add-rat} enables us to improve
Proposition~\ref{propn:add-diff}, relaxing differentiability to continuity.
The following result was known to Cauchy himself (cited in Hardy,
Littlewood and P\'olya~\cite{HLP}, proof of Theorem~84).

\begin{propn}
\lbl{propn:add-cts}
Every continuous additive function $\R \to \R$ is linear.
\end{propn}

\begin{proof}
Let $f \from \R \to \R$ be a continuous additive function, and write $c =
f(1)$.  By Lemma~\ref{lemma:add-rat}, $f(q) = cq$ for all $q \in \Q$. Thus,
the two functions $f$ and $x \mapsto cx$ are equal when restricted to $\Q$.
But both are continuous, so they are equal on all of $\R$.
\end{proof}

It is now straightforward to relax continuity of $f$ to an apparently much
weaker condition:

\begin{propn}
\lbl{propn:add-cts-pt}
Every additive function $\R \to \R$ that is continuous at one or more point
is linear.
\end{propn}

In other words, every additive function is linear unless, perhaps, it is
discontinuous everywhere.

\begin{proof}
Let $f \from \R \to \R$ be an additive function continuous at a point $x
\in \R$.  By Proposition~\ref{propn:add-cts}, it is enough to show that $f$
is continuous.  Let $y, t \in \R$: then by additivity,
\[
f(y + t) - f(y)   
=
f(t)
=
f(x + t) - f(x)   
\to 0
\]
as $t \to 0$, as required.  
\end{proof}

Next we show that mere measurability suffices: every measurable additive
function is linear.

\begin{remark}
Readers unfamiliar with measure theory may wish to read the rest of this
remark then resume at Corollary~\ref{cor:add-transf}.
Measurability\index{measurability} is an extremely weak condition.  In the
usual logical framework for mathematics, there do exist nonmeasurable
functions and nonlinear additive functions (Remark~\ref{rmk:choice}).
However, every function that anyone has ever written down an explicit
formula for, or ever will, is measurable (by Remark~\ref{rmk:zf}).  So it
is not too dangerous to assume that every function is measurable and,
therefore, that every additive function is linear.
\end{remark}

There are several proofs that every measurable additive function is linear.
The first was published by Maurice%
%
\index{Frechet, Maurice@Fr\'echet, Maurice} 
%
Fr\'echet in his 1913 paper
`Pri la funkcia ekvacio $f(x + y) = f(x) + f(y)$' \cite{Frec}.  (Fr\'echet
wrote many papers in Esperanto,%
%
\index{Esperanto}
%
and served three years as the president of the Internacia Scienca Asocio
Esperantista.)  Here we give the proof by Banach~\cite{BanaSEF}.  It is
based on a standard measure-theoretic result of Lusin~\cite{Lusi}, which
makes precise Littlewood's% 
%
\index{Littlewood, John Edensor} 
%
maxim that every measurable function is `nearly continuous'~\cite{Litt}.

Write $\lambda$\ntn{lambdaLeb} for Lebesgue measure on $\R$.

\begin{thm}[Lusin]
\lbl{thm:lusin}
\index{Lusin's theorem}
Let $a \leq b$ be real numbers, and let $f \from [a, b] \to \R$ be a
measurable function.  Then for all $\epsln > 0$, there exists a closed
subset $V \sub [a, b]$ such that $f|_V$ is continuous and $\lambda\bigl(
[a, b] \without V \bigr) < \epsln$.
\end{thm}

\begin{proof}
See Theorem~7.5.2 of Dudley~\cite{Dudl}, for instance.
\end{proof}

Following Banach, we deduce:

\begin{thm}
\lbl{thm:add-meas}
Every measurable additive function $\R \to \R$ is linear.
\end{thm}

\begin{proof}
Let $f \from \R \to \R$ be a measurable additive function.  
By Lusin's theorem, we can choose a closed set $V \sub [0, 1]$ such that
$f|_V$ is continuous and $\lambda(V) > 2/3$.  Since $V$ is compact, $f|_V$
is uniformly continuous.

By Proposition~\ref{propn:add-cts-pt}, it is enough to prove that $f$ is
continuous at $0$.  Let $\epsln > 0$.  We have to show that $|f(x)| <
\epsln$ for all $x$ in some neighbourhood of $0$.

By uniform continuity, we can choose $\delta > 0$ such that for $v, v' \in
V$,
\[
|v - v'| < \delta \implies |f(v) - f(v')| < \epsln.
\]
I claim that $|f(x)| < \epsln$ for all $x \in \R$ such that $|x| <
\min\{\delta, 1/3\}$.  Indeed, take such an $x$.  Then, writing $V - x = \{
v - x \such v \in V\}$, the inclusion-exclusion property of Lebesgue
measure $\lambda$ gives
\[
\lambda\bigl(V \cap (V - x)\bigr)
=
\lambda(V) + \lambda(V - x) - \lambda\bigl(V \cup (V - x)\bigr).
\]
Consider the right-hand side.  For the first two terms, we have $\lambda(V)
> 2/3$ and so $\lambda(V - x) > 2/3$.  For the last, if $x \geq 0$ then $V
\cup (V - x) \sub [-1/3, 1]$, if $x \leq 0$ then $V \cup (V - x) \sub [0,
  4/3]$, and in either case, $\lambda(V \cup (V - x)) \leq 4/3$.  Hence
\[
\lambda\bigl(V \cap (V - x)\bigr) > 2/3 + 2/3 - 4/3 = 0.
\]
In particular, $V \cap (V - x)$ is nonempty, so we can choose an element
$y$.  Then $y, x + y \in V$ with $|y - (x + y)| = |x| < \delta$, so $|f(y)
- f(x + y)| < \epsln$ by definition of $\delta$.  But since $f$ is
additive, this means that $|f(x)| < \epsln$, as required.
\end{proof}

The regularity condition can be weakened still further; see
Reem~\cite{Reem} for a recent survey.  However, measurability is as weak a
condition as we will need.

\begin{remark}
\lbl{rmk:choice}
Assuming the axiom of choice, there do exist additive functions $\R \to \R$
that are not linear.  To see this, first note that the real line $\R$ is a
vector space over $\Q$ in the evident way.  Choose a basis $B$ for $\R$
over $\Q$.  Choose an element $b$ of $B$, and let $\phi \from B \to \R$ be
the function taking value $1$ at $b$ and $0$ elsewhere.  By the universal
property of bases, $\phi$ extends uniquely to a $\Q$-linear map
$f \from \R \to \R$.

Certainly $f$ is additive.  On the other hand, we can show that $f$ is not
$\R$-linear (that is, not `linear' in the terminology of this section).
Indeed, any $\R$-linear function $\R \to \R$ either is identically zero or
vanishes nowhere except at $0$.  Now $f$ is not identically zero,
since $f(b) = \phi(b) = 1$.  But also, for any $b' \neq b$ in $B$,
we have $f(b') = \phi(b') = 0$ with $b' \neq 0$, so $f$ vanishes at some
point other than $0$.  Hence $f$ is a nonlinear, additive function $\R \to
\R$.
\end{remark}

\begin{remark}
\lbl{rmk:zf} 
It is consistent with the Zermelo--Fraenkel%
%
\index{Zermelo--Fraenkel axioms}%
\index{set theory} 
% 
axioms of set theory (that is, ZFC without the axiom of choice) that all
functions $\R \to \R$ are measurable.  This is a 1970 theorem of
Solovay~\cite{SoloMST}.%
%
\index{Solovay, Robert}
% 
If all functions $\R \to \R$ are measurable then
by Theorem~\ref{thm:add-meas}, all additive functions are linear.

On the other hand, the axiom of choice is also consistent with ZF.  If the
axiom of choice%
%
\index{axiom of choice} 
% 
holds then by Remark~\ref{rmk:choice}, not all additive functions are
linear.

Hence, starting from ZF, one may consistently assume \emph{either} that
every additive function is linear \emph{or} that not every additive
function is linear.
\end{remark}

Theorem~\ref{thm:add-meas} classifies the measurable functions that convert
addition into addition.  One can easily adapt it to classify the
functions that convert addition into multiplication, multiplication into
multiplication, and so on:

\begin{cor}
\lbl{cor:add-transf}
\begin{enumerate}
\item 
\lbl{part:add-transf-exp}
Let $f \from \R \to (0, \infty)$ be a measurable function.  The following
are equivalent:
% 
\begin{enumerate}
\item 
$f(x + y) = f(x)f(y)$ for all $x, y \in \R$;

\item
there exists $c \in \R$ such that $f(x) = e^{cx}$ for all $x \in \R$.
\end{enumerate}

\item
\lbl{part:add-transf-log}
Let $f \from (0, \infty) \to \R$ be a measurable function.  The following
are equivalent:
% 
\begin{enumerate}
\item 
$f(xy) = f(x) + f(y)$ for all $x, y \in (0, \infty)$;

\item
there exists $c \in \R$ such that $f(x) = c \log x$ for all $x \in (0,
\infty)$. 
\end{enumerate}

\item
\lbl{part:add-transf-power}
Let $f \from (0, \infty) \to (0, \infty)$ be a measurable function.  The
following are equivalent:
% 
\begin{enumerate}
\item 
$f(xy) = f(x)f(y)$ for all $x, y \in (0, \infty)$

\item
there exists $c \in \R$ such that $f(x) = x^c$ for all $x \in (0, \infty)$.
\end{enumerate}
\end{enumerate}
\end{cor}

\begin{proof}
For~\bref{part:add-transf-exp}, evidently~\hardref{(b)}
implies~\hardref{(a)}.  Assuming~\hardref{(a)}, define $g \from \R \to
\R$ by $g(x) = \log f(x)$.  Then $g$ is measurable and additive, so by
Theorem~\ref{thm:add-meas}, there is some constant $c \in \R$ such that
$g(x) = cx$ for all $x \in \R$.  It follows that $f(x) = e^{cx}$ for all $x
\in \R$, as required.

Parts~\bref{part:add-transf-log} and~\bref{part:add-transf-power} are proved
similarly, putting $g(x) = f(e^x)$ and $g(x) = \log f(e^x)$.
\end{proof}

\begin{remark}
\lbl{rmk:defn-log}
In this book, the notation $\log$ means the natural logarithm $\ln
= \log_e$.  However, the choice of base for logarithms is usually
unimportant, as it is in
Corollary~\ref{cor:add-transf}\bref{part:add-transf-log}: changing the base
amounts to multiplying the logarithm by a positive constant, which is
in any case absorbed by the free choice of the constant $c$.
\end{remark}

Theorem~\ref{thm:add-meas} also allows us to classify the additive
functions that are defined on only half of the real line.

\begin{cor}
\lbl{cor:cauchy-halfline}
Let $f \from [0, \infty) \to \R$ be a measurable function satisfying $f(x +
  y) = f(x) + f(y)$ for all $x, y \in [0, \infty)$.  Then there exists $c
    \in \R$ such that $f(x) = cx$ for all $x \in [0, \infty)$.    
\end{cor}

\begin{proof}
First we extend $f \from [0, \infty) \to \R$ to a measurable additive
  function $g \from \R \to \R$.  By the hypothesis on $f$, for all
  $a^+, a^-, b^+, b^- \in [0, \infty)$,
\[
a^+ - a^- = b^+ - b^-
\implies
f(a^+) - f(a^-) = f(b^+) - f(b^-).
\]
We can, therefore, consistently define a function $g \from \R \to \R$ by
\[
g(a^+ - a^-) = f(a^+) - f(a^-)
\]
($a^+, a^- \in [0, \infty)$).  To prove that $g$ is additive, let $x, y \in
  \R$, and choose $a^\pm, b^\pm \in [0, \infty)$ such that
\[
x = a^+ - a^-,
\qquad
y = b^+ - b^-.
\]
Then 
\[
x + y = (a^+ + b^+) - (a^- + b^-)
\]
with $a^+ + b^+, a^- + b^- \in [0, \infty)$.  Hence
% 
\begin{align*}
g(x + y)        &
=
f(a^+ + b^+) - f(a^- + b^-)     \\
&
=
f(a^+) + f(b^+) - f(a^-) - f(b^-)       \\
&
=
f(a^+) - f(a^-) + f(b^+) - f(b^-)       \\
&
=
g(x) + g(y),
\end{align*}
% 
as required.  To prove that $g$ is measurable, note that
\[
g(x) 
=
\begin{cases}
f(x)    &\text{if } x \geq 0,   \\
-f(-x)  &\text{if } x \leq 0
\end{cases}
\]
($x \in \R$), as if $x \geq 0$ then we can take $a^+ = x$ and $a^- = 0$
in the definition of $g$, and similarly for $x \leq 0$.  Since $f$ is
measurable, so is $g$.

By Theorem~\ref{thm:add-meas}, there exists a constant $c$ such that
$g(x) = cx$ for all $x \in \R$.  It follows that $f(x) = cx$ for all
$x \in [0, \infty)$.
\end{proof}

The techniques and results of this section can be assembled in several ways
to derive variant theorems.  Rather than attempting to catalogue all the
possibilities, we illustrate the point with two particular variants needed
later.

\begin{cor}
\lbl{cor:cauchy-log-01}
Let $f \from (0, 1] \to \R$ be a measurable function.  The following are
equivalent:
% 
\begin{enumerate}
\item 
\lbl{part:cauchy-log-01-condns}
$f(xy) = f(x) + f(y)$ for all $x, y \in (0, 1]$;

\item
\lbl{part:cauchy-log-01-form}
there exists a constant $c \in \R$ such that $f(x) = c\log x$ for all $x
\in (0, 1]$.  
\end{enumerate}
\end{cor}

\begin{proof}
Trivially, \bref{part:cauchy-log-01-form}
implies~\bref{part:cauchy-log-01-condns}.  Now
assuming~\bref{part:cauchy-log-01-condns}, define $g \from [0, \infty) \to
\R$ by $g(u) = f(e^{-u})$.  Then $g$ is measurable and $g(u + v) = g(u) +
g(v)$ for all $u, v \in [0, \infty)$, so by
Corollary~\ref{cor:cauchy-halfline}, $g(u) = bu$ for some real constant
$b$.  It follows that $f(x) = -b\log x$ for all $x \in (0, 1]$, as
required.
\end{proof}

The moral of Corollary~\ref{cor:cauchy-log-01} is that for the Cauchy-like
functional equation $f(xy) = f(x) + f(y)$, there is no substantial
difference between solving it on the domain $(0, \infty)$ and solving it on
the domain $(0, 1]$ (or $[1, \infty)$, similarly).  But matters become very
different when we seek solutions on the discrete domain $\{1, 2, 3,
\ldots\}$, as we will discover in the next section.

\begin{remark}
\lbl{rmk:defn-inc}
In this text, we always use the terms `increasing' and `decreasing' in
their non-strict senses.  Thus, a function $f \from S \to \R$ on a subset
$S \sub \R$ is \demph{increasing}%
%
\index{increasing!function or sequence} 
% 
if
\[
x \leq y \implies f(x) \leq f(y)
\]
($x, y \in S$), and \demph{decreasing}\index{decreasing} if $-f$ is
increasing.  It is \demph{strictly}%
%
\index{strictly increasing!function}%
\index{increasing!strictly}
% 
increasing or decreasing%
%
\index{strictly decreasing function} 
% 
if $x < y$ implies $f(x) < f(y)$
or $f(x) > f(y)$, respectively.  The same terminology applies to
sequences.%
%
% \index{increasing!sequence}
\end{remark}

\begin{cor}
\lbl{cor:cauchy-mult-01}
Let $f \from (0, 1) \to (0, \infty)$ be an increasing function.  The
following are equivalent:
% 
\begin{enumerate}
\item
\lbl{part:cauchy-mult-01-condns}
$f(xy) = f(x)f(y)$ for all $x, y \in (0, 1)$;

\item
\lbl{part:cauchy-mult-01-form}
there exists a constant $c \in [0, \infty)$ such that $f(x) = x^c$ for all
  $x \in (0, 1)$.
\end{enumerate}
\end{cor}

\begin{proof}
Trivially, \bref{part:cauchy-mult-01-form}
implies~\bref{part:cauchy-mult-01-condns}.
Assuming~\bref{part:cauchy-mult-01-condns}, define $g \from (0, \infty) \to
\R$ by $g(u) = -\log f(e^{-u})$.  Then $g(u + v) = g(u) + g(v)$ for all $u,
v \in (0, \infty)$, and $g$ is also increasing.

By the same argument as in the proof of Proposition~\ref{lemma:add-rat},
$g(qu) = qg(u)$ for all $q, u \in (0, \infty)$ with $q$ rational.  Define
$\twid{g} \from (0, \infty) \to \R$ by $\twid{g}(u) = g(1)u$.  Then $g(q) =
\twid{g}(q)$ for all $q \in (0, \infty) \cap \Q$.  Since $g$ is increasing
and $\twid{g}$ is either increasing or decreasing (depending on the sign of
$g(1)$), it follows that $\twid{g}$ is increasing.  But now $g, \twid{g}
\from (0, \infty) \to \R$ are increasing functions that are equal on the
positive rationals, so $g = \twid{g}$.  Hence $f(x) = x^{g(1)}$ for all $x
\in (0, 1)$.
\end{proof}


\section{Logarithmic sequences}
\lbl{sec:log-seqs}


A sequence $f(1), f(2), \ldots$ of real numbers is
\demph{logarithmic}%
%
\index{logarithmic sequence} 
% 
if
% 
\begin{equation}
\lbl{eq:log-seq}
f(mn) = f(m) + f(n)
\end{equation}
% 
for all $m, n \geq 1$.  Certainly the sequence $(c \log n)_{n \geq 1}$ is
logarithmic, for any real constant $c$.  But in contrast to the situation
for functions $f \from (0, \infty) \to \R$ satisfying $f(xy) = f(x) + f(y)$
(Corollary~\ref{cor:add-transf}\bref{part:add-transf-log}), it is easy to
write down logarithmic sequences that are not of this simple form.  Indeed,
we can choose $f(p)$ arbitrarily for each prime $p$, and these choices
uniquely determine a logarithmic sequence, generally not of the form $(c
\log n)$.

However, there are reasonable conditions on a logarithmic sequence
$(f(n))$ guaranteeing that it is of the form $(c \log n)$.  One such
condition is that $f$ is increasing:
\[
f(1) \leq f(2) \leq \cdots.
\]
An alternative condition is that
\[
\lim_{n \to \infty} \bigl(f(n + 1) - f(n)\bigr) = 0.
\]
We will prove a single theorem implying both of these results.  But a
direct proof of the result on increasing sequences is short enough to be
worth giving separately, even though it is not logically necessary.

\begin{thm}[Erd\H{o}s]
\lbl{thm:erdos-inc}%
\index{Erdos, Paul@Erd\H{o}s, Paul}
%
Let $(f(n))_{n \geq 1}$ be an increasing sequence of real numbers.  The
following are equivalent:
% 
\begin{enumerate}
\item 
\lbl{part:erdos-inc-condns}
$f$ is logarithmic;

\item
\lbl{part:erdos-inc-form}
there exists a constant $c \geq 0$ such that $f(n) = c \log n$ for all $n
\geq 1$. 
\end{enumerate}
\end{thm}

This was first proved by Erd\H{o}s~\cite{ErdoDFA}.  In fact, he
showed more: as is customary in number theory, he only required
equation~\eqref{eq:log-seq} to hold when $m$ and $n$ are relatively prime.
But since we will not need the extra precision of that result, we will not
prove it.  

The argument presented here follows Khinchin (\cite{Khin}, p.~11).

\begin{proof}
Certainly~\bref{part:erdos-inc-form} implies~\bref{part:erdos-inc-condns}.
Now assume~\bref{part:erdos-inc-condns}.  By the logarithmic property,
\[
f(1) = f(1 \cdot 1) = f(1) + f(1),
\]
so $f(1) = 0$.  Since $f$ is increasing, $f(n) \geq 0$ for all $n$.  If
$f(n) = 0$ for all $n$ then~\bref{part:erdos-inc-form} holds with $c = 0$.
Assuming otherwise, we can choose some $N > 1$ such that $f(N) > 0$.

Let $n \geq 1$.  For each integer $r \geq 1$, there is an integer
$\ell_r \geq 1$ such that
\[
N^{\ell_r} \leq n^r \leq N^{\ell_r + 1}
\]
(since $N > 1$).  As $f$ is increasing and logarithmic, 
\[
\ell_r f(N) \leq r f(n) \leq (\ell_r + 1)f(N),
\]
which since $f(N) > 0$ implies that 
% 
\begin{equation}
\lbl{eq:inc-f}
\frac{\ell_r}{r} \leq \frac{f(n)}{f(N)} \leq \frac{\ell_r + 1}{r}.
\end{equation}
% 
As $\log$ is also increasing and logarithmic, the same argument gives
% 
\begin{equation}
\lbl{eq:inc-log}
\frac{\ell_r}{r} \leq \frac{\log n}{\log N} \leq \frac{\ell_r + 1}{r}.
\end{equation}
% 
Inequalities~\eqref{eq:inc-f} and~\eqref{eq:inc-log} together imply that
\[
\left| \frac{f(n)}{f(N)} - \frac{\log n}{\log N} \right| 
\leq
\frac{1}{r}.
\]
But this conclusion holds for all $r \geq 1$, so 
\[
\frac{f(n)}{f(N)} = \frac{\log n}{\log N}.
\]
Hence $f(n) = c \log n$, where $c = f(N)/\log N$.  And since this is true
for all $n \geq 1$, we have proved~\bref{part:erdos-inc-form}.
\end{proof}

We now prove the unified theorem promised above.  Before stating it, let us
recall the concept of \demph{limit%
%
\index{limit inferior} 
% 
inferior}.  Given a real sequence $(g(n))_{n \geq 1}$, define
\[
h(n) = \inf\bigl\{ g(n), g(n + 1), \ldots \bigr\} \in [-\infty, \infty)
\]
($n \geq 1$).  The sequence $(h(n))_{n \geq 1}$ is increasing and therefore
has a limit (perhaps $\pm\infty$), written as
\[
\liminf_{n \to \infty} g(n) = \lim_{n \to \infty} h(n) 
\in [-\infty, \infty].
\ntn{liminf}
\]
If the ordinary limit $\lim_{n \to \infty} g(n)$ exists then $\liminf_{n
  \to \infty} g(n) = \lim_{n \to \infty} g(n)$.  However, the limit
inferior exists whether or not the limit does. For instance, the sequence
$1, -1, 1, -1, \ldots$ has a limit inferior of $-1$, but no limit.

If $(f(n))$ is a sequence that either is increasing or
satisfies $f(n + 1) - f(n) \to 0$ as $n \to \infty$, then
\[
\liminf_{n \to \infty} \bigl(f(n + 1) - f(n)\bigr) \geq 0.
\]
The following theorem therefore implies both of the results mentioned above.

\begin{thm}[Erd\H{o}s, K\'atai, M\'at\'e]
\lbl{thm:erdos-liminf}%
% 
\index{Erdos, Paul@Erd\H{o}s, Paul}%
\index{Katai, Imre@K\'atai, Imre}%
\index{Mate, Attila@M\'at\'e, Attila}
% 
Let $(f(n))_{n \geq 1}$ be a sequence of real numbers such that
\[
\liminf_{n \to \infty} \bigl( f(n + 1) - f(n) \bigr) \geq 0.
\]
The following are equivalent:
% 
\begin{enumerate}
\item 
\lbl{part:erdos-liminf-condns}
$f$ is logarithmic;

\item
\lbl{part:erdos-liminf-form}
there exists a constant $c$ such that $f(n) = c \log n$ for all $n \geq 1$.
\end{enumerate}
\end{thm}

This result was stated without proof by Erd\H{o}s in 1957~\cite{ErdoDAA},
then proved independently by K\'atai~\cite{Kata} and by
M\'at\'e~\cite{Mate}, both in 1967.  Again, the logarithmic condition can
be relaxed by only requiring that~\eqref{eq:log-seq} holds when $m$ and $n$
are relatively prime, but again, we have no need for this extra precision.

The proof below follows Acz\'el and Dar\'oczy's adaptation of
K\'atai's argument (Theorem~0.4.3 of~\cite{AcDa}).  The strategy is to put
$c = \liminf_{n \to \infty} f(n)/\log n$ and show that $f(N)/\log N = c$
for all $N$.

\begin{proof}
It is trivial that~\bref{part:erdos-liminf-form}
implies~\bref{part:erdos-liminf-condns}.  Now
assume~\bref{part:erdos-liminf-condns}.  I claim that for all $N \geq 2$,
% 
\begin{equation}
\lbl{eq:liminf-eq}
\liminf_{n \to \infty} \frac{f(n)}{\log n}
=
\frac{f(N)}{\log N}.
\end{equation}
% 
Let $N \geq 2$.  First we show that the left-hand side
of~\eqref{eq:liminf-eq} is less than or equal to the right.  For each $r
\geq 1$, the logarithmic property of $f$ implies that
\[
\frac{f(N^r)}{\log(N^r)}
=
\frac{rf(N)}{r\log N}
=
\frac{f(N)}{\log N}.
\]
Since $N^r \to \infty$ as $r \to \infty$, it follows from the definition of
limit inferior that
\[
\liminf_{n \to \infty} \frac{f(n)}{\log n}
\leq
\frac{f(N)}{\log N}.
\]
Now we prove the opposite inequality, 
% 
\begin{equation}
\lbl{eq:liminf-claim}
\liminf_{n \to \infty} \frac{f(n)}{\log n} 
\geq
\frac{f(N)}{\log N}.
\end{equation}
% 
Let $\epsln > 0$.  By hypothesis, we can choose $k \geq 1$ such that for
all $n \geq N^k$,
% 
\begin{equation}
\lbl{eq:liminf-ineq}
f(n + 1) - f(n) \geq -\epsln.
\end{equation}
% 
Any integer $n \geq N^k$ has a base $N$ expansion
\[
n = c_\ell N^\ell + \cdots + c_1 N + c_0
\]
with $c_0, \ldots, c_\ell \in \{0, \ldots, N - 1\}$, $c_\ell \neq 0$,
and $\ell \geq k$.  Then
% 
\begin{align}
f(n)    &
\geq
f(c_\ell N^\ell + \cdots + c_1 N) - c_0 \epsln  
\lbl{eq:liminf-1}     \\
&
\geq
f(c_\ell N^\ell + \cdots + c_1 N) - N \epsln  
\lbl{eq:liminf-2}     \\
&
=
f(c_\ell N^{\ell - 1} + \cdots + c_1) + f(N) - N \epsln,  
\lbl{eq:liminf-3}
\end{align}
% 
where inequality~\eqref{eq:liminf-1} follows from~\eqref{eq:liminf-ineq}
using induction and the fact that $\ell \geq k$,
inequality~\eqref{eq:liminf-2} holds because $c_0 \leq N$, and
equation~\eqref{eq:liminf-3} follows from the logarithmic property of $f$.
As long as $\ell - 1 \geq k$, we can apply the same argument again with
$c_\ell N^{\ell - 1} + \cdots + c_1$ in place of $n = c_\ell N^\ell +
\cdots + c_0$, giving
\[
f(c_\ell N^{\ell - 1} + \cdots + c_1)
\geq
f(c_\ell N^{\ell - 2} + \cdots + c_2) + f(N) - N\epsln
\]
and so
\[
f(n)
\geq
f(c_\ell N^{\ell - 2} + \cdots + c_2) + 2(f(N) - N\epsln).
\]
Repeated application of this argument gives
\[
f(n)    
\geq 
f(c_\ell N^{k - 1} + \cdots + c_{\ell - k + 1}) 
+ (\ell - k + 1)(f(N) - N\epsln).
\]
Hence, writing $A = \min\bigl\{f(1), f(2), \ldots, f(N^k)\bigr\}$, 
% 
\begin{equation}
\lbl{eq:liminf-4}
f(n) 
\geq
A + (\ell - k + 1)(f(N) - N\epsln).
\end{equation}
% 
In~\eqref{eq:liminf-4}, the only term on the right-hand side that depends
on $n$ is $\ell$, which is equal to $\lfloor \log_N n \rfloor$, and
$\lfloor \log_N n \rfloor/\log_N n \to 1$ as $n \to \infty$.  Hence
% 
\begin{align*}
\liminf_{n \to \infty} \frac{f(n)}{\log_N n}    &
\geq
\liminf_{n \to \infty} \Biggl\{
\frac{A}{\log_N n} + \Biggl( 
\frac{\lfloor \log_N n \rfloor}{\log_N n} + \frac{-k + 1}{\log_N n} 
\Biggr)\bigl(f(N) - N\epsln\bigr)
\Biggr\}        \\
&
=
f(N) - N\epsln.
\end{align*}
% 
This holds for all $\epsln > 0$, so 
\[
\liminf_{n \to \infty} \frac{f(n)}{\log_N n} 
\geq 
f(N).
\]
Since $\log_N n = (\log n)/(\log N)$, this proves the claimed
inequality~\eqref{eq:liminf-claim} and, therefore,
equation~\eqref{eq:liminf-eq}.

Putting $c = \liminf_{n \to \infty} f(n)/\log n \in \R$, we have $f(N) = c
\log N$ for all $N \geq 2$.  Finally, the logarithmic property of $f$
implies that $f(1) = 0$, so $f(1) = c \log 1$ too.
\end{proof}

\begin{cor}
\lbl{cor:erdos-lim}
Let $(f(n))_{n \geq 1}$ be a sequence such that
% 
\begin{equation}
\lbl{eq:erdos-lim}
\lim_{n \to \infty} \bigl( f(n + 1) - f(n) \bigr) = 0.
\end{equation}
% 
The following are equivalent:
% 
\begin{enumerate}
\item 
\lbl{part:erdos-lim-condns}
$f$ is logarithmic;

\item
\lbl{part:erdos-lim-form}
there exists a constant $c$ such that $f(n) = c \log n$ for all $n \geq 1$.
\end{enumerate}
\qed
\end{cor}

To apply this corollary, we will need to be able to verify the limit
condition~\eqref{eq:erdos-lim}.  The following improvement lemma will be
useful.

\begin{lemma}
\lbl{lemma:seq-improvement}
Let $(a_n)_{n \geq 1}$ be a real sequence such that $a_{n + 1} -
\tfrac{n}{n + 1} a_n \to 0$ as $n \to \infty$.  Then $a_{n + 1} - a_n \to
0$ as $n \to \infty$. 
\end{lemma}

Our proof of Lemma~\ref{lemma:seq-improvement} follows that of
Feinstein~\cite{Fein}%
%
\index{Feinstein, Amiel}
% 
(p.~6--7), and uses a standard result:

\begin{propn}[Ces\`aro]
\lbl{propn:cesaro}
\index{Ces\`aro, Ernesto!limit}
% 
Let $(x_n)_{n \geq 1}$ be a real sequence, and for $n \geq 1$, write
\[
\ovln{x}_n = \tfrac{1}{n} (x_1 + \cdots + x_n).
\]
Suppose that $\lim\limits_{n \to \infty} x_n$ exists.  Then $\lim\limits_{n
  \to \infty} \ovln{x}_n$ exists and is equal to $\lim\limits_{n \to
  \infty} x_n$.
\end{propn}

\begin{proof}
This can be found in introductory analysis texts such as
Apostol~\cite{AposMA} (Theorem~12-48).
\end{proof}

\begin{pfof}{Lemma~\ref{lemma:seq-improvement}}
It is enough to prove that $a_n/(n + 1) \to 0$ as $n \to \infty$.  Write
$b_1 = a_1$ and $b_n = a_n - \tfrac{n - 1}{n} a_{n - 1}$ for $n \geq 2$;
then by hypothesis, $b_n \to 0$ as $n \to \infty$.  We have $n a_n = nb_n +
(n - 1)a_{n - 1}$ for all $n \geq 2$, so
\[
na_n = nb_n + (n - 1)b_{n - 1} + \cdots + 1b_1
\]
for all $n \geq 1$.  Dividing through by $n(n + 1)$ gives
% 
\begin{align}
\frac{a_n}{n + 1}       &
=
\frac{1}{2} \cdot \frac{1}{\hlf n(n + 1)} 
(b_1 + b_2 + b_2 + b_3 + b_3 + b_3 + \cdots 
+ \underbrace{b_n + \cdots + b_n}_n)    
\nonumber       \\
&
=
\frac{1}{2} \cdot M_1(b_1, b_2, b_2, b_3, b_3, b_3, \ldots, 
\underbrace{b_n, \ldots, b_n}_n),
\lbl{eq:seq-imp-1}
\end{align}
% 
where $M_1$ denotes the arithmetic mean.  Since $b_n \to 0$ as $n \to
\infty$, the sequence
\[
b_1, b_2, b_2, b_3, b_3, b_3, \ldots, 
\underbrace{b_n, \ldots, b_n}_n, \ldots
\]
also converges to $0$.  Proposition~\ref{propn:cesaro} applied to this
sequence then implies that
\[
M_1(b_1, b_2, b_2, b_3, b_3, b_3, \ldots, \underbrace{b_n, \ldots, b_n}_n)
\to 0 
\text{ as } n \to \infty.
\]
But by equation~\eqref{eq:seq-imp-1}, this means that $a_n/(n + 1) \to 0$
as $n \to \infty$, completing the proof.
\end{pfof}

\begin{remark}
Lemma~\ref{lemma:seq-improvement} can also be deduced from the
Stolz--Ces\`aro%
%
\index{Stolz--Ces\`aro theorem}%
\index{Ces\`aro, Ernesto!Stolz theorem@--Stolz theorem}
% 
theorem (Section~3.1.7 of Mure\c{s}an~\cite{Mure}, for
instance).  This is a discrete analogue of l'H\^opital's rule, and states
that given a real sequence $(x_n)$ and a strictly increasing sequence
$(y_n)$ diverging to $\infty$, if
\[
\frac{x_{n + 1} - x_n}{y_{n + 1} - y_n} \to \ell
\]
% $(x_{n + 1} - x_n)/(y_{n + 1} - y_n) \to \ell$ 
as $n \to \infty$ then $x_n/y_n \to \ell$ as $n \to \infty$.
Lemma~\ref{lemma:seq-improvement} follows by taking $x_n = na_n$ and $y_n =
\hlf n(n + 1)$.  (I thank X\={\i}l\'{\i}ng Zh\={a}ng for this observation.)
\end{remark}


\section{The $q$-logarithm}
\lbl{sec:q-log}


The $q$-logarithms ($q \in \R$) form a continuous one-parameter family of
functions that include the ordinary natural logarithm as the case $q = 1$.
They can be regarded as deformations of the natural logarithm.  We will
show that as a family, they are characterized by a single functional
equation.

For $q \in \R$, the \demph{$q$-logarithm}\index{q-logarithm@$q$-logarithm}
is the function 
\[
\ln_q \from (0, \infty) \to \R
\ntn{lnq}
\]
defined by
\[
\ln_q(x) 
=
\int_1^x t^{-q} \dee t
\]
($x \in (0, \infty)$).  Thus, 
\[
\ln_1(x) = \log(x)
\]
and for $q \neq 1$,
% 
\begin{equation}
\lbl{eq:q-log-general}
\ln_q(x)
=
\frac{x^{1 - q} - 1}{1 - q}.
\end{equation}
% 
Then $\ln_q(x) \to \ln_1(x)$ as $q \to 1$, by 
l'H\^opital's rule.

Let $q \in \R$.  The $q$-logarithm shares with the natural logarithm the
property that 
\[
\ln_q(1) = 0.
\]
However, in general
\[
\ln_q(xy) \neq \ln_q(x) + \ln_q(y).
\]
One can see this without calculation: for by
Corollary~\ref{cor:add-transf}\bref{part:add-transf-log}, the only
measurable functions that transform multiplication into addition are the
multiples of the natural logarithm.  There is nevertheless a simple formula
for $\ln_q(xy)$ in terms of $\ln_q(x)$ and $\ln_q(y)$:
\[
\ln_q(xy)
=
\ln_q(x) + \ln_q(y) + (1 - q)\ln_q(x)\ln_q(y).
\]
Later, we will use a second formula for $\ln_q(xy)$:
% 
\begin{equation}
\lbl{eq:q-log-mult}
\ln_q(xy)
=
\ln_q(x) + x^{1 - q} \ln_q(y).
\end{equation}
% 
Similarly, in general
\[
\ln_q(1/x) \neq -\ln_q(x),
\]
but instead we have the following three formulas for $\ln_q(1/x)$:
% 
\begin{align}
\ln_q(1/x)      &
=
\frac{-\ln_q(x)}{1 + (1 - q)\ln_q(x)}
\nonumber       \\
&
=
-x^{q - 1} \ln_q(x)    
\nonumber       \\
&
=
-\ln_{2 - q}(x).
\lbl{eq:q-log-reciprocal}
\end{align}
% 
By~\eqref{eq:q-log-reciprocal}, replacing $\ln_q$ by the function $x
\mapsto -\ln_q(1/x)$ defines an involution $\ln_q \leftrightarrow \ln_{2 -
  q}$ of the family of $q$-logarithms, with a fixed point at the classical
logarithm $\ln_1$.  Finally, there is a quotient formula
% 
\begin{equation}
\lbl{eq:q-log-qt}
\ln_q(x/y) 
=
y^{q - 1} \bigl(\ln_q(x) - \ln_q(y)\bigr),
\end{equation}
% 
obtained from equation~\eqref{eq:q-log-mult} by substituting $y$ for $x$
and $x/y$ for $y$.

\begin{remark}
\index{q-logarithm@$q$-logarithm!history of}
% 
The history of the $q$-logarithms as an \emph{explicit} object of study
goes back at least as far as a 1964 paper of Box and Cox in statistics
(Section~3 of~\cite{BoCo}).  The name `$q$-logarithm' appears to have been
introduced by Umarov, Tsallis and Steinberg in 2008~\cite{UTS}, working in
statistical mechanics.  

But as Umarov et al.\ warned, there is more than one system of
$q$-analogues of the classical notions of calculus.  For instance, there
is the system developed by the early twentieth-century clergyman
F.~H. Jackson~\cite{JackQFC} (a modern account of which can be found in Kac
and Cheung~\cite{KaCh}).  In particular, this has given rise to a different
notion of $q$-logarithm, as developed in Chung, Chung, Nam and
Kang~\cite{CCNK}.  Ernst~\cite{Erns} gives a full historical treatment of
the various branches of $q$-calculus.  In any case, none of the
developments just mentioned use the $q$-logarithms considered here.
% 
% See
%   \href{https://mathoverflow.net/questions/279173}{https://mathoverflow.net/questions/279173}
%   for more refs on $q$-stuff
% 
\end{remark}

We now prove that the $q$-logarithms are characterized by a simple
functional equation.  The proof is essentially the argument behind
Theorem~84 in the classic text of Hardy,%
%
\index{Hardy, Godfrey Harold} 
%
Littlewood%
%
\index{Littlewood, John Edensor}
%
and P\'olya~\cite{HLP}%
%
\index{Polya, George@P\'olya, George}.

\begin{thm}
\lbl{thm:q-log}% 
\index{q-logarithm@$q$-logarithm!characterization of} 
% 
Let $f \from (0, \infty) \to \R$ be a measurable function.  The following
are equivalent:
% 
\begin{enumerate}
\item 
\lbl{part:q-log-condns}
there exists a function $g \from (0, \infty) \to \R$ such that for all $x,
y \in (0, \infty)$,
% 
\begin{equation}
\lbl{eq:q-log-fe}
f(xy) = f(x) + g(x) f(y);
\end{equation}

\item
\lbl{part:q-log-form}
$f = c\ln_q$ for some $c, q \in \R$, or $f$ is constant.
\end{enumerate}
\end{thm}

\begin{proof}
First suppose that~\bref{part:q-log-form} holds.  If $f = c\ln_q$ for some
$c, q \in \R$ then equation~\eqref{eq:q-log-fe} holds with $g(x) = x^{1 -
  q}$, by equation~\eqref{eq:q-log-mult}.  Otherwise, $f$ is constant,
so~\eqref{eq:q-log-fe} holds with $g \equiv 0$.

Now assume~\bref{part:q-log-condns}.  Since $f(xy) = f(yx)$,
equation~\eqref{eq:q-log-fe} implies that
\[
f(x) + g(x) f(y) = f(y) + g(y) f(x),
\]
or equivalently
% 
\begin{equation}
\lbl{eq:q-log-sym}
f(x) \bigl(1 - g(y)\bigr) = f(y) \bigl(1 - g(x)\bigr),
\end{equation}
% 
for all $x, y \in (0, \infty)$.  If $f \equiv 0$ then $f$ is constant
and~\bref{part:q-log-form} holds.  Assuming otherwise, we can choose $y_0
\in (0, \infty)$ such that $f(y_0) \neq 0$.  Taking $y = y_0$
in~\eqref{eq:q-log-sym} and putting $a = (1 - g(y_0))/f(y_0)$ gives
% 
\begin{equation}
\lbl{eq:q-log-gf}
g(x) = 1 - af(x)
\end{equation}
% 
($x \in \R$).  Since $f$ is measurable, so is $g$.  There are now two
cases: $a = 0$ and $a \neq 0$.

\textbf{Case 1: $a = 0$.}  Then $g \equiv 1$, so the original functional
equation~\eqref{eq:q-log-fe} states that $f(xy) = f(x) + f(y)$.  Since $f$
is measurable, Corollary~\ref{cor:add-transf}\bref{part:add-transf-log}
implies that $f = c \log = c \ln_1$ for some $c \in \R$.

\textbf{Case 2: $a \neq 0$.}  Then equation~\eqref{eq:q-log-gf}
can be rewritten as
% 
\begin{equation}
\lbl{eq:q-log-fg}
f(x) = \tfrac{1}{a} (1 - g(x))
\end{equation}
% 
($x \in (0, \infty)$).  Substituting this into the original functional
equation~\eqref{eq:q-log-fe} gives 
% 
\begin{equation}
\lbl{eq:q-log-g-mult}
g(xy) = g(x)g(y)
\end{equation}
% 
($x, y \in (0, \infty)$).  In particular, $g(x) = g(\sqrt{x})^2 \geq 0$ for
all $x \in (0, \infty)$.  There are now two subcases: $g$ either sometimes
vanishes or never vanishes.

If $g(x_0) = 0$ for some $x_0 \in (0, \infty)$ then
\[
g(x) = g(x_0)g(x/x_0) = 0
\]
for all $x \in (0, \infty)$, so $g \equiv 0$.  Hence by
equation~\eqref{eq:q-log-fg}, $f$ is constant.

Otherwise, $g(x) > 0$ for all $x \in (0, \infty)$.  Since $g$ is measurable
and satisfies the multiplicativity condition~\eqref{eq:q-log-g-mult},
Corollary~\ref{cor:add-transf}\bref{part:add-transf-power} implies that
there is some constant $t \in \R$ such that $g(x) = x^t$ for all $x \in (0,
\infty)$.  We have assumed that $f \not\equiv 0$, so $g \not\equiv 1$ (by
equation~\eqref{eq:q-log-fg}), so $t \neq 0$.  Hence
\[
f(x)
=
\tfrac{1}{a} (1 - x^t)
=
\tfrac{-t}{a}\ln_{1 - t}(x)
\]
for all $x \in (0, \infty)$, completing the proof.
\end{proof}


