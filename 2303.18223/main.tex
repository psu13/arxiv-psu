\documentclass[10pt,journal,compsoc,x11names]{IEEEtran}
\usepackage[switch]{lineno}
\usepackage{amsmath} 
\usepackage{xspace}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[hyphens]{url}

\usepackage{ragged2e}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{color}
\usepackage{colortbl}
\usepackage{tablefootnote}
\usepackage{pifont}
\usepackage{makecell}
\usepackage[most]{tcolorbox}
\usepackage{framed}
\usepackage{mdframed}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{longtable}
\usepackage{float}
\usepackage{booktabs}

\newcommand{\paratitle}[1]{\vspace{1.5ex}\noindent\textbf{#1}}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}

\newcommand{\ie}{\emph{i.e.,}\xspace}
\newcommand{\aka}{\emph{a.k.a.,}\xspace}
\newcommand{\eg}{\emph{e.g.,}\xspace}
\newcommand{\wrt}{\emph{w.r.t.}\xspace}
\newcommand{\wo}{\emph{w/o}\xspace}
\newcommand{\etc}{\emph{etc}}
\newcommand{\ignore}[1]{}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\definecolor{gold}{RGB}{205,133,63}
\definecolor{fGreen}{RGB}{34,139,34}
\definecolor{tOrange}{RGB}{255,215,0}
\definecolor{tBlue}{RGB}{135,206,250}
\definecolor{tPink}{RGB}{255,204,204}
\definecolor{tGreen}{RGB}{205,230,199}
\definecolor{tGold}{RGB}{255,215,0}

\usepackage{cite}
\usepackage[numbers,sort&compress]{natbib}


\ifCLASSINFOpdf
\else
\fi


\newcommand\MYhyperrefoptions{bookmarks=true,bookmarksnumbered=true,
pdfpagemode={UseOutlines},plainpages=false,pdfpagelabels=true,
colorlinks=true,linkcolor={black},citecolor={black},urlcolor={black},
pdftitle={Bare Demo of IEEEtran.cls for Computer Society Journals},%
pdfsubject={Typesetting},%
pdfauthor={Michael D. Shell},%
pdfkeywords={Computer Society, IEEEtran, journal, LaTeX, paper, template}}%



\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{A Survey of Large Language Models}

\author{Wayne Xin Zhao, Kun Zhou*, Junyi Li*, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie and Ji-Rong Wen
\IEEEcompsocitemizethanks{
\IEEEcompsocthanksitem Version: v13 (major update on November  23, 2023).  
\IEEEcompsocthanksitem GitHub link: \url{https://github.com/RUCAIBox/LLMSurvey} 
\IEEEcompsocthanksitem Chinese version link: \url{https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese.pdf} 
\IEEEcompsocthanksitem * K. Zhou and J. Li contribute equally to this work. 
\IEEEcompsocthanksitem 
The authors are mainly with Gaoling School of Artificial Intelligence and School of Information, Renmin University of China, Beijing, China;  Jian-Yun Nie is with DIRO, Universit\'{e} de Montr\'{e}al, Canada. \\
Contact e-mail: batmanfly@gmail.com
\IEEEcompsocthanksitem \textcolor{red}{The authors of this survey paper reserve all the copyrights of the figures/tables, and any use of these materials for publication purpose must be officially granted by the survey authors.} 
}%
}




\markboth{}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Advanced Demo of IEEEtran.cls for IEEE Computer Society Journals}









\IEEEtitleabstractindextext{%
\begin{abstract}
\justifying
Ever since the Turing Test was proposed in the 1950s,  humans have explored the mastering of  language intelligence by machine.  
Language is essentially a complex, intricate system of human expressions governed by grammatical rules.
 It poses  a significant challenge to develop capable artificial intelligence~(AI) algorithms  for comprehending and grasping a language. 
 As a major approach, 
 \emph{language modeling}
 has been widely studied  for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. 
Recently, pre-trained language models~(PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various natural language processing~(NLP) tasks. 
Since the researchers have found that model scaling can lead to an improved model capacity, they further investigate the scaling effect by increasing the parameter scale to an even larger size.  
Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement, but also exhibit some special abilities (\eg in-context learning) that are not present in small-scale  language models (\eg BERT). 
To discriminate the language models in different parameter scales,   
the research community has coined the term \emph{large language models~(LLM)} for the PLMs of significant  size  {(\eg containing tens or hundreds of billions of parameters)}. 
Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress 
is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted   widespread attention from society. 
The technical evolution of LLMs has been making an important impact on the entire AI community, which would  revolutionize the way how we develop and use AI algorithms. Considering this rapid technical progress, in this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Furthermore, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions. This survey provides an up-to-date review of the literature on LLMs, which can be a useful resource for both researchers and engineers. 
\end{abstract}

\begin{IEEEkeywords}
Large Language Models; Emergent Abilities; Adaptation Tuning;  
{Utilization; Alignment; Capacity Evaluation}
\end{IEEEkeywords}}


\maketitle


\IEEEdisplaynontitleabstractindextext


\IEEEpeerreviewmaketitle



\input{sections/intro}
\input{sections/overview}
\input{sections/pretraining}
\input{sections/adaptation}
\input{sections/utilization}
\input{sections/evaluation}
\input{sections/application}
\input{sections/conclusion}











\ifCLASSOPTIONcompsoc
  \section*{Acknowledgments}
\else
  \section*{Acknowledgment}
\fi

The authors would like to thank Yankai Lin and Yutao Zhu for proofreading this paper. 
Since the first release of this paper, we have received a number of valuable comments from the readers. 
We sincerely thank the readers who have written to us with constructive suggestions and comments: Tyler Suard, Damai Dai, Liang Ding,  Stella Biderman,  Kevin Gray,  Jay Alammar, Yubo Feng,  Mark Holmstrom, Xingdong Liu, Il-Seok Oh, Yiting Liu,  Shaojun Wang,  Gaoyan Ou,  Todd Morrill, Hao Liu,  Zhenyu Zhang, and Xinlin Zhuang.
\\

   
Since the v11 version (June 29, 2023),  we have been adding a large number of experiments and prompt practices. These new contents are completed by a number of volunteers in our team. Here, we add a special part to thank all the students who have worked very hard on this part (also including the ones on our author list). 

\paratitle{Contribution on Experiments.} 
We would like to sincerely thank the following people for their hard work involved in experiments shown in Table~\ref{tab-experimental-res}.

$\bullet$ Xiaoxue Cheng: implement the experiments for evaluation on Language Generation and HaluEval tasks.

$\bullet$ Yuhao Wang: implement the experiments for evaluation on interaction with environment tasks.

$\bullet$ Bowen Zheng: implement the experiments for evaluation on tool manipulation tasks.

\paratitle{Contribution on Tips.}
We list the following guys for their contributions on the corresponding numbers of provided tips for designing prompts in Table~\ref{tab-tips}.

$\bullet$ Xiaolei Wang: T3, O3

$\bullet$ Beichen Zhang: D2, D5

$\bullet$ Zhipeng Chen: D3, D4

$\bullet$ Junjie Zhang: D6

$\bullet$ Bowen Zheng: D7

$\bullet$ Zican Dong: D8

$\bullet$ Xinyu Tang: C2

$\bullet$ Yifan Du: T4

$\bullet$ Tianyi Tang: O6, O7, D9

$\bullet$ Yupeng Hou: O8, C3

$\bullet$ Salvatore Raieli: C4


\ifCLASSOPTIONcaptionsoff
  \newpage
\fi





\bibliographystyle{IEEEtran}
\bibliography{newbib}


\end{document}
