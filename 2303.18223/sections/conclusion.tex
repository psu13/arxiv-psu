\section{Conclusion and Future Directions}
\label{sec-con}
In this survey, we have reviewed the recent progress of large language models~(LLMs), and introduced the key concepts, findings, and techniques for understanding and utilizing LLMs. We focus on the large-sized models (\ie having a size larger than 10B) while excluding the contents of early pre-trained language models (\eg BERT and GPT-2) that  have been well covered in the existing literature.  
In particular, our survey has discussed four important aspects of LLMs, \ie pre-training, adaptation, utilization, and evaluation. For each aspect, we highlight the techniques or findings that are key to the success of LLMs.
Furthermore, we also summarize the available resources for developing LLMs and discuss important implementation guidelines for reproducing LLMs. 
This survey tries to cover the most recent literature about LLMs and provides a good reference resource on this topic for both researchers and engineers.
      
Next, we summarize the discussions of this survey, and introduce the challenges and future directions for LLMs, in the following aspects. 

\paratitle{Basics and Principles.} 
Instead of training on specific task goals, LLMs learn from unsupervised pre-training on large-scale text data. This is quite different from previous multi-task learning approaches, which aim to extend the training tasks as possible to achieve sufficient generalization.  
Thus, it is essential to reveal the basic principles  or elements that establish the foundation of the abilities of LLMs.  
Although the basic idea of language models is intuitive, it is still challenging to formally explain why LLMs trained by  simple language modeling objectives (\eg next token prediction) can become capable of solving various real-world tasks. 
To investigate this problem, a promising approach is to study the  
capacity learning (or selection) mechanism based on unsupervised pre-training, since 
  the model capacity of LLMs strongly depends on  pre-training data.  
In addition, \emph{scaling}   plays an important role in improving the capacity of LLMs~\cite{Brown-NeurIPS-2020-Language,Wei-arxiv-2022-Emergent,Rae-arxiv-2021-Scaling}, and it is very useful to conduct more theoretical analysis about how the behaviors of large models relate to those of small models, \eg what behaviors of large models can be inferred from small models and what can't be predicted  indeed.  
Another research direction is to explore more deep analysis on model generalization for LLMs, since increasing concerns have been raised about whether LLMs can generalize beyond the knowledge encoded by pre-training data. Furthermore,
data contamination has become a severe issue for fairly assessing the performance of LLMs~\cite{zhou-arxiv-2023-dont}, and thus 
setting appropriate evaluation protocol will be the basis to investigate and analyze the model capacity of LLMs. 






\paratitle{Model Architecture.} Due to the scalability and effectiveness, 
Transformer 
has become the de facto architecture for building LLMs. %
Various strategies have been proposed to improve the performance of this architecture, such as  neural network configuration  and scalable parallel  training (see discussions in Section~\ref{sec:configuration}).   
However, Transformer still suffers from  high training  costs and slow inference rates. More efforts~\cite{peng-2023-arxiv-rwkv,sun-2023-arxiv-retnet} are still in need to develop improved model architectures  for large-scale pre-training. 
Specially, system-level or hardware-level optimization (\eg FlashAttention~\cite{Dao-2023-arxiv-flashattention2}) is worth more  exploration to improve the efficiency of Transformer architectures. %
In addition, as an important basic  capacity, existing LLMs typically maintain a long context window. For example, the most recent GPT-4 Turbo enables a long context of  128K tokens, and Claude 2.1 also supports the input up to 200K tokens. Although many efforts have been made to enhance  the long context modeling ability of LLMs~\cite{su-online-2023-Rerope,Press-ICLR-2022-Train}, the resulting models still can't well process the information in the context window~\cite{Liu-arxiv-2023-Lost}. To address this issue, specific architecture adaptations or algorithms might be needed to enhance the modeling and utilization  of long context information.  Another worrying concern is that existing work mostly focuses on training LLMs with decoder-only Transformers. Despite the effectiveness, it severely limits the more wide, diverse explorations on alternative model architectures. 


  



\paratitle{Model Training.} 
For pre-training, it is essential to establish a data-centric infrastructure and training procedure for LLM optimization, which can effectively support a systematic process of data collection, data cleaning, data mixture, and data  curriculum. Furthermore, it also calls for more flexible mechanisms of hardware support or resource schedule, so as to better organize and utilize the resources in a computing cluster. 
In  practice, it is very  challenging to pre-train capable LLMs, due to the huge compute consumption and the sensitivity to data quality and  training tricks~\cite{Zeng-arxiv-2022-GLM,Scao-arxiv-2022-BLOOM}. 
Thus, it becomes particularly important to develop systemic, economical pre-training approaches for optimizing LLMs, \eg predictable scaling~\cite{OpenAI-OpenAI-2023-GPT-4} and proxy model training~\cite{Xie-arxiv-2023-doremi}.  %
More training recipes or principles should be investigated and shared to reduce the potential  risk of degradation or failure in large-scale model optimization.  
Although increasingly more model checkpoints and cleaned datasets have been released, there still lacks  reproducible work on pre-training data preparation (\eg detailed cleaning strategies) and data scheduling (\eg data mixture and curriculum).   %
Since it is very costly to pre-train a LLM from scratch, it is  important to design suitable mechanisms for continually pre-training or fine-tuning the LLM based on publicly available model checkpoints (\eg LLaMA~\cite{Touvron-arxiv-2023-LLaMA} and Flan-T5~\cite{Chung-arxiv-2022-Scaling}).    
For this purpose, a number of technical issues have to be resolved, \eg  catastrophic forgetting and task specialization.   
{Furthermore, it is also useful to develop effective tuning strategies that effectively inject or edit specific knowledge~\cite{Yao-arxiv-2023-Editing}, \eg correcting the outdated facts.}




\paratitle{Model Utilization.} 
Based on the natural language interface, \emph{prompting} has become the prominent approach for using LLMs to solving various tasks. %
By combining task descriptions and demonstration examples into prompts, in-context learning~(ICL) endows LLMs with the ability to perform well on new tasks, even outperforming full-data fine-tuned models in some cases. 
To enhance the ability of complex reasoning, advanced prompting techniques have been proposed,  exemplified by the chain-of-thought~(CoT) strategy, which includes the intermediate reasoning steps into prompts.
Furthermore, planning is a promising approach for solving  complex tasks, which iteratively  invokes LLMs by leveraging tool use capacities. Despite these efforts, several basic  problems related to prompting are still  under-explored:   why a good prompt can elicit the correct answer but a bad prompt cannot,  how to reveal the working principles of advanced prompting methods (\eg ICL and  CoT) and  further improve these existing approaches, and how to efficiently find the effective prompts for  LLMs on  specific tasks.     
%
Furthermore, from a practical perspective, it has become a fundamental challenge to reduce the inference cost of LLMs, especially in large-scale deployment. 
Another popular research direction is  retrieval-augmented generation, where retrieved contexts from supporting sources are included into prompts for task solving. It has been shown that retrieval augmentation can extend the knowledge boundary and improve the question answering capacity~\cite{Ren-arxiv-2023-Investigating}, but may suffer from the effectiveness of long context utilization by LLMs~\cite{Liu-arxiv-2023-Lost}. %
 

\paratitle{Safety and Alignment.}
Despite the capacities, LLMs are faced with great safety challenges in practical use.
As a fundamental issue of probabilistic modeling nature, LLMs exhibit a tendency to generate hallucinations~\cite{Bang-arxiv-2023-A}, referring to texts that seem plausible but may be factually incorrect~\cite{OpenAI-OpenAI-2023-GPT-4}.  
What is worse, 
LLMs might be elicited by intentional instructions to produce harmful, biased, or toxic texts for malicious systems, leading to the potential risks of misuse~\cite{Brown-NeurIPS-2020-Language,Ouyang-arxiv-2022-Training}. 
To have a detailed  discussion of the safety issues of LLMs (\eg privacy, overreliance, disinformation, and influence operations), the readers can refer to the {GPT-3/4 technical reports~\cite{OpenAI-OpenAI-2023-GPT-4,Brown-NeurIPS-2020-Language}.  %
As the major technical approach to averting these issues, alignment methods (\eg RLHF)~\cite{Ouyang-arxiv-2022-Training,Glaese-arxiv-2022-Improving} have been widely used by leveraging human feedback for developing well-aligned LLMs.
However, RLHF heavily relies on high-quality human feedback data from professional labelers, which is costly and time-consuming to recruit qualified human annotators. %
Therefore, it is necessary to improve the RLHF framework for reducing the efforts of human labelers and seek a more efficient annotation approach with guaranteed data quality,  \eg LLMs can be employed to assist the labeling work. 
Furthermore, it is also suggested to develop simplified optimization algorithms for alignment~\cite{Rafailov-arxiv-2023-Direct,Guo-arxiv-2023-Beyond}, to reduce   the training difficulty and unstability of RLHF. 
As another practical approach, red teaming~\cite{Ganguli-arxiv-2022-Red,Perez-EMNLP-2022-Red} has been adopted for improving the model safety of LLMs, which utilizes the collected adversarial prompts to refine the LLMs (\ie  avoiding the attacks from red teaming). 
In addition, 
privacy  concerns are also important to consider when fine-tuning LLMs with domain-specific data, and thus federated based learning~\cite{Kuang-2023-arxiv-FederatedScope} can be useful in privacy-restricted scenarios.   

 



\paratitle{Application and Ecosystem.}
As LLMs have shown strong capacities  in solving various tasks, they can be applied in a broad range of real-world applications (\ie following task-specific natural language instructions).   %
As a remarkable progress, ChatGPT has potentially changed the way how humans access information, which has been additionally integrated in  the release of \emph{New Bing}. Generally, in the near future, it can be foreseen that LLMs would have a significant impact on  information-seeking techniques, including both search engines and recommender systems.
Furthermore, LLMs make it possible to develop more intelligent systems (\eg autonomous AI agents) to tackle various complex tasks in real-world scenarios.  
Specially, Assistants API has been launched by OpenAI (featured by  instructions, knowledge and tool use), enabling rapid development of agent-like assistants within the applications.  
This wave of technical innovation would  lead to an  ecosystem of LLM-empowered applications (\eg OpenAI’s GPT Store), which has a close connection  with human life.   
Lastly, the rise of LLMs sheds light on the exploration of artificial general intelligence~(AGI). It is promising to develop more smart AI systems than ever. However, in this development process, AI safety should be one of the primary concerns, \ie making AI lead to good for humanity but not bad~\cite{OpenAI-blog-2023-Planning}.  


\section*{\textsc{Coda}}
It is not an easy job to write this long survey and update its content with timely work. First of all, we would like to sincerely thank the support from the readers and our team members. We work very hard on this survey, and hope that it can present a comprehensive, timely reference for LLMs. 

\paratitle{Survey Writing}. This survey was planned during a   discussion meeting held by our research team, and we aimed to summarize the recent advances of large language models as a highly readable report for our team members. The first draft was finished on March 13, 2023, in which our team members tried their best to include the related studies about LLMs in a relatively  objective, comprehensive way. 
Then, we have extensively revised the writing and contents in several passes.  
Due to the space limit, we can only include a fraction of existing  LLMs in Figure~\ref{fig:llms_timeline} and Table~\ref{tab:resource_model}, by setting the selection criterion.
However, we set a more relaxed criterion for model selection on our GitHub page (\url{https://github.com/RUCAIBox/LLMSurvey}), which will be regularly maintained. 
We release the initial version on March 31, 2023,  the major revision on June 29, 2023, and second   version on September 10, 2023, and this latest version (major revision) on November 23,  2023. 

\paratitle{Seeking for Advice}. Despite all our efforts, this survey is still far from perfect: we are likely to miss important references or topics, and might also have non-rigorous expressions or discussions. 
We will continuously update this survey, and improve the quality as much as we can. 
For us, survey writing is also a learning process for LLMs by ourselves. 
For readers with constructive suggestions to improve this survey, you are welcome to leave comments on the GitHub page of our survey or directly email our authors. 
We will make  revisions following the received comments or suggestions in a future version, and acknowledge the readers who have contributed constructive suggestions in our survey.

\paratitle{Update log}. In this part, we regularly  maintain an update log for the submissions of this survey to arXiv: 
\begin{itemize}
\item First release on March 31, 2023: the initial  version. 
\item Update on April 9, 2023: add the affiliation information, revise Figure~\ref{fig:llms_timeline} and Table~\ref{tab:resource_model} and clarify the corresponding selection criterion for LLMs, improve the writing,  and correct  some minor errors. 
\item Update on April 11, 2023: correct  the errors for library resources.  
\item Update on April 12, 2023: revise  Figure~\ref{fig:llms_timeline} and Table~\ref{tab:resource_model}, and clarify the release date of LLMs.
\item Update on April 16, 2023: add a new Section~\ref{sec-GPT-series} about the technical evolution of GPT-series models.
\item Update on April 24, 2023: add the discussion about scaling laws and add some explanations about the model sizes for emergent abilities (Section~\ref{sec-background}); add an illustrative figure for the attention patterns for different architectures in Figure~\ref{fig:architectures}, and add the detailed formulas in Table~\ref{tab:detailed_configuration}.
\item Update on April 25, 2023: revise some copy errors in figures and tables. 
\item Update on April 27, 2023: add efficient tuning in Section~\ref{sec-PEFT}.
\item Update on April 28, 2023: revise  Section~\ref{sec-PEFT}.
\item Update on May 7, 2023: revise Table~\ref{tab:resource_model}, Table~\ref{tab:corpora}, and some minor points.
\item {Update on June 29, 2023 (major revision): }
\begin{itemize}
\item Section~\ref{sec:introduction}: add Figure~\ref{fig:paper_number} for the trends of published LLM papers in arXiv;
\item  Section~\ref{sec-overview}: add  Figure~\ref{fig:openai} for GPT's evolution and the corresponding discussion;
\item  Section~\ref{sec-resource}: add Figure~\ref{fig:llama_family} for LLaMA family and the corresponding discussion;
\item  Section~\ref{sec-adaptation}: add latest discussion about the synthetic data formatting of instruction tuning in Section~\ref{sec-instruction-formatted},  the empirical analysis for instruction tuning in Section~\ref{instruction-results},   parameter-efficient model adaptation in Section~\ref{sec-PEFT} and memory-efficient adaptation in Section~\ref{sec-memory};
\item Section~\ref{sec-utilization}: add latest discussion about the underlying mechanism of ICL~\ref{sec-ICL-mechanism},  planning for complex task solving in Section~\ref{subsec-planning};
\item Section~\ref{sec-evaluation}: update Table~\ref{tab:dataset} for representative datasets for evaluating advanced abilities of LLMs, and empirical ability evaluation in Section~\ref{sec-empirical};
\item  Section~\ref{subsec:promptdesign}: add prompt design;%
\item  Section~\ref{sec-application}: add the discussions on applications of LLMs in finance and scientific research domains;
\end{itemize}
\item {Update on September 10, 2023 (major revision): }
\begin{itemize}
\item 
{Claim the copyrights of the figures and tables in this paper.}
\item %
{Add latest LLMs, techniques and their descriptions in Section~\ref{sec-resource}, Section~\ref{sec-pretraining}, Section~\ref{sec-adaptation}, Section~\ref{sec-utilization} and Section~\ref{sec-evaluation};}
\item {Section~\ref{sec-pretraining}: add latest discussion about the decoding strategy in Section~\ref{sec-decoding};}
\item {Section~\ref{sec-adaptation}: add latest discussion about the practical tricks for instruction tuning in Section~\ref{sec-ituning-strategy}, the empirical analysis on LLaMA (13B) for instruction tuning in Section~\ref{instruction-results}, practical strategies for RLHF in Section~\ref{sub:RLHF},  alignment without RLHF in Section~\ref{sec-alignment-withoutRL} and remarks on SFT and RLHF in Section~\ref{sec-remarks-SFTRL};}
\item {Section~\ref{sec-utilization}: update the content about the planning for complex task solving in Section~\ref{subsec-planning};}
\item {Section~\ref{sec-evaluation}: add  discussions about evaluation approaches in Section~\ref{subsec-evaapp}, Table~\ref{tab-category-evaluation} for the category of existing evaluation work, and update empirical ability evaluation in Section~\ref{sec-empirical} and the results on Table~\ref{tab-experimental-res};}
\item  {Section~\ref{subsec:promptdesign}: add new prompt examples in Table~\ref{tab-tips};}%
\end{itemize}

\item {Update on November 23, 2023 (this version): }
\begin{itemize}
    \item %
    {
Section~\ref{sec:introduction}: add Figure~\ref{fig:task_solvers} for the evolution process of four generations of language models;}
    \item %
    {Section~\ref{sec-overview}: add more discussion about scaling laws and how emergent abilities relate to scaling laws;}
    \item {Section~\ref{sec-resource}: add latest LLMs in Figure~\ref{fig:llms_timeline} and Table~\ref{tab:resource_model}, latest APIs in Section~\ref{sec:apis_for_llms}, commonly used datasets for instruction tuning and alignment tuning in Section~\ref{sec:commonly_used_fituning}, and several libraries in Section~\ref{sec:library};}
    \item {Section~\ref{sec-pretraining}: add latest discussion about the data scheduling, including data mixtures and data curriculum in Section~\ref{sec:data_scheduling}; add summary of data preparation in Section~\ref{sec:data_prepare_sug}; add discussion about modeling long context in Section~\ref{sec:long_context}; add discussion about decoding efficiency issues and add latest decoding strategies in Section~\ref{sec-decoding};}
    \item {Section~\ref{sec-adaptation}: add latest discussion about instance construction and tuning strategies in Section~\ref{sec-instruction}; add latest discussion about process-supervised RLHF in Section~\ref{sub:RLHF}, and the empirical study on quantized LLaMA models (7B and 13B) in Section~\ref{sec:quantization_empirical};}
    \item {Section~\ref{sec-utilization}: add latest discussion about prompt optimization in Section~\ref{sec:prompt_opt}, and update the content about chain-of-thought prompting in Section~\ref{subsec-cot};}
    \item {Section~\ref{sec-application}: add latest discussion about LLM for research directions in Section~\ref{sec:llm4community};}
    \item {Section~\ref{sec-con}: revise the content in the several aspects.}
\end{itemize}

\end{itemize}

\paratitle{Planning Content}. We will regularly include  new content into this survey, to make it more self-contained and up-to-date. Here, we list several potential topics that might appear in the next major version(s): (1) more experiments with larger language models for both instruction tuning and ability evaluation; (2) more detailed prompting practice; (3) training recipe; (4) more theoretical analysis and discussion; (5) more discussions on applications. 


\paratitle{Clarifications on Experiments}. In this version, we have included a number experiments on instruction-tuning (Table~\ref{tab-instruction-tuning-res}), overall ability evaluation (Table~\ref{tab-experimental-res}), and prompt engineering (Table~\ref{tab-instructions}). Due to the limit of computational resources, our experiments are not complete, limited to small-sized models or a few comparisons. Despite that, we feel that it might be  meaningful to share the partial results to the public. We will try to include the missing results of larger models or more comparisons in the future versions. \textbf{We also call for support of computing power for conducting more comprehensive experiments.}  

\paratitle{Chinese Version}. We also provide a translated Chinese version (corresponding to the first release) of this survey paper at the link: \url{https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM\_Survey\_Chinese.pdf}. Four volunteers contribute to check and revise the content, and they are Yiwen Hu, Xin Deng, Xinming Hou, Yanbin Yin, and Zhanshuo Cao (in order of contribution). We will also continuously update the Chinese version, but it may not be as timely as the latest English version.