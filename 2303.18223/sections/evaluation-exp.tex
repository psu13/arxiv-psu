
\begin{table*}[htb]
\renewcommand\arraystretch{1.1}
\setlength\tabcolsep{2.5pt}
    \centering
    \caption{Evaluation on the eight abilities of  LLMs with specially selected tasks. The shade of the \colorbox[HTML]{FC8D59}{Orange} and \colorbox[HTML]{92BFDB}{Blue} fonts denote the performance orders of the results in closed-source and open-source models, respectively. This table will be continuously updated by incorporating the results of more models.
    }
    \label{tab-experimental-res}
\resizebox{1.85\columnwidth}{!}{
\begin{tabular}{lccccccccc}
\toprule
\multirow{2.5}{*}{\textbf{Models}}   & \multicolumn{4}{c}{\textbf{Language Generation}} & \multicolumn{5}{c}{\textbf{Knowledge Utilization}} \\ 
\cmidrule(r){2-5}\cmidrule(r){6-10}
& LBD$\uparrow$ & WMT$\uparrow$ & XSum$\uparrow$ & HumanEval$\uparrow$ & TriviaQA$\uparrow$ & NaturalQ$\uparrow$ & WebQ$\uparrow$ & ARC$\uparrow$ & WikiFact$\uparrow$ \\
\midrule
ChatGPT & \cellcolor[HTML]{FEE8DD}55.81 & \cellcolor[HTML]{FCA77F}{36.44} & \cellcolor[HTML]{FC8D59}{21.71} & \cellcolor[HTML]{FC8D59}{79.88} & \cellcolor[HTML]{FC8D59}{54.54} & \cellcolor[HTML]{FC8D59}{21.52} & \cellcolor[HTML]{FEDCCC}17.77 & \cellcolor[HTML]{FC8D59}{93.69} & \cellcolor[HTML]{FEDCCC}{29.25} \\
Claude & \cellcolor[HTML]{FCA77F}64.47 & \cellcolor[HTML]{FEE8DD}{31.23} & \cellcolor[HTML]{FEE8DD}{18.63} & \cellcolor[HTML]{FEF7F3}{51.22} & \cellcolor[HTML]{FEF7F3}40.92 & \cellcolor[HTML]{FEF7F3}13.77 & \cellcolor[HTML]{FEF7F3}14.57 & \cellcolor[HTML]{FEF7F3}66.62 & \cellcolor[HTML]{FCA77F}{34.34} \\
Claude 2 & \cellcolor[HTML]{FEF7F3}45.20 & \cellcolor[HTML]{FEF7F3}12.93 & \cellcolor[HTML]{FEDCCC}19.13 & \cellcolor[HTML]{FCA77F}78.04 & \cellcolor[HTML]{FCA77F}54.30 & \cellcolor[HTML]{FCA77F}21.30 & \cellcolor[HTML]{FC8D59}21.06 & \cellcolor[HTML]{FEE8DD}79.97 & \cellcolor[HTML]{FC8D59}35.83\\
Davinci003 & \cellcolor[HTML]{FC8D59}69.98 & \cellcolor[HTML]{FC8D59}{37.46} & \cellcolor[HTML]{FEF7F3}{18.19} & \cellcolor[HTML]{FEDCCC}{67.07} & \cellcolor[HTML]{FEE8DD}51.51 & \cellcolor[HTML]{FEE8DD}17.76 & \cellcolor[HTML]{FEE8DD}16.68 & \cellcolor[HTML]{FEDCCC}88.47 & \cellcolor[HTML]{FEF7F3}{28.29}  \\
Davinci002 &  \cellcolor[HTML]{FEDCCC}58.85 & \cellcolor[HTML]{FEDCCC}{35.11} & \cellcolor[HTML]{FCA77F}{19.15} & \cellcolor[HTML]{FEE8DD}{56.70} & \cellcolor[HTML]{FEDCCC}52.11 & \cellcolor[HTML]{FEDCCC}20.47 & \cellcolor[HTML]{FCA77F}{18.45} & \cellcolor[HTML]{FCA77F}89.23 & \cellcolor[HTML]{FEE8DD}{29.15}  \\
\midrule
LLaMA 2-Chat~(7B) & 56.12 & 12.62 & \cellcolor[HTML]{A7CBE2}16.00 & 11.59 & \cellcolor[HTML]{92BFDB}38.93 & \cellcolor[HTML]{92BFDB}12.96 & \cellcolor[HTML]{A7CBE2}11.32 & \cellcolor[HTML]{92BFDB}72.35 & 23.37 \\
Vicuna~(13B) & 62.45 & \cellcolor[HTML]{A7CBE2}20.49 & \cellcolor[HTML]{92BFDB}17.87 & \cellcolor[HTML]{92BFDB}20.73 & \cellcolor[HTML]{C4DDEC}29.04 & \cellcolor[HTML]{C6DEED}10.75 & \cellcolor[HTML]{92BFDB}11.52 & \cellcolor[HTML]{E5F0F7}
20.69 & \cellcolor[HTML]{92BFDB}28.76 \\
Vicuna~(7B) & \cellcolor[HTML]{C4DDEC}63.90 & \cellcolor[HTML]{C6DEED}{19.95} & \cellcolor[HTML]{C6DEED}{13.59} & \cellcolor[HTML]{A7CBE2}{17.07} & 28.58 & \cellcolor[HTML]{C4DDEC}9.17 & 6.64 & 16.96 & \cellcolor[HTML]{C6DEED}{26.95} \\
Alpaca~(7B) & \cellcolor[HTML]{E5F0F7}63.35 & \cellcolor[HTML]{92BFDB}{21.52} & {8.74} & {13.41} & 17.14 & 3.24 & 3.00 & \cellcolor[HTML]{C6DEED}49.75 & \cellcolor[HTML]{C4DDEC}{26.05}\\
ChatGLM~(6B) & 33.34 & \cellcolor[HTML]{C4DDEC}{16.58} & \cellcolor[HTML]{C4DDEC}{13.48} & {13.42} & 13.42 & 
4.40 & \cellcolor[HTML]{C4DDEC}9.20 & \cellcolor[HTML]{A7CBE2}{55.39} & {16.01} \\
\midrule
LLaMA 2~(7B) & \cellcolor[HTML]{C6DEED}66.39 & 11.57 & \cellcolor[HTML]{E5F0F7}11.57 & \cellcolor[HTML]{A7CBE2}17.07 & \cellcolor[HTML]{C6DEED}30.92 & 5.15 & 2.51 & \cellcolor[HTML]{C4DDEC}24.16 & \cellcolor[HTML]{A7CBE2}28.06\\
LLaMA~(7B) & \cellcolor[HTML]{92BFDB}67.68 & \cellcolor[HTML]{E5F0F7}{13.84} & {8.77} & \cellcolor[HTML]{C4DDEC}{15.24} & \cellcolor[HTML]{A7CBE2}{34.62} & \cellcolor[HTML]{E5F0F7}7.92 & \cellcolor[HTML]{C6DEED}{11.12} & 4.88 & {19.78} \\
Falcon~(7B) & \cellcolor[HTML]{A7CBE2}66.89 & {4.05} & {10.00} & {10.37} & \cellcolor[HTML]{E5F0F7}28.74 & \cellcolor[HTML]{A7CBE2}{10.78} & \cellcolor[HTML]{E5F0F7}8.46 & 4.08 & \cellcolor[HTML]{E5F0F7}{23.91} \\
Pythia~(12B) & 61.19 & {5.43} & {8.87} & \cellcolor[HTML]{E5F0F7}{14.63} & 15.73 &
1.99 & 4.72 & 11.66 & {20.57} \\
Pythia~(7B) & 56.96 & {3.68} & {8.23} & {9.15} & 10.16 & 1.77 & 3.74 & 11.03 & {15.75}\\
\midrule[0.8pt]
\multirow{2.5}{*}{\textbf{Models}} & \multicolumn{3}{c}{\textbf{Knowledge Reasoning}} & \multicolumn{2}{c}{\textbf{Symbolic Reasoning}} & \multicolumn{2}{c}{\textbf{Mathematical Reasoning}} & \multicolumn{2}{c}{\textbf{Interaction with Environment}}\\ 
\cmidrule(r){2-4}\cmidrule(r){5-6}\cmidrule(r){7-8}\cmidrule(r){9-10}
& OBQA$\uparrow$ & HellaSwag$\uparrow$ & SocialIQA$\uparrow$ & C-Objects$\uparrow$ & Penguins$\uparrow$ & GSM8k$\uparrow$ & MATH$\uparrow$ & ALFW$\uparrow$ & WebShop$\uparrow$ \\
\midrule
ChatGPT & \cellcolor[HTML]{FCA77F}81.20 & \cellcolor[HTML]{FCA77F}61.43 & \cellcolor[HTML]{FC8D59}73.23 & \cellcolor[HTML]{FEF7F3}53.20 & \cellcolor[HTML]{FEF7F3}40.27 & \cellcolor[HTML]{FCA77F}{78.47} & \cellcolor[HTML]{FC8D59}{33.78} & \cellcolor[HTML]{FEF7F3}58.96 & \cellcolor[HTML]{FEDCCC}45.12/15.60 \\
Claude & \cellcolor[HTML]{FC8D59}81.80 & \cellcolor[HTML]{FEDCCC}54.95 & \cellcolor[HTML]{FC8D59}73.23 & \cellcolor[HTML]{FEE8DD}59.95 & \cellcolor[HTML]{FEE8DD}47.65 & \cellcolor[HTML]{FEDCCC}70.81 & \cellcolor[HTML]{FEDCCC}20.18 & \cellcolor[HTML]{FCA77F}76.87 & \cellcolor[HTML]{FCA77F}47.72/23.00 \\
Claude 2 & \cellcolor[HTML]{FEE8DD}71.60 & \cellcolor[HTML]{FEE8DD}50.75 & \cellcolor[HTML]{FEE8DD}58.34 & \cellcolor[HTML]{FC8D59}{66.76} & \cellcolor[HTML]{FC8D59}{74.50} & \cellcolor[HTML]{FC8D59}{82.87} & \cellcolor[HTML]{FCA77F}{32.24} & \cellcolor[HTML]{FC8D59}77.61 & \cellcolor[HTML]{FEE8DD}34.96/19.20 \\
Davinci003 & \cellcolor[HTML]{FEDCCC}74.40 & \cellcolor[HTML]{FC8D59}62.65 & \cellcolor[HTML]{FEDCCC}69.70 & \cellcolor[HTML]{FCA77F}{64.60} & \cellcolor[HTML]{FEDCCC}61.07 & \cellcolor[HTML]{FEE8DD}57.16 & \cellcolor[HTML]{FEE8DD}17.66 & \cellcolor[HTML]{FEDCCC}65.67 & \cellcolor[HTML]{FC8D59}{64.08/32.40} \\
Davinci002 & \cellcolor[HTML]{FEF7F3}69.80 & \cellcolor[HTML]{FEF7F3}47.81 & \cellcolor[HTML]{FEF7F3}57.01 & \cellcolor[HTML]{FEDCCC}62.55 & \cellcolor[HTML]{FCA77F}{67.11} & \cellcolor[HTML]{FEF7F3}49.96 & \cellcolor[HTML]{FEF7F3}14.28 & \cellcolor[HTML]{FCA77F}{76.87} & \cellcolor[HTML]{FEF7F3}29.66/15.20 \\
\midrule
LLaMA 2-Chat~(7B) & \cellcolor[HTML]{A7CBE2}45.62 & \cellcolor[HTML]{C6DEED}
74.01 & \cellcolor[HTML]{C4DDEC}43.84 & \cellcolor[HTML]{C4DDEC}43.40 & \cellcolor[HTML]{A7CBE2}38.93 & 9.63 & 2.22 & \cellcolor[HTML]{92BFDB}11.19 & \cellcolor[HTML]{92BFDB}{24.51/5.60} \\
Vicuna~(13B) & \cellcolor[HTML]{E5F0F7}43.65 & \cellcolor[HTML]{E5F0F7}70.51 & \cellcolor[HTML]{C6DEED}45.97 & \cellcolor[HTML]{92BFDB}53.55 & \cellcolor[HTML]{C6DEED}36.91 & \cellcolor[HTML]{92BFDB}18.50 & \cellcolor[HTML]{A7CBE2}3.72 & \cellcolor[HTML]{A7CBE2}8.96 & \cellcolor[HTML]{A7CBE2}{22.74/5.00} \\
Vicuna~(7B) & \cellcolor[HTML]{C4DDEC}43.84 & 69.25 & \cellcolor[HTML]{A7CBE2}
46.27 & \cellcolor[HTML]{A7CBE2}44.25 & \cellcolor[HTML]{C4DDEC}36.24 & \cellcolor[HTML]{A7CBE2}14.03 & \cellcolor[HTML]{C6DEED}3.54 & 1.49 & \cellcolor[HTML]{C4DDEC}
6.90/1.40\\
Alpaca~(7B) & \cellcolor[HTML]{92BFDB}47.82 & 69.81 & \cellcolor[HTML]{92BFDB}47.55 & 39.35 & \cellcolor[HTML]{92BFDB}40.27 & \cellcolor[HTML]{E5F0F7}4.93 & \cellcolor[HTML]{92BFDB}4.16 & 4.48 & 0.00/0.00\\
ChatGLM~(6B) & 30.42 & 29.27 & 33.18 & 14.05 & 14.09 & 3.41 & 1.10 & 0.00 & 0.00/0.00 \\
\midrule
LLaMA 2~(7B) & \cellcolor[HTML]{C6DEED}44.81 & \cellcolor[HTML]{A7CBE2}74.25 &41.72 & \cellcolor[HTML]{C6DEED}43.95 & \cellcolor[HTML]{E5F0F7}35.75 & \cellcolor[HTML]{C6DEED}10.99 & \cellcolor[HTML]{E5F0F7}2.64 & \cellcolor[HTML]{A7CBE2}8.96 & 0.00/0.00 \\
LLaMA~(7B) & 42.42 & \cellcolor[HTML]{C4DDEC}73.91 & 41.46 & \cellcolor[HTML]{E5F0F7}39.95 & 34.90 & \cellcolor[HTML]{C6DEED}10.99 & \cellcolor[HTML]{C4DDEC}3.12 & 2.24 & 0.00/0.00 \\
Falcon~(7B) & 39.46 & \cellcolor[HTML]{92BFDB}74.58 & \cellcolor[HTML]{E5F0F7}
42.53 & 29.80 & 24.16 & 1.67 & 0.94 & \cellcolor[HTML]{C4DDEC}
{7.46} & 0.00/0.00 \\
Pythia~(12B) & 37.02 & 65.45 & 41.53 & 32.40 & 26.17 & 2.88 & 1.96 & 5.22 & \cellcolor[HTML]{E5F0F7}3.68/0.60 \\
Pythia~(7B) & 34.88 & 61.82 & 41.01 & 29.05 & 27.52 & 1.82 & 1.46  & \cellcolor[HTML]{C4DDEC}{7.46} & \cellcolor[HTML]{C6DEED}{10.75/1.80}\\
\midrule[0.8pt]
\multirow{2.5}{*}{\textbf{Models}}  &  \multicolumn{5}{c}{\textbf{Human Alignment}} & \multicolumn{4}{c}{\textbf{Tool Manipulation}} \\
\cmidrule(r){2-6}\cmidrule(r){7-10}
& TfQA$\uparrow$ & C-Pairs$\downarrow$ & WinoGender$\uparrow$ & RTP$\downarrow$ & HaluEval$\uparrow$ & HotpotQA$\uparrow$ & Gorilla-TH$\uparrow$ & Gorilla-TF$\uparrow$ & Gorilla-HF$\uparrow$ \\
\midrule
ChatGPT & \cellcolor[HTML]{FCA77F}{69.16} & \cellcolor[HTML]{FEE8DD}{18.60} & \cellcolor[HTML]{FCA77F}{62.50}/\cellcolor[HTML]{FCA77F}{72.50}/\cellcolor[HTML]{FCA77F}{79.17} & \cellcolor[HTML]{FC8D59}{3.07} & \cellcolor[HTML]{FC8D59}{66.64} & \cellcolor[HTML]{FEF7F3}{23.80} & \cellcolor[HTML]{FCA77F}{67.20} & \cellcolor[HTML]{FC8D59}
{44.53} & \cellcolor[HTML]{FCA77F}{19.36}\\
Claude & \cellcolor[HTML]{FEDCCC}{67.93} & \cellcolor[HTML]{FEF7F3}{32.73} & \cellcolor[HTML]{FEE8DD}{71.67}/\cellcolor[HTML]{FEE8DD}{55.00}/\cellcolor[HTML]{FEE8DD}{52.50} & \cellcolor[HTML]{FEDCCC}{3.75} & \cellcolor[HTML]{FCA77F}{63.75}  & \cellcolor[HTML]{FEDCCC}{33.80} & \cellcolor[HTML]{FEE8DD}{22.04} & \cellcolor[HTML]{FEDCCC}{7.74} & \cellcolor[HTML]{FEDCCC}{7.08}\\
Claude 2 & \cellcolor[HTML]{FC8D59}{71.11} & \cellcolor[HTML]{FEDCCC}{10.67} & \cellcolor[HTML]{FEF7F3}{60.00}/60.00/55.83 & \cellcolor[HTML]{FCA77F}{3.20} & \cellcolor[HTML]{FEF7F3}50.63 & \cellcolor[HTML]{FC8D59}{36.4} & \cellcolor[HTML]{FEDCCC}{61.29} & \cellcolor[HTML]{FCA77F}{22.19} & \cellcolor[HTML]{FC8D59}{23.67} \\
Davinci003 & \cellcolor[HTML]{FEE8DD}{60.83} & \cellcolor[HTML]{FC8D59}{0.99} & \cellcolor[HTML]{FC8D59}67.50/68.33/{79.17} & \cellcolor[HTML]{FEE8DD}{8.81} & \cellcolor[HTML]{FEE8DD}{58.94}  & \cellcolor[HTML]{FCA77F}
{34.40} & \cellcolor[HTML]{FC8D59}
{72.58} & \cellcolor[HTML]{FEE8DD}{3.80} & \cellcolor[HTML]{FEE8DD}{6.42}\\
Davinci002 & \cellcolor[HTML]{FEF7F3}{53.73} & \cellcolor[HTML]{FCA77F}{7.56} & \cellcolor[HTML]{FEDCCC}{72.50}/70.00/64.17 & \cellcolor[HTML]{FEF7F3}{10.65} & \cellcolor[HTML]{FEDCCC}{59.67}  & \cellcolor[HTML]{FEE8DD}{26.00} & \cellcolor[HTML]{FEF7F3}{2.69} & \cellcolor[HTML]{FEF7F3}{1.02} & \cellcolor[HTML]{FEF7F3}{1.00}\\
\midrule
LLaMA 2-Chat~(7B) & \cellcolor[HTML]{92BFDB}{69.77} & \cellcolor[HTML]{A7CBE2}{48.54} & 47.50/46.67/46.67 & \cellcolor[HTML]{A7CBE2}{4.61} & \cellcolor[HTML]{C6DEED}43.82 & \cellcolor[HTML]{C4DDEC}{4.40} & 0.00 & 0.00 & \cellcolor[HTML]{C6DEED}{0.22} \\
Vicuna~(13B) & \cellcolor[HTML]{C6DEED}{62.30} & \cellcolor[HTML]{92BFDB}{45.95} & \cellcolor[HTML]{C6DEED}{50.83}/50.83/52.50 & \cellcolor[HTML]{E5F0F7}{5.00} & \cellcolor[HTML]{92BFDB}49.01 & \cellcolor[HTML]{A7CBE2}{11.20} & 0.00 & \cellcolor[HTML]{92BFDB}{0.44} & \cellcolor[HTML]{92BFDB}{0.89} \\
Vicuna~(7B) & \cellcolor[HTML]{C4DDEC}{57.77} & {67.44} & 49.17/49.17/49.17 & \cellcolor[HTML]{C6DEED}{4.70} & \cellcolor[HTML]{C4DDEC}{43.44} & \cellcolor[HTML]{C6DEED}{6.20} & {0.00} & {0.00} & \cellcolor[HTML]{A7CBE2}{0.33}\\
Alpaca~(7B) & {46.14} & {65.45} & \cellcolor[HTML]{A7CBE2}{53.33}/51.67/{53.33} & \cellcolor[HTML]{C4DDEC}{4.78} & \cellcolor[HTML]{A7CBE2}{44.16}  & \cellcolor[HTML]{92BFDB}{11.60} & {0.00} & {0.00} & \cellcolor[HTML]{C4DDEC}{0.11}\\
ChatGLM~(6B) & \cellcolor[HTML]{A7CBE2}{63.53} & \cellcolor[HTML]{C6DEED}{50.53} & 47.50/47.50/46.67 & \cellcolor[HTML]{92BFDB}{2.89} & {41.82} & \cellcolor[HTML]{E5F0F7}{4.00} & {0.00} & {0.00} & {0.00}\\
\midrule
LLaMA 2~(7B) & 50.06 & \cellcolor[HTML]{C4DDEC}{51.39} & 48.83/48.83/50.83 & 6.17 & \cellcolor[HTML]{E5F0F7}42.23 & 3.80 & 0.00 & 0.00 & \cellcolor[HTML]{C4DDEC}{0.11} \\
LLaMA~(7B) & {47.86} & {67.84} & \cellcolor[HTML]{92BFDB}54.17/{52.50}/51.67 & {5.94} & {14.18} & {1.60} & {0.00} & {0.00} & \cellcolor[HTML]{C4DDEC}{0.11}\\
Falcon~(7B) & {53.24} & {68.04} & \cellcolor[HTML]{E5F0F7}50.00/50.83/50.00 & {6.71} & {37.41} & {1.00} & {0.00} & {0.00} & {0.00}\\
Pythia~(12B) & \cellcolor[HTML]{E5F0F7}{54.47} & {65.78} & 49.17/48.33/49.17 & {6.59} & {27.09} & {0.40} & {0.00} & {0.00} & {0.00}\\
Pythia~(7B) & {50.92} & \cellcolor[HTML]{E5F0F7}{64.79} & \cellcolor[HTML]{C4DDEC}51.67/49.17/50.00 & {13.02} & {25.84}  & {0.20} & {0.00} & {0.00} & {0.00}\\
\bottomrule\end{tabular}
}
\end{table*}



\begin{table*}[!h]
    \centering
    \caption{Prompt examples and their performance of ChatGPT on representative tasks. For most  tasks, we compare the performance for \emph{simple} and \emph{complex}  prompts. We also present the  reported performance of supervised methods. ``LG'', ``KU'', ``CR'', ``SDG'', ``IR'' are short for ``language generation'', ``knowledge utilization'', ``complex reasoning'', ``structured data generation'', ``information retrieval''. ``-'' means there is no reported supervised result previously on this dataset.}
    \label{tab-instructions}
\scriptsize %
\begin{tabular}{cp{0.10\textwidth}c p{0.55\textwidth} rr}
\toprule
\multicolumn{2}{c}{\textbf{Tasks}}   & \textbf{Datasets} & \makecell[c]{\textbf{Instructions}} &  \textbf{ChatGPT} & \textbf{Supervised} \\ 
\midrule
        \multirow{13.5}{*}{LG} & \multirow{4.5}{*}{Translation} & \multirow{4.5}{*}{WMT} & \texttt{I want you to act as a translator. Please translate the English sentence into Czech.} & 20.66 & \multirow{4.5}{*}{41.40~\cite{Zan-WMT-2022-Vega-MT}}\\
        \cmidrule{4-5}
        & & & \texttt{I want you to act as a translator. Translate the given English sentence into Czech, and ensure that the translated sentence is semantically consistent with the given sentence. $\backslash$n Sentence: \{source sentence\} $\backslash$n Translation:} & 21.12 \\
\cmidrule{2-6}
& \multirow{4.5}{*}{Summarization} & \multirow{4.5}{*}{XSum} & \texttt{Please generate a one-sentence summary for the given document.} & 21.71 & \multirow{4.5}{*}{42.08~\cite{Zhao-arxiv-2022-Calibrating}} \\
\cmidrule{4-5}
        & & & \texttt{\{document\} Try your best to summarize the main content of the given document. And generate a short summary in 1 sentence for it.$\backslash$n Summary:
} & 23.01 &\\
\midrule
\multirow{13}{*}{KU} & \multirow{2.5}{*}{Closed-Book QA} & \multirow{2.5}{*}{ARC} & \texttt{Choose your answer to the question. \{query\} \{options\}} & 85.19 & \multirow{2.5}{*}{92.00~\cite{Khashabi-EMNLP-2020-UnifiedQA}} \\
\cmidrule{4-5}
& & & \texttt{Choose a correct answer according to the given question, and output the corresponding id, do not answer other content except the answer id.} & 85.86 \\
\cmidrule{2-6}
& \multirow{6.5}{*}{Open-Book QA} & \multirow{6.5}{*}{OBQA} & \texttt{Choose your answer to the question: \{question\} \{choices\}. You must only output A, B, C, or D without any extra explanation. The answer is} & 81.20 & \multirow{6.5}{*}{87.20~\cite{Khashabi-EMNLP-2020-UnifiedQA}} \\
\cmidrule{4-5}
& & & \texttt{Following is a question that requires multi-step reasoning, use of additional common and commonsense knowledge, and rich text comprehension. Choose your answer to the question: $\backslash$n Question: Frilled sharks and angler fish live far beneath the surface of the ocean, which is why they are known as $\backslash$n Choices: $\backslash$n A. Deep sea animals $\backslash$n B. fish $\backslash$n C. Long Sea Fish $\backslash$n D. Far Sea Animals $\backslash$n You must only output A, B, C, or D without any extra explanation. The answer is} & 82.20 \\
\cmidrule{2-6}
& \multirow{2.5}{*}{Fact Extraction} & \multirow{2.5}{*}{WikiF} & \texttt{Complete the sentence with one or a few words.} & 29.25 & \multirow{2.5}{*}{34.20~\cite{Liang-arxiv-2022-Holistic}}  \\
\cmidrule{4-5}
& & & \texttt{Complete the given sentence with one entity name in Wikipedia (MUST be a noun) as short as possible, and ensure that the completed sentence conforms to the facts.} & 31.21\\
\midrule
\multirow{13}{*}{CR} & \multirow{2.5}{*}{Symbolic Reasoning} & \multirow{2.5}{*}{C-Objects} & \texttt{Problem: \{problem\}$\backslash$n
Answer:} & 53.20 & \multirow{2.5}{*}{---} \\
\cmidrule{4-5}
& & & \texttt{You are an expert in reasoning problem. Here are some examples about symbolic reasoning. You can use the knowledge in examples and solve the last problem. You should follow the examples and generate the final answer without external solution or words.} & 66.75\\
\cmidrule{2-6}
\cmidrule{2-6}
& \multirow{4.5}{*}{Math Word Problems} & \multirow{4.5}{*}{GSM8k} & \texttt{Problem: \{problem\}$\backslash$n
Solution: Let's think step by step.
} & 78.47 & \multirow{4.5}{*}{63.20~\cite{Zhu-arxiv-2022-Solving}} \\
\cmidrule{4-5}
& & & \texttt{Let's use python to solve math problems. Here are three examples how to do it,$\backslash$n Q: Olivia has \$23. She bought five bagels for \$3 each. How much money does she have left?$\backslash$n```def solution():$\backslash$n ~~~~"""Olivia has \$23. She bought five bagels for \$3 each. How much money does she have left?"""$\backslash$n ~~~~money\_initial = 23$\backslash$n ~~~~bagels = 5$\backslash$n ~~~~bagel\_cost = 3$\backslash$n ~~~~money\_spent = bagels * bagel\_cost$\backslash$n ~~~~money\_left = money\_initial - money\_spent$\backslash$n ~~~~result = money\_left$\backslash$n ~~~~return result```$\backslash$n ...... $\backslash$n How about this question?$\backslash$n Q:} & 79.30\\
\midrule
\multirow{7}{*}{SDG} & Code Synthesis & HumanEval & \texttt{I want you act as a code completer. Given a code snippet, your objective is to complete the code and ensure that it can achieve the described functionality.} & 79.88 & 48.20~\cite{Nguyen-arxiv-2023-Meet} \\
\cmidrule{2-6}
& \makecell[l]{Text-to-SQL} & Spider & \texttt{\#\#\# Complete sqlite SQL query only and with no explanation.$\backslash$n \#$\backslash$n\#\#\# Sqlite SQL tables, with their properties: $\backslash$n\#$\backslash$n\{table\}$\backslash$n\# \{foreign\_key\}$\backslash$n\#$\backslash$n\#\#\# \{question\}$\backslash$n SELECT} & 70.10 & 84.10~\cite{Li-arxiv-2023-RESDSQL} \\
\midrule
\multirow{15}{*}{IR} & Recommendation & MovieLens & \texttt{I've watched the following movies in the past in order: $\backslash$n \{user\_his\_text\} $\backslash$n$\backslash$n Now there are \{recall\_budget\} candidate movies that I can watch next: $\backslash$n \{candidate\_text\_order\} $\backslash$n Please rank these \{recall\_budget\} movies by measuring the possibilities that I would like to watch next most, according to my watching history. Please think step by step. $\backslash$n Note that my most recently watched movie is \{recent\_item\}. Please show me your ranking results with order numbers. Split your output with line break. You MUST rank the given candidate movies. You can not generate movies that are not in the given candidate list.} & 48.80 & 76.25~\cite{Kang-ICDM-2018-Self}
\\
\cmidrule{2-6}
& Conversational \quad Recommendation & ReDial & \texttt{Recommend 10 items that are consistent with user preference. The recommendation list can contain items that the dialog mentioned before. The format of the recommendation list is: no. title (year). Don't mention anything other than the title of items in your recommendation list} & 17.20 & 25.60~\cite{Yang-NAACL-2022-Improving} \\
\bottomrule\end{tabular}
\end{table*}




\subsection{Empirical Evaluation}\label{sec-empirical}
The above evaluation benchmarks and approaches are mainly employed to evaluate the overall abilities of LLMs. 
In this part, we conduct a fine-grained evaluation of the abilities discussed in Section~\ref{sec:basicability} and Section~\ref{sec:superior}.
For each kind of ability, we select representative tasks and datasets for conducting evaluation experiments to examine the corresponding performance of LLMs. %

\subsubsection{Experimental Settings}
In this part, we introduce the experimental settings for our evaluation.

\paratitle{Evaluation Models.} 
To conduct the evaluation, we consider representative LLMs from open-source models to closed-source API-accessing models as follows:

\textbullet~\emph{Open-source models.} 
Existing open-source models can be categorized into base models and instruction-tuned models. Base models are only pre-trained on a large general-purpose corpus with the language modeling objective, but without further supervised fine-tuning. In our evaluation, we select four representative base models including LLaMA (7B)~\cite{Touvron-arxiv-2023-LLaMA}, LLaMA 2 (7B)~\cite{Touvron-2023-llama2-arxiv}, Pythia (7B and 12B)~\cite{Biderman-arxiv-2023-Pythia}, and Falcon (7B)~\cite{Ebtesam-arxiv-2023-Falcon}\footnote{Experiments with larger models are still in schedule due to the limit of computational resources. }. 
Instruction-tuned models are those fine-tuned using instructions (\ie task datasets, daily chat, or synthetic instructions). In our experiments, we select four representative instruction-tuned models including Vicuna (7B and 13B)~\cite{vicuna2023}, Alpaca (7B)~\cite{alpaca}, and ChatGLM (6B)~\cite{Zeng-arxiv-2022-GLM}.
{In addition, we also include LLaMA 2-Chat (7B)~\cite{Touvron-2023-llama2-arxiv} for comparison, and it is a representative model that has been aligned with human via instruction tuning and RLHF, based on LLaMA 2 (7B).}

\textbullet~\emph{Closed-source models.} 
In addition to the open-source models, there are also closed-source models that can only be accessed via APIs, which have gained much attention from both developers and researchers. 
Here, we select four representative closed-source models including text-davinci-002/003 (short as \emph{Davinci002/003}), ChatGPT, Claude, %
{and Claude 2, where the first three models are developed by OpenAI and the other two are developed by Anthropic.}


\paratitle{Tasks and Datasets.} Next, we set up the evaluation tasks and datasets for the abilities discussed in  Section~\ref{sec:basicability} and Section~\ref{sec:superior}.  %
{We mainly evaluate the zero-shot performance of LLMs on these datasets. For more complex tasks that are hard to be solved in the zero-shot manner (\eg mathematical reasoning and tool manipulation), we mainly report the 3-shot performance, considering the context length limit of open-source models. 
}

\textbullet~\emph{Language generation.} As discussed before, for language generation, we consider evaluating three kinds of tasks, \ie language modeling, conditional text generation, and code synthesis. Specially, we select four commonly-used datasets, namely LAMBADA~\cite{Paperno-ACL-2016-LAMBADA} (language modeling), WMT'22~\cite{Kocmi-WMT-2022-Findings} (machine translation), XSum~\cite{Naryan-EMNLP-2018-XSUM} (text summarization), and HumanEval~\cite{Chen-arxiv-2021-evaluating} (code synthesis) for evaluation. 
{
In WMT'22, we construct a new evaluation set by selecting 1000 examples for each language pair from the original large-scale test set to examine the average performance of LLMs in machine translation.}
We evaluate the zero-shot performance of LLMs on these datasets, and compute the \emph{accuracy} of predicting words for LAMBADA, \emph{BLEU-4} for WMT'22, \emph{ROUGE-L} for XSum, and \emph{pass@$10$} for HumanEval.


\textbullet~\emph{Knowledge utilization.} 
To evaluate the ability of knowledge utilization, %
we select four question answering datasets (\ie TriviaQA~\cite{Joshi-ACL-2017-TriviaQA}, Natural Questions~\cite{Kwiatkowski-ACL-2019-Natural}, Web Questions~\cite{Berant-EMNLP-2013-Semantic}, and ARC~\cite{Clark-arxiv-2018-Think}), and a fact extraction dataset, WikiFact~\cite{Goodrich-KDD-2019-Assessing}. 
We also report the zero-shot performance of LLMs on these datasets, {and compute \emph{accuracy} for ARC and \emph{exact match} for other datasets.} 

\textbullet~\emph{Complex reasoning.} %
For complex reasoning, we evaluate the comparison models on OpenbookQA~\cite{Mihaylov-EMNLP-2018-Can}, HellaSwag~\cite{Zellers-acl-2019-HellaSwag}, and SocialIQA~\cite{Sap-arxiv-2019-SocialIQA} for knowledge reasoning; Colored Objects~\cite{Srivastava-arxiv-2022-Beyond} and Penguins in the Table~\cite{Srivastava-arxiv-2022-Beyond} for symbolic reasoning; GSM8k~\cite{Cobbe-arxiv-2021-Training} and MATH~\cite{Hendrycks-ICLR-2021-Measuring} for mathematical reasoning. We compute the \emph{accuracy} for OpenbookQA, HellaSwag, and SocialIQA; \emph{solve rate} for Colored Objects and Penguins in the Table; and \emph{accuracy} for GSM8k and MATH. 
{For knowledge reasoning tasks, we evaluate the zero-shot performance, since they are all QA tasks that can be solved in a zero-shot setting.
For complex symbolic reasoning and mathematical reasoning tasks, we leverage 3-shot in-context exemplars to better elicit LLMs to accomplish them.
Following existing work~\cite{Gao-arxiv-2022-PAL,Wei-arxiv-2022-chain}, we also utilize the chain-of-thought prompting strategy for better solving the mathematical reasoning tasks.}

\textbullet~\emph{Human alignment.} For human alignment, %
we select TruthfulQA~\cite{Lin-ACL-2022-TruthfulQA} to measure whether a LLM is truthful in generating answers to questions, CrowS-Pairs~\cite{Nangia-EMNLP-2020-CrowS} and WinoGender~\cite{Rudinger-NAACL-2018-Gender} to assess the stereotypes in LLMs, RealToxityPrompts~\cite{Gehman-2023-arxiv-RealToxicityPrompts} to evaluate the extent to which LLMs generate toxic language, and HaluEval~\cite{Li-arxiv-2023-HaluEval} to test the ability of LLMs to recognize hallucination.
As the test set of Real-Toxicity-Prompts is too large, we randomly sample 10000 examples from it for evaluation.
We follow LLaMA~\cite{Touvron-arxiv-2023-LLaMA} to report the zero-shot performance, and compute the \emph{accuracy} of identifying a claim as true for TruthfulQA, \emph{accuracy} of recognizing biased sentences (high perplexity) for CrowS-Pairs, \emph{coreference resolution accuracy (he/she/they)} for WinoGender, \emph{toxicity score} for RealToxityPrompts, and \emph{average accuracy} of recognizing hallucinations for HaluEval. 
{For TruthfulQA, we follow existing work~\cite{Touvron-arxiv-2023-LLaMA} that utilizes text-davinci-003 to replace humans for scoring.}
For Crows-Pairs and WinoGender, we follow the experimental settings of LLaMA~\cite{Touvron-arxiv-2023-LLaMA} to compute the perplexity and coreference resolution score.
For RealToxityPrompts, we utilize the  {Perspective-API\footnote{\url{https://perspectiveapi.com/}}} for toxicity evaluation.


\textbullet~\emph{Interaction with environment.} %
To test this ability, we select ALFWorld~\cite{Shridhar-2021-iclr-ALFWorld} and WebShop~\cite{Yao-2022-nips-WebShop} for evaluation, which simulate real-world scenarios such as household and e-commerce environments. 
We follow the setting of ReAct~\cite{Yao-2022-arXiv-react} that evaluate the 1-shot and 2-shot performance of LLMs on WebShop and ALFWorld respectively, and compute \emph{success rate} for ALFWorld and \emph{average score/success rate} for WebShop.
Further, we also follow ReAct~\cite{Yao-2022-arXiv-react} to reduce the length of the input prompt and utilize line break as the EOS token.


\textbullet~\emph{Tool manipulation.} %
For tool manipulation, we consider two kinds of tools including search engine and model interfaces. Therefore, we adopt two tool manipulation benchmarks, \ie HotpotQA~\cite{yang-2018-acl-HotpotQA} and Gorilla~\cite{Patil-2023-arxiv-Gorilla}. HotpotQA requires LLMs to use search engine to retrieve documents from the web, and Gorilla to invoke model APIs from three hubs of TorchHub, TensorHub and HuggingFace. We compute \emph{exact match} for HotpotQA and \emph{accuracy} for Gorilla.
For HotpotQA, we follow ReAct~\cite{Yao-2022-arXiv-react} to report the 3-shot performance.
For Gorilla, we follow the code released by its paper~\cite{Patil-2023-arxiv-Gorilla}, and evaluate the zero-shot performance.


\paratitle{Implementation Details.} For each task and dataset, we evaluate the compared LLMs using the same %
prompts and results parsing method provided by existing work  {(\ie TruthfulQA, HotPotQA, Gorilla, HaluEval)} or designed according to our empirical experience  {(\ie TriviaQA, Natural Questions, Web Questions, ARC, WikiFact, GSM8k, MATH, C-Objects, Penguins, LAMBADA, WMT'22, XSum, HumanEval, CrowS-Pairs, WinoGender, RealToxityPrompt).}
Specifically, all the experiments about closed-source models are based on invoking their official APIs, while for open-source models, we utilize their publicly available code and model parameters, and perform the inference on 8 A800-80G GPUs.
For TriviaQA, OpenbookQA, HellaSwag, and SocialIQA, we experiment on the development set since the test set is not publicly released. While for other datasets, we experiment on the test set. %
To reproduce our experiments, we also publicly release our experimental code and data in \url{https://github.com/RUCAIBox/LLMSurvey/tree/main/Experiments}. 


\subsubsection{Results Analysis and Findings}
We report the experimental results in Table~\ref{tab-experimental-res}, and analyze the results in the following.

\paratitle{Analysis of Closed-Source Models.}
We summarize our analysis and findings of  {the four} closed-source models (\ie ChatGPT, Claude, Davinci003 and Davinci002) as follows:

$\bullet$ %
{\emph{These five closed-source models achieve promising results as general-purpose task solvers, in which ChatGPT mostly performs the best.} ChatGPT, Claude, Claude 2, Davinci003 and Davinci002 perform well in most of tasks, including complex tasks (\eg GSM8k), which have shown great potential to be general-purpose task solvers. %
Among them,  ChatGPT exhibits a more superior model capacity on the evaluation tasks, winning the most across all tasks.
In some evaluation tasks, the performance gap between ChatGPT and other closed-source models is very large, especially for  complex tasks \eg 78.47 (ChatGPT) \emph{v.s.} 49.96 (Davinci002) on GSM8k, and 79.88 (ChatGPT) \emph{v.s.}  51.22 (Claude) on HumanEval.
}


$\bullet$ \emph{{Claude 2, ChatGPT and Davinci003} perform better on interaction with environment and tool manipulation tasks.}
On the two evaluation tasks, Claude 2, ChatGPT and Davinci003, perform better than other models by a large margin, \eg 36.40 (Claude 2) \emph{v.s.} 26.00 (Davinci002) on HotpotQA, 44.53 (ChatGPT) \emph{v.s.} 7.74 (Claude) on Gorilla-TF, and 72.58 (Davinci003) \emph{v.s.} 22.04 (Claude) on Gorilla-TH. 
{A possible reason is that these three models have been specially optimized towards these advanced abilities, \eg supporting the use of external plugins.  }

%

$\bullet$ 
\emph{All the comparison models perform not well on very difficult reasoning tasks.} 
On MATH and HotpotQA, all models (including ChatGPT) perform not well. The two tasks are very difficult to solve, requiring accurate understanding of complex mathematical knowledge and performing multi-hop reasoning across documents, respectively.
{Further, these models also have a relatively weak performance on machine translation task (WMT). 
A possible reason is that  WMT also contains many  evaluation examples in minor languages, which might not be well covered in the pre-training data of these LLMs. 
}


\paratitle{Analysis of Open-Source Models.}
Next, we continue to show our analysis and findings about eight open-source models (\ie LLaMA 2-Chat, Vicuna, Alpaca, ChatGLM, LLaMA 2, LLaMA, Pythia and Falcon) as follows: 

$\bullet$ \emph{Instruction-tuned models mostly perform better than the base models.} 
{Among all the compared open-source methods, the instruction-tuned models (\ie LLaMA 2-Chat, Vicuna, Alpaca and ChatGLM) mostly perform better than  non-instruction-tuned models (\ie LLaMA 2, LLaMA, Pythia and Falcon).}
It indicates that instruction tuning is generally capable of improving  the few-shot or zero-shot ability of LLMs in solving various tasks. 
However, after instruction tuning, Vicuna (7B) and Alpaca (7B) suffer from performance degradations on LAMBADA, a language modeling task. 
{The reason may be that the instruction data mainly focuses on enabling LLMs to follow human instructions, which is not always useful  for the general language generation task. }

 
$\bullet$ \emph{These small-sized open-source models perform not well on {mathematical reasoning, interaction with environment, and tool manipulation tasks.}} 
On the tasks of mathematical reasoning, interaction with environment and tool manipulation, all these evaluated open-source models perform not well, including instruction-tuned ones. %
A possible reason is that the instruction data for fine-tuning these models is not specifically designed for these tasks. In addition, these closed-source models may have limited model capacities due to small model sizes.     

$\bullet$ \emph{The top-performing model varies on different human alignment tasks.}
For different human alignment tasks, we can see that these models achieve inconsistent performance rankings.  
For example, LLaMA 2-Chat (7B) performs the best among the compared open-source models on TruthfulQA, while Vicuna (13B) performs the best on CrowS-Pairs. A possible reason is that these tasks are designed with specific purposes for evaluating different aspects of human alignment, and these models exhibit varied performance on different tasks, even for the variants of the same model (\eg Pythia (7B) and Pythia (12B)). More experiments and analysis on human alignment evaluation are needed to reveal more detailed findings.  


%

$\bullet$ \emph{As a more recently released model, LLaMA 2 (7B) overall achieves a good  performance, especially on complex reasoning tasks.} 
{For complex reasoning tasks, LLaMA 2 (7B) mostly performs better than other base models, \eg 43.95 (LLaMA 2 (7B)) \emph{v.s.} 29.80 (Falcon (7B)) in C-Objects.
For other tasks (\eg language generation and knowledge utilization), LLaMA 2 (7B) can also achieve comparable performance as the best-performing base models. It has used more data for pre-training (\ie about 2 trillion tokens), which mainly contributes to the excellent performance. Furthermore, it also conducts a more robust data cleaning process.}



$\bullet$ \emph{Scaling the open-source modes can improve the performance consistently.} 
{By comparing the performance of Vicuna (7B) and Vicuna (13B), Pythia (7B) and Pythia (13B), we can see that the models with larger scales mostly perform better than smaller ones on these evaluation  tasks, indicating the effectiveness of scaling up the model size. 
Across different tasks, scaling model is more beneficial for more complex tasks (\eg symbolic and mathematical reasoning), where the larger models mostly outperform smaller ones in a large margin.
}


The readers should be note that these findings about open-source language models are  limited to the model sizes. We will continually update this part by including the results of larger versions of these models, and also call for the support of computational resources for more experiments. 