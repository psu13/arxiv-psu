



\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}



\begin{flushright}
\rightskip=0.8cm\textit{``The limits of my language mean the limits of my world.''} \\
\vspace{.2em}
\rightskip=.8cm---\emph{Ludwig Wittgenstein}
\end{flushright}
\vspace{1em}


\begin{figure*}[h]
    \centering
    \begin{minipage}{0.45\textwidth}
           \includegraphics[width=\textwidth]{images/paper_number1.pdf}
           \captionof*{figure}{(a) Query="Language Model"}
        \end{minipage}
        \qquad
        \begin{minipage}{0.45\textwidth}
           \includegraphics[width=\textwidth]{images/paper_number2.pdf}
           \captionof*{figure}{(b) Query="Large Language Model"}
    \end{minipage}
    \caption{{The trends of the  cumulative numbers  of arXiv papers that contain  the keyphrases  ``\emph{language model}" (since June 2018)  and ``\emph{large language model}'' (since October 2019), respectively. The statistics are calculated using   exact match by querying the keyphrases in title or abstract by months. We set different x-axis ranges for the two keyphrases, because ``language models'' have been explored at an earlier time.  We label the points corresponding to  important landmarks in the research progress of LLMs. A sharp increase occurs after the release of ChatGPT: the average number of published  arXiv papers that contain ``\emph{large language model}'' in title or abstract goes from 0.40 per day to 8.58 per day (Figure~\ref{fig:paper_number}(b)). }}
    \label{fig:paper_number}
\end{figure*}

\begin{figure*}
    \centering
\includegraphics[width=.9\textwidth]{images/task-solvers.pdf}
    \caption{An  evolution process of the four generations of language models~(LM) from the perspective of task solving capacity. Note that the time period for each stage may not be very accurate, and we set the time mainly according to the publish date of the most  representative  studies at each stage. For neural language models, we abbreviate the paper titles of two representative studies to name the two approaches: NPLM~\cite{Bengio-JMLR-2003-A}   (``\emph{A neural probabilistic language model}'') and NLPS~\cite{Collobert-JMLR-2011}    (``\emph{Natural language processing (almost) from scratch}''). Due to the space limitation, we don't list all representative studies in this figure. } \label{fig:task_solvers}
\end{figure*}

\IEEEPARstart{L}{anguage}  is a prominent ability in human beings to express and communicate, which develops in early childhood and evolves over a lifetime~\cite{instinct-book,hauser-science-2002-faculty}.  Machines, however, cannot naturally grasp the abilities of understanding and communicating in the form of human language, unless equipped with powerful artificial intelligence~(AI) algorithms. It has been a longstanding research challenge to achieve this goal, to enable machines to read, write, and communicate like humans~\cite{turing-test}. 


Technically, \emph{language modeling~(LM)} is one of the major approaches to advancing language intelligence of machines.  
In general, LM aims to model  the generative likelihood of word sequences, so as to predict the probabilities of future (or missing) tokens. 
The research of LM has received extensive attention in the literature, which can be divided into four major development stages:   


$\bullet$ \emph{Statistical language models~(SLM)}.  SLMs~\cite{NLP-speech-book,SLM-2004,rosenfeld2000two,stolcke2002srilm} are developed based on  \emph{statistical learning} methods that rose in the 1990s. The basic idea is to build the word prediction model based on the Markov assumption, \eg predicting the next word based on the most recent context. The SLMs with a fixed context length $n$ are also called $n$-gram language models, \eg   bigram and trigram language models. SLMs have been widely applied to enhance task performance in information retrieval~(IR)~\cite{SLM-IR1,SLM-IR2} and natural language processing~(NLP)~\cite{Thede-acl-1999-a,bahl1989tree,Brants-emnlp-2007-large}.  
However, they often suffer from the curse of dimensionality: it is difficult to accurately estimate high-order language models since an exponential number of transition probabilities need to be  estimated.
{Thus}, specially designed smoothing strategies  such as back-off estimation~\cite{Katz-IEEE-1987-estimation} and Goodâ€“Turing estimation~\cite{Gale-JQL-1995-good} have been introduced {to alleviate the data sparsity problem. }


$\bullet$ \emph{Neural language models~(NLM)}. NLMs~\cite{Bengio-JMLR-2003-A,Mikolov-INTERSPEECH-2010,Kombrink-INTERSPEECH-2011} characterize the probability of  word sequences by neural networks, \eg multi-layer perceptron~(MLP) and recurrent neural networks~(RNNs).   
As a remarkable contribution, the work in \cite{Bengio-JMLR-2003-A} introduced the concept of \emph{distributed representation} of words and built the word prediction function conditioned on the aggregated context features (\ie the distributed word vectors).  
By extending the idea of learning effective features for text data, a general neural network approach was developed to build a unified, end-to-end  solution for various NLP tasks~\cite{Collobert-JMLR-2011}. Furthermore, word2vec~\cite{Mikolov-NIPS-2013,Mikolov-ICLR-2013} was proposed to build a simplified shallow neural network for learning distributed word representations, which were demonstrated to be very effective across a variety of NLP tasks. 
These studies have initiated the use of language models for representation learning (beyond word sequence modeling),  having  an important impact on the field of NLP.  
 

 

  
$\bullet$ \emph{Pre-trained language models~(PLM)}. As an early attempt,  ELMo~\cite{Peters-NAACL-2018} was proposed to  capture  context-aware word representations by first pre-training a bidirectional LSTM~(biLSTM)  network ({instead of learning fixed word representations}) and then fine-tuning the biLSTM network according to specific downstream tasks.  Furthermore, based on the highly parallelizable Transformer 
architecture~\cite{Vaswani-NIPS-2017-Attention} with self-attention mechanisms, 
BERT~\cite{Devlin-NAACL-2019-BERT} was proposed by  pre-training  bidirectional language models with specially designed pre-training tasks on large-scale unlabeled corpora.  These pre-trained context-aware word representations are very effective as general-purpose semantic features, which have largely raised the performance bar of NLP tasks. This study has inspired a large number of follow-up work, which sets the ``\emph{pre-training} and \emph{fine-tuning}'' learning paradigm. 
Following this paradigm, a great number of studies on PLMs have been developed, introducing either different architectures~\cite{Lewis-ACL-2020-BART,Fedus-JMLR-2021-Switch} (\eg GPT-2~\cite{radford-blog-2019-language} and BART~\cite{Lewis-ACL-2020-BART}) or improved pre-training strategies~\cite{Liu-CoRR-2019-RoBERTa,Sanh-ICLR-2022-Multitask,Wang-ICML-2022-What}. In this paradigm, it often requires fine-tuning the PLM for adapting to different downstream tasks.  

$\bullet$ \emph{Large language models~(LLM)}. Researchers find that scaling PLM (\eg scaling model size or data size)  often leads to an improved model capacity on downstream tasks (\ie following the scaling law~\cite{Kaplan-arxiv-2020-Scaling}). A number of studies have explored the performance limit by training an ever larger PLM (\eg the 175B-parameter GPT-3 and the 540B-parameter PaLM). Although scaling is mainly conducted in model size (with similar architectures and pre-training tasks), these large-sized PLMs display different behaviors from smaller PLMs (\eg 330M-parameter BERT and 1.5B-parameter GPT-2) and show surprising abilities (called \emph{emergent abilities}~\cite{Wei-arxiv-2022-Emergent}) in solving a series of complex tasks. 
For example, GPT-3 can  solve few-shot tasks through \emph{in-context learning}, whereas GPT-2 cannot do well. 
Thus, the research community coins the  term  ``\emph{large language models~(LLM)}''\footnote{Note that a LLM is not necessarily more capable than a small PLM, and  emergent abilities may not  occur in some LLMs. } for these large-sized PLMs~\cite{Shanahan-arxiv-2022-Talking,Wei-arxiv-2022-chain,Hoffmann-arxiv-2022-Training,Taylor-arxiv-2022-Galactica}, which attract  increasing   research attention (See Figure~\ref{fig:paper_number}). 
A remarkable application of LLMs is \emph{ChatGPT}\footnote{https://openai.com/blog/chatgpt/}  that adapts the LLMs from the GPT series for dialogue, which presents an amazing conversation ability with humans. We can observe a sharp increase of the  arXiv papers that are related to LLMs after the release of ChatGPT in Figure~\ref{fig:paper_number}.

{
As discussed before, language model is not a new technical concept specially for LLMs, but has evolved with the advance of artificial intelligence over the decades. Early language models mainly aim to model and generate text data, while latest language models (\eg GPT-4) focus on  {complex task solving}. From \emph{language modeling} to \emph{task solving}, it is an important leap in scientific thinking, which is the key to understand the development of   language models in the research history. 
From the perspective of task solving, the four generations of language models have exhibited different levels of model capacities.  
  In Figure~\ref{fig:task_solvers}, we describe the  evolution  process  of language models in terms of the task solving capacity.
At first, statistical language models mainly assisted in some specific tasks (\eg retrieval or speech tasks), in which the predicted or estimated probabilities can enhance the performance of task-specific approaches.
Subsequently, neural language models focused on learning task-agnostic representations (\eg features), aiming  to reduce the efforts for human feature engineering. Furthermore, pre-trained language models  learned context-aware representations that can be optimized according to downstream  tasks. For the latest generation of language model, LLMs are enhanced by exploring the scaling effect on model capacity, which can be considered as   general-purpose task solvers. To summarize, in the evolution process, the task scope that can be solved by language models have been greatly extended, and the task performance attained by language models have been significantly enhanced. 
}

In the existing literature, PLMs have been widely discussed and surveyed~\cite{Liu-survey-2023-Pre-train,Zhou-arxiv-2023-A,Han-AIopen-2021-PTM,qiu-CoRR-2020-PTM}, while LLMs are seldom reviewed in a systematic way. To  motivate our survey, we first highlight three major differences between LLMs and PLMs. 
First, LLMs display some surprising emergent abilities that may not be observed in previous smaller  PLMs. These abilities are key to the performance of language models on complex tasks, making AI algorithms unprecedently powerful and effective.   
Second, LLMs would  revolutionize  the way that humans  develop and use AI algorithms.    
Unlike small PLMs, the major approach to accessing LLMs is  through the prompting interface (\eg GPT-4 API). Humans have to understand how LLMs work and format their tasks in a way that LLMs can follow.   
Third, the development of LLMs no longer draws a clear distinction between research and engineering. The training of LLMs requires extensive  practical experiences in large-scale data processing and distributed parallel training.  
To develop capable LLMs, researchers have to solve complicated engineering issues, working with engineers or being engineers. 

Nowadays,  LLMs are posing a significant impact on the AI community, and the advent of ChatGPT and GPT-4 leads to the rethinking of the possibilities of artificial general intelligence~(AGI). OpenAI has published a technical article entitled ``\emph{Planning for AGI and beyond}'', which discusses the short-term and long-term plans to approach  AGI~\cite{OpenAI-blog-2023-Planning}, and a more recent paper has argued that GPT-4 might be considered as an early version of an AGI system~\cite{Bubeck-arxiv-2023-Sparks}. 
The research areas of AI are being revolutionized by the rapid progress of LLMs. 
In the field of NLP,  LLMs can serve as a general-purpose  language task solver (to some extent), and  the research paradigm has been shifting towards the use of LLMs. 
In the field of IR, traditional search engines are challenged by the new information seeking way through AI chatbots (\ie ChatGPT), and  \emph{New Bing}\footnote{https://www.bing.com/new} presents an initial attempt that enhances  the search results based on LLMs. 
In the field of  CV, the researchers  try to develop ChatGPT-like vision-language models that can better serve multimodal dialogues~\cite{Huang-CoRR-2023,Cao-arxiv-2023-comprehensive, driess-arxiv-2023-palm,wu-arxiv-2023-visual}, and  GPT-4~\cite{OpenAI-OpenAI-2023-GPT-4} has  supported multimodal input by integrating the visual information.  
This new wave of technology would potentially lead to a prosperous
 ecosystem of real-world applications based on LLMs. 
For instance, Microsoft 365  is being empowered by  LLMs (\ie Copilot) to automate the office work, and OpenAI supports the use of   plugins in ChatGPT for implementing special functions.  
 

Despite the  progress and impact,  the underlying principles of LLMs are still not well explored. Firstly, it is  mysterious   why emergent abilities occur in  LLMs,   instead of smaller PLMs. As a more general issue, there  lacks a deep, detailed investigation of the key factors that contribute to the superior abilities of LLMs. 
It is important to study when and how LLMs obtain such abilities~\cite{FU-blog-2022-how}. Although there are some meaningful discussions about this problem~\cite{Wei-arxiv-2022-Emergent,FU-blog-2022-how}, more principled investigations are needed to uncover the ``\emph{secrets}`` of LLMs. 
Secondly, it is difficult for the research community to train capable LLMs.  
Due to the huge demand of computation resources, it is very costly to carry out repetitive, ablating studies for investigating the effect of various strategies for training LLMs. 
Indeed,  LLMs are mainly trained by industry, where many important training details (\eg data collection and cleaning) are not revealed to the public. 
Thirdly, it is  challenging to align LLMs with human values or preferences. Despite the capacities, LLMs are also likely to produce toxic, fictitious, or harmful contents. It requires effective and efficient control approaches  to  eliminating the potential risk of the use of LLMs~\cite{OpenAI-OpenAI-2023-GPT-4}.  


Faced with both opportunities and challenges, it needs more  attention on the research and development of LLMs. 
In order to provide a basic understanding of LLMs,  
this survey conducts a literature review of the recent advances in LLMs 
from four major aspects, including \emph{pre-training} (how to pre-train a capable LLM), \emph{adaptation} (how to effectively adapt pre-trained LLMs for better use), \emph{utilization} (how to use LLMs for solving various downstream tasks) and   
\emph{capability evaluation} (how to evaluate the abilities of LLMs and existing empirical findings).  
We thoroughly comb the literature and summarize the key findings, techniques, and methods of LLMs.  
For this survey, we also 
create a GitHub project website by collecting the supporting resources for LLMs, at the link \url{https://github.com/RUCAIBox/LLMSurvey}.  
We are also aware of several related review articles on PLMs or LLMs~\cite{Han-AIopen-2021-PTM,qiu-CoRR-2020-PTM,Li-IJCAI-2021-Pretrained,Liu-survey-2023-Pre-train,Lu-arxiv-2022-Survey,Dong-arxiv-2023-A,Shanahan-arxiv-2022-Talking,Huang-arxiv-2022-Towards,Qiao-arxiv-2022-Reasoning,Cao-arxiv-2023-comprehensive,Zhou-FITEE-2023-ChatGPT,Zhao-arxiv-2022-Dense}. These papers either  discuss PLMs or some specific (or general) aspects of LLMs. 
Compared with them, we focus on the techniques and methods to develop and use LLMs and provide a relatively comprehensive reference to important aspects  of LLMs. 


The remainder of this survey is organized as follows: Section 2 introduces the background for LLMs and the evolution of GPT-series models, followed by the summarization of available resources for developing LLMs in Section 3. Sections 4, 5, 6, and 7 review and summarize the recent progress from the four aspects of pre-training, adaptation, utilization, and capacity evaluation, respectively. 
Then, Section 8 discusses the practical guide for prompt design, and Section 9 reviews the applications of LLMs in several representative domains. 
Finally, we conclude the survey in Section 10 by summarizing the major findings and discuss the remaining issues for future work. 
