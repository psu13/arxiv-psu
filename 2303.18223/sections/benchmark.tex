\begin{table*}[htbp]
    \centering  
    \caption{{A category of existing evaluation work. ``General'' denotes that the evaluation focuses on an overall performance of multiple abilities. The evaluated abilities are not limited to the representative basic and advanced abilities mentioned in Section~\ref{sec:basicability} and \ref{sec:superior}.}}
    \label{tab-category-evaluation}
    \footnotesize
    \begin{tabular}{ccccc}
    \toprule
    \textbf{Method} & \textbf{Evaluation} & \textbf{Model Types} & \textbf{Abilities/Domain} & \textbf{Data Source}\\
    \midrule
    \multirow{28}{*}{Benchmark} & MMLU~\cite{Hendrycks-ICLR-2021-Measuring} & Base/Fine-tuned/Specialized & General & Human exam/practice \\
    & BIG-bench~\cite{Srivastava-arxiv-2022-Beyond} & Base/Fine-tuned/Specialized & General & Human annotation \\
    & HELM~\cite{Liang-arxiv-2022-Holistic} & Base/Fine-tuned/Specialized & General & Benchmark collection \\
    & Open LLM Leaderboard~\cite{Edward-2023-hf-open} & Base/Fine-tuned/Specialized & General & Benchmark collection \\
    & AGIEval~\cite{Zhong-2023-arxiv-AGIEval} & Base/Fine-tuned/Specialized & General & Human exam/practice \\
    & MMCU~\cite{Zeng-arxiv-2023-MMCU} & Base/Fine-tuned/Specialized & General & Human exam/practice \\
    & M3KE~\cite{Liu-2023-arxiv-M3KE} & Base/Fine-tuned/Specialized & General & Human exam/practice \\
    & C-Eval~\cite{Huang-arxiv-2023-CEval} & Base/Fine-tuned/Specialized & General & Human exam/practice \\
    & Xiezhi~\cite{Gu-2023-arxiv-Xiezhi} & Base/Fine-tuned/Specialized & General & Human exam/practice \\
    & OpenCompass~\cite{2023opencompass} & Base/Fine-tuned/Specialized & General & Benchmark collection \\
    & Chain-of-Thought Hub~\cite{Fu-arxiv-2023-Chain} & Base/Fine-tuned & General & Benchmark collection \\
    & KoLA~\cite{Yu-arxiv-2023-KoLA} & Base/Fine-tuned & Knowledge utilization & Web \\
    & ARB~\cite{Sawada-arxiv-2023-ARB} & Fine-tuned & Complex reasoning & Human exam/practice \\
    & APIBench~\cite{Peng-arxiv-2023-Revisiting} & Base/Fine-tuned & Tool manipulation & Web \\
    & APIBank~\cite{Li-arxiv-2023-API-Bank} & Fine-tuned & Tool manipulation & Synthesis \\
    & ToolAlpaca~\cite{Tang-arxiv-2023-ToolAlpaca} & Base/Fine-tuned & Tool manipulation & Synthesis \\
    & T-Bench~\cite{Xu-arxiv-2023-On} & Fine-tuned & Tool manipulation & Synthesis \\
    & ToolBench~\cite{Qin-arxiv-2023-ToolLLM} & Fine-tuned & Tool manipulation & Synthesis \\
    & BOLAA~\cite{Liu-arxiv-2023-BOLAA} & Base/Fine-tuned &  Environment interaction & Benchmark collection \\
    & AgentBench~\cite{Liu-arxiv-2023-AgentBench} & Base/Fine-tuned & Environment interaction & Human annotation/Synthesis \\
    & HaluEval~\cite{Li-arxiv-2023-HaluEval} & Base/Fine-tuned & Human alignment & Human annotation/Synthesis \\
    & PromptBench~\cite{Zhu-arxiv-2023-PromptBench} & Base/Fine-tuned & Robustness & Benchmark collection \\
    & HumanEval~\cite{Chen-arxiv-2021-evaluating} & Base/Fine-tuned/Specialized & Code synthesis & Human annotation \\
    & MultiMedQA~\cite{singhal-arxiv-2022-large} & Specialized & Healthcare & Benchmark collection \\
    & FLUE~\cite{Shah-arxiv-2023-FLUE} & Specialized & Finance & Benchmark collection \\
    & LegalBench~\cite{Guha-arxiv-2022-LegalBench} & Specialized & Legal & Human annotation \\
    \midrule
    \multirow{2}{*}{Human} & Chatbot Arena~\cite{Zheng-2023-arxiv-Judging} & Base/Fine-tuned/Specialized & Human Alignment & Human annotation \\
    & SciBench~\cite{Wang-arxiv-2023-SciBench} & Fine-tuned & Complex reasoning & Human exam/practice \\
    \midrule
    \multirow{5}{*}{Model} & AlpacaEval~\cite{Li-2023-github-alpaca_eval} & Fine-tuned & Instruction following & Synthesis \\
    & MT-bench~\cite{Zheng-2023-arxiv-Judging} & Fine-tuned & Human alignment & Human annotation \\
    & TrustGPT~\cite{Huang-arxiv-2023-TrustGPT} & Base/Fine-tuned & Human alignment & Benchmark collection \\
    & LMExamQA~\cite{Bai-arxiv-2023-Benchmarking} & Base/Fine-tuned & Knowledge utilization & Synthesis \\
    & ChatEval~\cite{Chan-arixiv-2023-ChatEval} & Base/Fine-tuned & Knowledge utilization & Benchmark collection \\
    \bottomrule
    \end{tabular}
\label{tab:benchmark}
\end{table*}

\subsection{Benchmarks and Evaluation Approaches}
In the above, we have discussed the basic and advanced abilities of LLMs.
Next, we will introduce existing evaluation benchmarks and approaches~\cite{Chang-2023-arxiv-A,Zhuang-2023-arxiv-Through}.

\subsubsection{Comprehensive Evaluation Benchmarks}
Recently, several comprehensive benchmarks~\cite{Hendrycks-ICLR-2021-Measuring,Srivastava-arxiv-2022-Beyond,Liang-arxiv-2022-Holistic} have been released for the evaluation of LLMs.
In this part, we introduce several widely used benchmarks, \ie MMLU, BIG-bench, HELM, and a series of human exam benchmarks.

$\bullet$ \emph{MMLU}~\cite{Hendrycks-ICLR-2021-Measuring} is a versatile benchmark for large-scale evaluation of multi-task knowledge understanding, covering a wide range of knowledge domains {from mathematics and computer science to humanities and social sciences. The difficulties of these tasks vary from basic to advanced.} 
{As shown in existing work, LLMs mostly outperform small models by a substantial margin on this benchmark~\cite{Chowdhery-arxiv-2022-PaLM,Chung-arxiv-2022-Scaling,Taylor-arxiv-2022-Galactica,Touvron-arxiv-2023-LLaMA}, which shows the scaling law in model size. %
{More recently, GPT-4 achieves a remarkable record (86.4\% in 5-shot setting) in MMLU, which is significantly  better than the previous state-of-the-art models~\cite{OpenAI-OpenAI-2023-GPT-4}.}}

$\bullet$ \emph{BIG-bench}~\cite{Srivastava-arxiv-2022-Beyond} is a collaborative benchmark intended to probe existing LLMs from various aspects.
{It comprises 204 tasks that encompass a broad range of topics, including linguistics, childhood development, mathematics, commonsense reasoning, biology, physics, social bias, software development, and so on.}
{By scaling  the model size, LLMs can even outperform the average human performance under the few-shot setting on 65\% of tasks in BIG-bench~\cite{Chowdhery-arxiv-2022-PaLM}.}
{Considering the high evaluation cost of the entire benchmark,  a lightweight benchmark BIG-bench-Lite has been proposed, which contains  24 small yet diverse and challenging tasks from BIG-bench.}
Additionally, the BIG-bench hard (BBH) benchmark~\cite{Suzgun-arxiv-2022-Challenging} has been proposed to concentrate on investigating the currently unsolvable tasks of LLMs by selecting the challenging tasks in which LLMs exhibit inferior performance compared to humans.
Since BBH becomes more difficult, small models mostly achieve performance close to random.  
{As a comparison, CoT prompting can elicit the abilities of LLMs to perform step-by-step reasoning for enhancing the performance, even exceeding the average human performance in BBH.}

$\bullet$ \emph{HELM}~\cite{Liang-arxiv-2022-Holistic} is a comprehensive benchmark that currently implements a core set of 16 scenarios and 7 categories of metrics. It is built on top of many prior studies, conducting a holistic evaluation of language models. 
As shown in the experimental results of HELM, instruction tuning can consistently boost the performance of LLMs in terms of accuracy, robustness, and fairness. Further, for reasoning tasks, the LLMs that have been pre-trained on the code corpus show superior performance.

%

$\bullet$ \emph{Human-level test benchmarks} aim to evaluate the comprehensive  ability of LLMs with questions designed for testing humans, such as AGIEval~\cite{Zhong-2023-arxiv-AGIEval}, MMCU~\cite{Zeng-arxiv-2023-MMCU}, M3KE~\cite{Liu-2023-arxiv-M3KE},  C-Eval~\cite{Huang-arxiv-2023-CEval} and {Xiezhi}~\cite{Gu-2023-arxiv-Xiezhi}.
These benchmarks encompass a wide range of domains, difficulty levels, and languages to provide a comprehensive evaluation of LLMs' general capabilities.
Compared to publicly available models, models offering API services (\eg  GPT-4, ChatGPT, Claude) demonstrate superior performance compared to publicly available models on these evaluation benchmarks.
As the best-performing model in evaluations, GPT-4 surpasses average human performance in AGIEval~\cite{Zhong-2023-arxiv-AGIEval}.
{However, it still lags behind the top human performance on these challenging benchmarks.} 
Hence, there remains ample room for further enhancements in the overall  abilities of LLMs, particularly for publicly accessible models. 

The above benchmarks cover a variety of mainstream evaluation tasks and real-world human exam questions for the evaluation of LLMs. 
Also, there are several benchmarks that focus on evaluating specific abilities of LLMs, such as TyDiQA~\cite{Clark-trans-2020-TyDi} for multilingual knowledge utilization and MGSM~\cite{Shi-arxiv-2022-Language} for multilingual mathematical reasoning. To conduct the  evaluation, one can select suitable benchmarks according to specific goals.
{In addition, there are also several open-source evaluation frameworks for researchers to evaluate LLMs on existing benchmarks or extend new tasks for customized evaluations, such as Language Model Evaluation Harness~\cite{Leo-zenodo-2021-A} and OpenAI Evals~\cite{OpenAI-OpenAI-2023-GPT-4}.}
{Further, some researchers
also construct continuously updated leaderboards by aggregating representative benchmarks, to compare the performance of existing LLMs, such as Open LLM Leaderboard~\cite{Edward-2023-hf-open}.
The above benchmarks and leaderboards provide important references to demonstrate the basic and advanced abilities of LLMs. We will give more deep discussions on pros and cons on evaluation approaches in Section~\ref{subsec-evaapp}. 
}

\subsubsection{Evaluation Approaches}\label{subsec-evaapp}
After introducing existing benchmarks, in this part, we will review existing evaluation approaches for assessing the performance of LLMs.
To organize our discussion, we categorize LLMs into three different types: \emph{base LLMs} (pre-trained model checkpoints), \emph{fine-tuned LLMs} (instruction or alignment fine-tuned model checkpoints), and \emph{specialized LLMs} (adapted model checkpoints for some specific task or domain).
Here, we keep both fine-tuned LLMs and specialized LLMs, to distinguish the different purposes of LLMs: general or specific  task solvers.
To evaluate the three types of LLMs, we can test the LLM's performance related to  different abilities (\eg basic or advanced abilities as discussed in Section~\ref{sec:basicability} and \ref{sec:superior}).
In general, there are three main approaches to evaluating LLMs, namely benchmark-based approach~\cite{Hendrycks-ICLR-2021-Measuring}, human-based approach~\cite{Zheng-2023-arxiv-Judging}, and model-based approach~\cite{Li-2023-github-alpaca_eval}.
Table~\ref{tab:benchmark} shows an illustration of the relationship among LLM type, evaluation approach, and tested abilities. 
Next, we will discuss the evaluation approaches for different types of LLMs.

\paratitle{Evaluation of Base LLMs.}
Base LLMs refer to the model checkpoints obtained right after pre-training.
For base LLMs, we mainly focus on examining the basic abilities (Section~\ref{sec:basicability}), such as complex reasoning and knowledge utilization.
Since most of these basic abilities can be assessed with well-defined tasks, benchmark-based approaches have been widely used to evaluate base LLMs.
Next, we will introduce common evaluation benchmarks and evaluation procedures for base LLMs.

$\bullet$~\emph{Common benchmarks.}
To evaluate base LLMs, typical benchmarks are designed in the form of close-ended problems like multiple-choice questions.
These commonly used benchmarks can be mainly divided into two categories: knowledge-oriented and reasoning-oriented benchmarks.
Knowledge-oriented benchmarks (\eg MMLU~\cite{Hendrycks-ICLR-2021-Measuring} and C-Eval~\cite{Huang-arxiv-2023-CEval}) aim to evaluate the capacity of world knowledge, while reasoning-oriented benchmarks (\eg GSM8K~\cite{Gao-arxiv-2023-Human}, BBH~\cite{Suzgun-arxiv-2022-Challenging}, and MATH~\cite{Hendrycks-ICLR-2021-Measuring}) focus on evaluating the capability of solving complex reasoning tasks.
Further, some recently proposed benchmarks (\eg OpenCompass~\cite{2023opencompass}) combine these two types for a comprehensive comparison.

$\bullet$~\emph{Benchmark based evaluation procedure.}
To perform the benchmark evaluation, each problem will first be formatted into a prompt for LLMs to generate the result text.
Then, the generated result text will be parsed with human-written rules to get the predicted answer.
Finally, the performance of LLMs can be automatically calculated using standard metrics like accuracy by comparing the predicted answer with the ground-truth one.
The evaluation approach can be conducted in either the few-shot or zero-shot setting, which might lead to different evaluation results or rankings.
Since base LLMs have not been instruction fine-tuned (with relatively weak task generalization ability), the few-shot setting is often more suitable for evaluation.
For some complex reasoning tasks, CoT prompts also need to be used to fully exhibit the capacity during evaluation.
Another note is that this evaluation approach can also be applied to assess the abilities of fine-tuned LLMs.
Actually, several leaderboards (\eg Open LLM Leaderboard~\cite{Edward-2023-hf-open}) are built upon this approach, evaluating both base and fine-tuned LLMs.

\paratitle{Evaluation of Fine-tuned LLMs.} 
Fine-tuned LLMs in this part refer to the model checkpoints obtained after instruction tuning or alignment tuning based on pre-trained model weights\footnote{In some cases, it is also called \emph{chat models}.}.
Typically, fine-tuned LLMs will be tested on various abilities (\eg knowledge utilization and human alignment), and thus it is common that they are assessed with multiple evaluation approaches.
In addition to benchmark-based evaluation, human-based and model-based approaches have also been widely used to evaluate the advanced abilities of fine-tuned LLMs.
Next, we will introduce the two evaluation methods.

$\bullet$~\emph{Human-based evaluation.}
Unlike automatic evaluation for basic abilities, human evaluation typically considers more factors or abilities  in real-world use, such as human alignment and tool manipulation.
In this evaluation approach, test tasks are usually in the form of open-ended questions, and human evaluators are invited to  make judgments on the quality of answers generated by LLMs.
Typically, there are two main types of scoring methods for human evaluators: pairwise comparison and single-answer grading.
In pairwise comparison, given the same question, humans are assigned two answers from different models to determine which one is better, while in single-answer grading, they only need to score a single answer at a time.
For example, HELM~\cite{Liang-arxiv-2022-Holistic} employs humans to perform single-answer grading on summarization and disinformation tasks, while Chatbot Arena~\cite{Zheng-2023-arxiv-Judging} constructs a crowdsourcing platform that allows users to engage in conversations with two anonymous chat LLMs and report pairwise comparison results.

$\bullet$~\emph{Model-based evaluation.}
Since human-based evaluation is both expensive and time-consuming, some work has proposed leveraging powerful closed-source LLMs such as ChatGPT and GPT-4 as a surrogate for human evaluators~\cite{Zheng-2023-arxiv-Judging, Li-2023-github-alpaca_eval}.
For example, AlpacaEval~\cite{Li-2023-github-alpaca_eval} collects a set of instructions and utilizes a capable LLM (\eg GPT-4) as the judge to perform pair-wise comparisons against the reference outputs.
Furthermore, MT-bench~\cite{Zheng-2023-arxiv-Judging} collects a set of multi-turn questions for evaluation and improves the reliability of LLM-based evaluators through methods like ICL and CoT.
Compared with human evaluators, LLMs such as ChatGPT and GPT-4 can achieve high agreement with humans, in both small-scale handcrafted and large-scale crowdsourced evaluation tasks.
Despite this, these closed-source LLMs are limited in access and have the potential risk of data leakage.
To address this, recent work~\cite{Zheng-2023-arxiv-Judging} has explored fine-tuning open-source LLMs (\eg Vicuna~\cite{vicuna2023}) as model evaluators using scoring data from human evaluators, which has  narrowed the gap with powerful closed-source LLMs (\eg GPT-4).

%

\paratitle{Evaluation of Specialized LLMs.}
Specialized LLMs refer to the model checkpoints specially adapted to some domains or applications like healthcare~\cite{singhal-arxiv-2022-large} and finance~\cite{Shah-2022-EMNLP-When}.
As special task solvers, specialized LLMs will be tested not only on general abilities (\eg basic ability like complex reasoning and advanced ability like human alignment), but also on specific abilities related to their designated domains or applications.
For this purpose, one often needs to construct specific benchmarks tailored for the target domains or applications. 
Then, these domain-specific benchmarks can be combined with general benchmarks to conduct both comprehensive and targeted evaluation for specialized LLMs.
For example, MultiMedQA~\cite{singhal-arxiv-2022-large} is a specific benchmark in healthcare, which includes medical examinations and healthcare questions.
In this work~\cite{singhal-arxiv-2022-large}, MultiMedQA has been combined  with MMLU~\cite{Hendrycks-ICLR-2021-Measuring} to assess the performance of specialized LLMs for healthcare, such as Med-PaLM~\cite{singhal-arxiv-2022-large}.
Similarly, FLUE~\cite{Shah-2022-EMNLP-When} constructs a benchmark for finance, spanning from financial sentiment analysis to question answering.
It has been used collaboratively with BBH~\cite{Suzgun-arxiv-2022-Challenging} to evaluate finical LLMs like BloombergGPT~\cite{wu-arxiv-2023-bloomberggpt}.

\paratitle{Pros and Cons of Different Evaluation Approaches}. 
In the above, we have discussed different evaluation approaches to assess the abilities of LLMs.
Next, we simply analyze the pros and cons of each evaluation approach.  %

$\bullet$~\emph{Benchmark-based approach}.
This evaluation approach can leverage existing benchmarks for assessing the performance of LLMs.
The tasks involved in these benchmarks often contain sufficient test samples to measure the core abilities (\eg reasoning).
The whole evaluation procedure can be (almost) automatic, and it is convenient to carry out test experiments for various base LLMs, especially useful for monitoring the performance of model checkpoints during pre-training.
However, LLMs are often sensitive to the evaluation settings, including the question prompts, zero-shot or few-shot tests, and the answer parsing methods.
Thus, one should take possible influencing factors into consideration when conducting the evaluation experiments.
The evaluation results should be noted with the adopted evaluation settings.
Another issue is the data contamination~\cite{Chowdhery-arxiv-2022-PaLM,zhou-arxiv-2023-dont}, \ie the test data itself or relevant content has been contained in the pre-training corpora.
This phenomenon has become increasingly severe since more and more open data has been collected for developing LLMs.

$\bullet$~\emph{Human-based approach}. 
Human evaluation offers several advantages when assessing the capabilities of LLMs to solve real-world tasks.
One of the key benefits is its ability to directly reflect the actual abilities of LLMs.
Based on feedback and experiences from real users, human evaluation provides a more direct measure of LLMs' performance in real-world scenarios.
Further, it can conduct more flexible and diverse evaluation tasks based on human evaluators.
For instance, users can submit various queries and test the abilities of LLMs according to their own task cognition.
It allows for a deep understanding of the strengths and weaknesses of LLMs across different types of tasks and contexts.
However, human evaluation also has inherent limitations that could potentially affect its accuracy and consistency.
Factors such as personalized tastes and varying education levels among evaluators can introduce biases or even inconsistencies in the evaluation process.
In some cases, users' judgments are likely to be subjective, which may not reflect the true capabilities of the LLMs.
Moreover, conducting robust and reliable human evaluations often requires a large number of evaluators, which can be very expensive and time-consuming.
In addition, human evaluation is often not reproducible, making it infeasible to extend existing evaluation results or track the progress of LLMs.

$\bullet$~\emph{Model-based approach}.
As a surrogate for human-based approaches, model-based approaches serve to diminish the reliance on human involvement, and enable more efficient and scalable evaluation.
In addition, LLMs can provide meaningful explanations for the assigned rating scores, thereby enhancing the interpretability of evaluations.
Despite their scalability and explanability, model-based approaches have been found to suffer from several issues, including position, verbosity, and self-enhancement bias~\cite{Zheng-2023-arxiv-Judging}.
Specially, position bias (\ie the order to present the responses) refers to the fact that LLMs tend to assign high scores for the answers at specific positions over others, 
 verbosity bias means that LLMs favor verbose answers even if they are short in quality compared with shorter answers, and 
self-enhancement bias indicates that LLMs often overrate in their own generations. 
In addition, since LLMs have limited capacities in  solving complex reasoning problems, they cannot serve as qualified evaluators for some difficult tasks (\eg mathematical reasoning).
These limitations can be mitigated to some extent by specific prompt engineering and fine-tuning strategies~\cite{Zheng-2023-arxiv-Judging}.



To summarize, our categorization (Table~\ref{tab:benchmark}) of existing work on LLM evaluation is mainly based on two major dimensions, namely evaluation methodology and model type, which are further extended with the test abilities.
There are some recent work~\cite{Chang-2023-arxiv-A,Zhuang-2023-arxiv-Through} that also has discussed the categorization or taxonomies of existing work for LLM evaluation.