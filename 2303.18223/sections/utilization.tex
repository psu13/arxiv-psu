\section{Utilization}
\label{sec-utilization}

\begin{table*}[t]
\centering
\caption{Typical LLM utilization methods and their key points for ICL, CoT, and planning. Note that the key points only highlight the most important technical contribution.} 
\label{tab:utilization}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|l|l}
\toprule
\textbf{Approach}                              & \textbf{Representative Work}     & \textbf{Key Point}                                                              \\
\midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}In-context\\Learning~(ICL)\end{tabular}} & KATE~\cite{Liu-ACL-2022-What}         & Demonstration selection (similar; k-NN)                      \\
                                                                                    & EPR~\cite{Rubin-NAACL-2022-Learning}  & Demonstration selection (dense retrieval; constrative learning)    \\
                                                                                    & SG-ICL~\cite{Kim-2022-arxiv-Self}     & Demonstration selection (LLM as the demonstration generator) \\
                                                                                    & APE~\cite{Zhou-2023-ICLR-Large}       & Demonstration format (automatic generation \& selection)                         \\
                                                                                    & Structured Prompting~\cite{Hao-2022-arxiv-Structured} & Demonstration format (grouped context encoding; rescaled attention) \\
                                                                                    & GlobalE \& LocalE~\cite{Lu-ACL-2022-Fantasically}     & Demonstration order (entropy-based metric; probing set generation with LLM) \\
\midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}Chain-of-thought\\Prompting~(CoT)\end{tabular}}  & Complex CoT~\cite{Fu-arxiv-2022-Complexity}                       & Demonstration (complexity-based selection)                          \\
                                                                                            & Auto-CoT~\cite{Zhang-arxiv-2022-Automatic}                        & Demonstration (automatic generation)                             \\
                                                                                            & Selection-Inference~\cite{Creswell-2022-arXiv-selection}          & Generation (alternate between selection and inference)                      \\
                                                                                            & Self-consistency~\cite{Wang-arxiv-2022-Self-Consistency}          & Generation (diverse paths; self-ensemble)             \\
                                                                                            & DIVERSE~\cite{Li-arxiv-2022-On}                                   & Generation (diverse paths); Verification (step-wise voting)           \\
                                                                                            & Rationale-augmented ensembles~\cite{Wang-arxiv-2022-Rationale}    & Generation (rationale sampling) \\
\midrule
\multirow{13}{*}{Planning}            & Least-to-most prompting~\cite{Zhou-arxiv-2022-Least} & Plan generation (text-based; problem decomposition)                                                  \\
                                     & DECOMP~\cite{Khot-2022-arXiv-Decomposed}             & Plan generation (text-based; problem decomposition) \\
                                     & PS~\cite{Wang-arXiv-2023-Plan}                       & Plan generation (text-based) \\
                                     & Faithful CoT~\cite{Lyu-arxiv-2023-Faithful}          & Plan generation (code-based) \\
                                     & PAL~\cite{Gao-arxiv-2022-PAL}                        & Plan generation (code-based; Python) \\
                                     & HuggingGPT~\cite{Shen-2023-arXiv-Hugginggpt}         & Plan generation (code-based; models from HuggingFace) \\
                                     & AdaPlanner~\cite{Sun-2023-arXiv-adaplanner}          & Plan refinement (skill memory) \\
                                     & TIP~\cite{Lu-2023-arXiv-multimodal}                  & Feedback acquisition (visual perception) \\
                                     & RAP~\cite{Hao-2023-arXiv-reasoning}                  & Feedback acquisition (LLM as the world model); Plan refinement (Monte Carlo Tree Search) \\
                                     & ChatCoT~\cite{Chen-2023-arXiv-chatcot}               & Feedback acquisition (tool); Plan refinement (conversation between LLM and tools) \\
                                     & ReAct~\cite{Yao-2022-arXiv-react}                    & Feedback acquisition (tool); Plan refinement (synergizing reasoning and acting)                                       \\
                                     & Reflexion~\cite{Shinn-2023-arXiv-Reflexion}          & Feedback acquisition (text-based self-reflection); Plan refinement (dynamic memory)                   \\
                                     & Tree of Thoughts~\cite{Yao-arxiv-2023-Tree}          & Feedback acquisition (vote comparison); Plan refinement (tree-based search)                                                       \\
\bottomrule
\end{tabular}%
}
\end{table*}

After pre-training or adaptation tuning, 
a major approach to using LLMs is to design suitable \textit{prompting} strategies for solving various tasks. {In existing literature, task-specific prompts can be effectively learned through manual creation and automatic optimization.} 
{A representative prompting method is \textit{in-context learning}~\cite{Brown-NeurIPS-2020-Language, Dong-arxiv-2023-A}, which formulates the task description and/or demonstrations in the form of natural language text.}
In addition, \textit{chain-of-thought prompting}~\cite{Wei-arxiv-2022-chain} can be employed to enhance in-context learning by involving a series of intermediate reasoning steps in prompts.
Furthermore, \textit{planning}~\cite{Zhou-arxiv-2022-Least} is proposed for solving complex tasks, which first breaks them down into smaller sub-tasks and then generates a plan of action to solve these sub-tasks one by one.
We summarize representative work for these prompting approaches in Table~\ref{tab:utilization}.
Next, we will elaborate on the details of the four techniques.

\input{sections/prompt-design}
\input{sections/ICL.tex}
\input{sections/CoT.tex}
\input{sections/planning.tex}

