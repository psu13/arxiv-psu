% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{100}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Bengio-JMLR-2003-A}
Y.~Bengio, R.~Ducharme, P.~Vincent, and C.~Janvin, ``A neural probabilistic
  language model,'' \emph{J. Mach. Learn. Res.}, vol.~3, pp. 1137--1155, 2003.

\bibitem{Collobert-JMLR-2011}
R.~Collobert, J.~Weston, L.~Bottou, M.~Karlen, K.~Kavukcuoglu, and P.~P. Kuksa,
  ``Natural language processing (almost) from scratch,'' \emph{J. Mach. Learn.
  Res.}, vol.~12, pp. 2493--2537, 2011.

\bibitem{instinct-book}
S.~Pinker, \emph{The Language Instinct: How the Mind Creates Language}.\hskip
  1em plus 0.5em minus 0.4em\relax Brilliance Audio; Unabridged edition, 2014.

\bibitem{hauser-science-2002-faculty}
M.~D. Hauser, N.~Chomsky, and W.~T. Fitch, ``The faculty of language: what is
  it, who has it, and how did it evolve?'' \emph{science}, vol. 298, no. 5598,
  pp. 1569--1579, 2002.

\bibitem{turing-test}
A.~M. Turing, ``Computing machinery and intelligence,'' \emph{Mind}, vol.
  {LIX}, no. 236, pp. 433--460, 1950.

\bibitem{NLP-speech-book}
F.~Jelinek, \emph{Statistical Methods for Speech Recognition}.\hskip 1em plus
  0.5em minus 0.4em\relax MIT Press, 1998.

\bibitem{SLM-2004}
J.~Gao and C.~Lin, ``Introduction to the special issue on statistical language
  modeling,'' \emph{{ACM} Trans. Asian Lang. Inf. Process.}, vol.~3, no.~2, pp.
  87--93, 2004.

\bibitem{rosenfeld2000two}
R.~Rosenfeld, ``Two decades of statistical language modeling: Where do we go
  from here?'' \emph{Proceedings of the IEEE}, vol.~88, no.~8, pp. 1270--1278,
  2000.

\bibitem{stolcke2002srilm}
A.~Stolcke, ``Srilm-an extensible language modeling toolkit,'' in \emph{Seventh
  international conference on spoken language processing}, 2002.

\bibitem{SLM-IR1}
X.~Liu and W.~B. Croft, ``Statistical language modeling for information
  retrieval,'' \emph{Annu. Rev. Inf. Sci. Technol.}, vol.~39, no.~1, pp. 1--31,
  2005.

\bibitem{SLM-IR2}
C.~Zhai, \emph{Statistical Language Models for Information Retrieval}, ser.
  Synthesis Lectures on Human Language Technologies.\hskip 1em plus 0.5em minus
  0.4em\relax Morgan {\&} Claypool Publishers, 2008.

\bibitem{Thede-acl-1999-a}
S.~M. Thede and M.~P. Harper, ``A second-order hidden markov model for
  part-of-speech tagging,'' in \emph{27th Annual Meeting of the Association for
  Computational Linguistics, University of Maryland, College Park, Maryland,
  USA, 20-26 June 1999}, R.~Dale and K.~W. Church, Eds.\hskip 1em plus 0.5em
  minus 0.4em\relax {ACL}, 1999, pp. 175--182.

\bibitem{bahl1989tree}
L.~R. Bahl, P.~F. Brown, P.~V. de~Souza, and R.~L. Mercer, ``A tree-based
  statistical language model for natural language speech recognition,''
  \emph{IEEE Transactions on Acoustics, Speech, and Signal Processing},
  vol.~37, no.~7, pp. 1001--1008, 1989.

\bibitem{Brants-emnlp-2007-large}
T.~Brants, A.~C. Popat, P.~Xu, F.~J. Och, and J.~Dean, ``Large language models
  in machine translation,'' in \emph{EMNLP-CoNLL 2007, Proceedings of the 2007
  Joint Conference on Empirical Methods in Natural Language Processing and
  Computational Natural Language Learning, June 28-30, 2007, Prague, Czech
  Republic}, J.~Eisner, Ed.\hskip 1em plus 0.5em minus 0.4em\relax {ACL}, 2007,
  pp. 858--867.

\bibitem{Katz-IEEE-1987-estimation}
S.~M. Katz, ``Estimation of probabilities from sparse data for the language
  model component of a speech recognizer,'' \emph{{IEEE} Trans. Acoust. Speech
  Signal Process.}, vol.~35, no.~3, pp. 400--401, 1987.

\bibitem{Gale-JQL-1995-good}
W.~A. Gale and G.~Sampson, ``Good-turing frequency estimation without tears,''
  \emph{J. Quant. Linguistics}, vol.~2, no.~3, pp. 217--237, 1995.

\bibitem{Mikolov-INTERSPEECH-2010}
T.~Mikolov, M.~Karafi{\'{a}}t, L.~Burget, J.~Cernock{\'{y}}, and S.~Khudanpur,
  ``Recurrent neural network based language model,'' in \emph{{INTERSPEECH}
  2010, 11th Annual Conference of the International Speech Communication
  Association, Makuhari, Chiba, Japan, September 26-30, 2010}, T.~Kobayashi,
  K.~Hirose, and S.~Nakamura, Eds.\hskip 1em plus 0.5em minus 0.4em\relax
  {ISCA}, 2010, pp. 1045--1048.

\bibitem{Kombrink-INTERSPEECH-2011}
S.~Kombrink, T.~Mikolov, M.~Karafi{\'{a}}t, and L.~Burget, ``Recurrent neural
  network based language modeling in meeting recognition,'' in
  \emph{{INTERSPEECH} 2011, 12th Annual Conference of the International Speech
  Communication Association, Florence, Italy, August 27-31, 2011}.\hskip 1em
  plus 0.5em minus 0.4em\relax {ISCA}, 2011, pp. 2877--2880.

\bibitem{Mikolov-NIPS-2013}
T.~Mikolov, I.~Sutskever, K.~Chen, G.~S. Corrado, and J.~Dean, ``Distributed
  representations of words and phrases and their compositionality,'' in
  \emph{Advances in Neural Information Processing Systems 26: 27th Annual
  Conference on Neural Information Processing Systems 2013. Proceedings of a
  meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States}, C.~J.~C.
  Burges, L.~Bottou, Z.~Ghahramani, and K.~Q. Weinberger, Eds., 2013, pp.
  3111--3119.

\bibitem{Mikolov-ICLR-2013}
T.~Mikolov, K.~Chen, G.~Corrado, and J.~Dean, ``Efficient estimation of word
  representations in vector space,'' in \emph{1st International Conference on
  Learning Representations, {ICLR} 2013, Scottsdale, Arizona, USA, May 2-4,
  2013, Workshop Track Proceedings}, Y.~Bengio and Y.~LeCun, Eds., 2013.

\bibitem{Peters-NAACL-2018}
M.~E. Peters, M.~Neumann, M.~Iyyer, M.~Gardner, C.~Clark, K.~Lee, and
  L.~Zettlemoyer, ``Deep contextualized word representations,'' in
  \emph{Proceedings of the 2018 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies,
  {NAACL-HLT} 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long
  Papers)}, M.~A. Walker, H.~Ji, and A.~Stent, Eds.\hskip 1em plus 0.5em minus
  0.4em\relax Association for Computational Linguistics, 2018, pp. 2227--2237.

\bibitem{Vaswani-NIPS-2017-Attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in
  \emph{Advances in Neural Information Processing Systems 30: Annual Conference
  on Neural Information Processing Systems 2017, December 4-9, 2017, Long
  Beach, CA, {USA}}, 2017, pp. 5998--6008.

\bibitem{Devlin-NAACL-2019-BERT}
J.~Devlin, M.~Chang, K.~Lee, and K.~Toutanova, ``{BERT:} pre-training of deep
  bidirectional transformers for language understanding,'' in \emph{Proceedings
  of the 2019 Conference of the North American Chapter of the Association for
  Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2019,
  Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)},
  J.~Burstein, C.~Doran, and T.~Solorio, Eds.\hskip 1em plus 0.5em minus
  0.4em\relax Association for Computational Linguistics, 2019, pp. 4171--4186.

\bibitem{Lewis-ACL-2020-BART}
M.~Lewis, Y.~Liu, N.~Goyal, M.~Ghazvininejad, A.~Mohamed, O.~Levy, V.~Stoyanov,
  and L.~Zettlemoyer, ``{BART:} denoising sequence-to-sequence pre-training for
  natural language generation, translation, and comprehension,'' in
  \emph{Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics, {ACL} 2020, Online, July 5-10, 2020}, 2020, pp.
  7871--7880.

\bibitem{Fedus-JMLR-2021-Switch}
W.~Fedus, B.~Zoph, and N.~Shazeer, ``Switch transformers: Scaling to trillion
  parameter models with simple and efficient sparsity,'' \emph{J. Mach. Learn.
  Res}, pp. 1--40, 2021.

\bibitem{radford-blog-2019-language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever \emph{et~al.},
  ``Language models are unsupervised multitask learners,'' \emph{OpenAI blog},
  p.~9, 2019.

\bibitem{Liu-CoRR-2019-RoBERTa}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis,
  L.~Zettlemoyer, and V.~Stoyanov, ``Roberta: {A} robustly optimized {BERT}
  pretraining approach,'' \emph{CoRR}, vol. abs/1907.11692, 2019.

\bibitem{Sanh-ICLR-2022-Multitask}
V.~Sanh, A.~Webson, C.~Raffel, S.~H. Bach, L.~Sutawika, Z.~Alyafeai,
  A.~Chaffin, A.~Stiegler, A.~Raja, M.~Dey, M.~S. Bari, C.~Xu, U.~Thakker,
  S.~S. Sharma, E.~Szczechla, T.~Kim, G.~Chhablani, N.~V. Nayak, D.~Datta,
  J.~Chang, M.~T. Jiang, H.~Wang, M.~Manica, S.~Shen, Z.~X. Yong, H.~Pandey,
  R.~Bawden, T.~Wang, T.~Neeraj, J.~Rozen, A.~Sharma, A.~Santilli,
  T.~F{\'{e}}vry, J.~A. Fries, R.~Teehan, T.~L. Scao, S.~Biderman, L.~Gao,
  T.~Wolf, and A.~M. Rush, ``Multitask prompted training enables zero-shot task
  generalization,'' in \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.\hskip 1em
  plus 0.5em minus 0.4em\relax OpenReview.net, 2022.

\bibitem{Wang-ICML-2022-What}
T.~Wang, A.~Roberts, D.~Hesslow, T.~L. Scao, H.~W. Chung, I.~Beltagy,
  J.~Launay, and C.~Raffel, ``What language model architecture and pretraining
  objective works best for zero-shot generalization?'' in \emph{International
  Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore,
  Maryland, {USA}}, ser. Proceedings of Machine Learning Research, vol. 162,
  2022, pp. 22\,964--22\,984.

\bibitem{Kaplan-arxiv-2020-Scaling}
J.~Kaplan, S.~McCandlish, T.~Henighan, T.~B. Brown, B.~Chess, R.~Child,
  S.~Gray, A.~Radford, J.~Wu, and D.~Amodei, ``Scaling laws for neural language
  models,'' \emph{CoRR}, vol. abs/2001.08361, 2020.

\bibitem{Wei-arxiv-2022-Emergent}
J.~Wei, Y.~Tay, R.~Bommasani, C.~Raffel, B.~Zoph, S.~Borgeaud, D.~Yogatama,
  M.~Bosma, D.~Zhou, D.~Metzler, E.~H. Chi, T.~Hashimoto, O.~Vinyals, P.~Liang,
  J.~Dean, and W.~Fedus, ``Emergent abilities of large language models,''
  \emph{CoRR}, vol. abs/2206.07682, 2022.

\bibitem{Shanahan-arxiv-2022-Talking}
M.~Shanahan, ``Talking about large language models,'' \emph{CoRR}, vol.
  abs/2212.03551, 2022.

\bibitem{Wei-arxiv-2022-chain}
J.~Wei, X.~Wang, D.~Schuurmans, M.~Bosma, E.~H. Chi, Q.~Le, and D.~Zhou,
  ``Chain of thought prompting elicits reasoning in large language models,''
  \emph{CoRR}, vol. abs/2201.11903, 2022.

\bibitem{Hoffmann-arxiv-2022-Training}
J.~Hoffmann, S.~Borgeaud, A.~Mensch, E.~Buchatskaya, T.~Cai, E.~Rutherford,
  D.~de~Las~Casas, L.~A. Hendricks, J.~Welbl, A.~Clark, T.~Hennigan, E.~Noland,
  K.~Millican, G.~van~den Driessche, B.~Damoc, A.~Guy, S.~Osindero,
  K.~Simonyan, E.~Elsen, J.~W. Rae, O.~Vinyals, and L.~Sifre, ``Training
  compute-optimal large language models,'' vol. abs/2203.15556, 2022.

\bibitem{Taylor-arxiv-2022-Galactica}
R.~Taylor, M.~Kardas, G.~Cucurull, T.~Scialom, A.~Hartshorn, E.~Saravia,
  A.~Poulton, V.~Kerkez, and R.~Stojnic, ``Galactica: {A} large language model
  for science,'' \emph{CoRR}, vol. abs/2211.09085, 2022.

\bibitem{Liu-survey-2023-Pre-train}
P.~Liu, W.~Yuan, J.~Fu, Z.~Jiang, H.~Hayashi, and G.~Neubig, ``Pre-train,
  prompt, and predict: {A} systematic survey of prompting methods in natural
  language processing,'' \emph{{ACM} Comput. Surv.}, pp. 195:1--195:35, 2023.

\bibitem{Zhou-arxiv-2023-A}
C.~Zhou, Q.~Li, C.~Li, J.~Yu, Y.~Liu, G.~Wang, K.~Zhang, C.~Ji, Q.~Yan, L.~He,
  H.~Peng, J.~Li, J.~Wu, Z.~Liu, P.~Xie, C.~Xiong, J.~Pei, P.~S. Yu, and
  L.~Sun, ``A comprehensive survey on pretrained foundation models: {A} history
  from {BERT} to chatgpt,'' \emph{CoRR}, vol. abs/2302.09419, 2023.

\bibitem{Han-AIopen-2021-PTM}
X.~Han, Z.~Zhang, N.~Ding, Y.~Gu, X.~Liu, Y.~Huo, J.~Qiu, Y.~Yao, A.~Zhang,
  L.~Zhang, W.~Han, M.~Huang, Q.~Jin, Y.~Lan, Y.~Liu, Z.~Liu, Z.~Lu, X.~Qiu,
  R.~Song, J.~Tang, J.~Wen, J.~Yuan, W.~X. Zhao, and J.~Zhu, ``Pre-trained
  models: Past, present and future,'' \emph{{AI} Open}, vol.~2, pp. 225--250,
  2021.

\bibitem{qiu-CoRR-2020-PTM}
X.~Qiu, T.~Sun, Y.~Xu, Y.~Shao, N.~Dai, and X.~Huang, ``Pre-trained models for
  natural language processing: {A} survey,'' \emph{CoRR}, vol. abs/2003.08271,
  2020.

\bibitem{OpenAI-blog-2023-Planning}
S.~Altman, ``Planning for agi and beyond,'' \emph{OpenAI Blog}, February 2023.

\bibitem{Bubeck-arxiv-2023-Sparks}
S.~Bubeck, V.~Chandrasekaran, R.~Eldan, J.~Gehrke, E.~Horvitz, E.~Kamar,
  P.~Lee, Y.~T. Lee, Y.~Li, S.~Lundberg, H.~Nori, H.~Palangi, M.~T. Ribeiro,
  and Y.~Zhang, ``Sparks of artificial general intelligence: Early experiments
  with gpt-4,'' vol. abs/2303.12712, 2023.

\bibitem{Huang-CoRR-2023}
S.~Huang, L.~Dong, W.~Wang, Y.~Hao, S.~Singhal, S.~Ma, T.~Lv, L.~Cui, O.~K.
  Mohammed, B.~Patra, Q.~Liu, K.~Aggarwal, Z.~Chi, J.~Bjorck, V.~Chaudhary,
  S.~Som, X.~Song, and F.~Wei, ``Language is not all you need: Aligning
  perception with language models,'' \emph{CoRR}, vol. abs/2302.14045, 2023.

\bibitem{Cao-arxiv-2023-comprehensive}
Y.~Cao, S.~Li, Y.~Liu, Z.~Yan, Y.~Dai, P.~S. Yu, and L.~Sun, ``A comprehensive
  survey of ai-generated content (aigc): A history of generative ai from gan to
  chatgpt,'' \emph{arXiv preprint arXiv:2303.04226}, 2023.

\bibitem{driess-arxiv-2023-palm}
D.~Driess, F.~Xia, M.~S. Sajjadi, C.~Lynch, A.~Chowdhery, B.~Ichter, A.~Wahid,
  J.~Tompson, Q.~Vuong, T.~Yu \emph{et~al.}, ``Palm-e: An embodied multimodal
  language model,'' \emph{arXiv preprint arXiv:2303.03378}, 2023.

\bibitem{wu-arxiv-2023-visual}
C.~Wu, S.~Yin, W.~Qi, X.~Wang, Z.~Tang, and N.~Duan, ``Visual chatgpt: Talking,
  drawing and editing with visual foundation models,'' \emph{arXiv preprint
  arXiv:2303.04671}, 2023.

\bibitem{OpenAI-OpenAI-2023-GPT-4}
OpenAI, ``Gpt-4 technical report,'' \emph{OpenAI}, 2023.

\bibitem{FU-blog-2022-how}
Y.~Fu, H.~Peng, and T.~Khot, ``How does gpt obtain its ability? tracing
  emergent abilities of language models to their sources,'' \emph{Yao Fu’s
  Notion}, Dec 2022.

\bibitem{Li-IJCAI-2021-Pretrained}
J.~Li, T.~Tang, W.~X. Zhao, and J.~Wen, ``Pretrained language model for text
  generation: {A} survey,'' in \emph{Proceedings of the Thirtieth International
  Joint Conference on Artificial Intelligence, {IJCAI} 2021, Virtual Event /
  Montreal, Canada, 19-27 August 2021}, Z.~Zhou, Ed.\hskip 1em plus 0.5em minus
  0.4em\relax ijcai.org, 2021, pp. 4492--4499.

\bibitem{Lu-arxiv-2022-Survey}
P.~Lu, L.~Qiu, W.~Yu, S.~Welleck, and K.~Chang, ``A survey of deep learning for
  mathematical reasoning,'' \emph{CoRR}, vol. abs/2212.10535, 2022.

\bibitem{Dong-arxiv-2023-A}
Q.~Dong, L.~Li, D.~Dai, C.~Zheng, Z.~Wu, B.~Chang, X.~Sun, J.~Xu, L.~Li, and
  Z.~Sui, ``A survey for in-context learning,'' \emph{CoRR}, vol.
  abs/2301.00234, 2023.

\bibitem{Huang-arxiv-2022-Towards}
J.~Huang and K.~C. Chang, ``Towards reasoning in large language models: {A}
  survey,'' \emph{CoRR}, vol. abs/2212.10403, 2022.

\bibitem{Qiao-arxiv-2022-Reasoning}
S.~Qiao, Y.~Ou, N.~Zhang, X.~Chen, Y.~Yao, S.~Deng, C.~Tan, F.~Huang, and
  H.~Chen, ``Reasoning with language model prompting: {A} survey,''
  \emph{CoRR}, vol. abs/2212.09597, 2022.

\bibitem{Zhou-FITEE-2023-ChatGPT}
J.~Zhou, P.~Ke, X.~Qiu, M.~Huang, and J.~Zhang, ``Chatgpt: potential,
  prospects, and limitations,'' in \emph{Frontiers of Information Technology \&
  Electronic Engineering}, 2023, pp. 1--6.

\bibitem{Zhao-arxiv-2022-Dense}
W.~X. Zhao, J.~Liu, R.~Ren, and J.~Wen, ``Dense text retrieval based on
  pretrained language models: {A} survey,'' \emph{CoRR}, vol. abs/2211.14876,
  2022.

\bibitem{Brown-NeurIPS-2020-Language}
T.~B. Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, S.~Agarwal,
  A.~Herbert{-}Voss, G.~Krueger, T.~Henighan, R.~Child, A.~Ramesh, D.~M.
  Ziegler, J.~Wu, C.~Winter, C.~Hesse, M.~Chen, E.~Sigler, M.~Litwin, S.~Gray,
  B.~Chess, J.~Clark, C.~Berner, S.~McCandlish, A.~Radford, I.~Sutskever, and
  D.~Amodei, ``Language models are few-shot learners,'' in \emph{Advances in
  Neural Information Processing Systems 33: Annual Conference on Neural
  Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
  virtual}, H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~Balcan, and H.~Lin, Eds.,
  2020.

\bibitem{Chowdhery-arxiv-2022-PaLM}
A.~Chowdhery, S.~Narang, J.~Devlin, M.~Bosma, G.~Mishra, A.~Roberts, P.~Barham,
  H.~W. Chung, C.~Sutton, S.~Gehrmann, P.~Schuh, K.~Shi, S.~Tsvyashchenko,
  J.~Maynez, A.~Rao, P.~Barnes, Y.~Tay, N.~Shazeer, V.~Prabhakaran, E.~Reif,
  N.~Du, B.~Hutchinson, R.~Pope, J.~Bradbury, J.~Austin, M.~Isard,
  G.~Gur{-}Ari, P.~Yin, T.~Duke, A.~Levskaya, S.~Ghemawat, S.~Dev,
  H.~Michalewski, X.~Garcia, V.~Misra, K.~Robinson, L.~Fedus, D.~Zhou,
  D.~Ippolito, D.~Luan, H.~Lim, B.~Zoph, A.~Spiridonov, R.~Sepassi, D.~Dohan,
  S.~Agrawal, M.~Omernick, A.~M. Dai, T.~S. Pillai, M.~Pellat, A.~Lewkowycz,
  E.~Moreira, R.~Child, O.~Polozov, K.~Lee, Z.~Zhou, X.~Wang, B.~Saeta,
  M.~Diaz, O.~Firat, M.~Catasta, J.~Wei, K.~Meier{-}Hellstern, D.~Eck, J.~Dean,
  S.~Petrov, and N.~Fiedel, ``Palm: Scaling language modeling with pathways,''
  \emph{CoRR}, vol. abs/2204.02311, 2022.

\bibitem{Touvron-arxiv-2023-LLaMA}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.~Lachaux, T.~Lacroix,
  B.~Rozi{\`{e}}re, N.~Goyal, E.~Hambro, F.~Azhar, A.~Rodriguez, A.~Joulin,
  E.~Grave, and G.~Lample, ``Llama: Open and efficient foundation language
  models,'' \emph{CoRR}, 2023.

\bibitem{Henighan-2020-scalinglaw}
T.~Henighan, J.~Kaplan, M.~Katz, M.~Chen, C.~Hesse, J.~Jackson, H.~Jun, T.~B.
  Brown, P.~Dhariwal, S.~Gray \emph{et~al.}, ``Scaling laws for autoregressive
  generative modeling,'' \emph{arXiv preprint arXiv:2010.14701}, 2020.

\bibitem{Xie-arxiv-2023-doremi}
S.~M. Xie, H.~Pham, X.~Dong, N.~Du, H.~Liu, Y.~Lu, P.~Liang, Q.~V. Le, T.~Ma,
  and A.~W. Yu, ``Doremi: Optimizing data mixtures speeds up language model
  pretraining,'' \emph{arXiv preprint arXiv:2305.10429}, 2023.

\bibitem{Villalobos-arXiv-2023-runout}
\BIBentryALTinterwordspacing
P.~Villalobos, J.~Sevilla, L.~Heim, T.~Besiroglu, M.~Hobbhahn, and A.~Ho,
  ``Will we run out of data? an analysis of the limits of scaling datasets in
  machine learning,'' \emph{CoRR}, vol. abs/2211.04325, 2022. [Online].
  Available: \url{https://doi.org/10.48550/arXiv.2211.04325}
\BIBentrySTDinterwordspacing

\bibitem{Muennighoff-arXiv-2023-dataconstrained}
N.~Muennighoff, A.~M. Rush, B.~Barak, T.~L. Scao, A.~Piktus, N.~Tazi,
  S.~Pyysalo, T.~Wolf, and C.~Raffel, ``Scaling data-constrained language
  models,'' \emph{arXiv preprint arXiv:2305.16264}, 2023.

\bibitem{McKenzie-2022-inverse}
\BIBentryALTinterwordspacing
I.~McKenzie, A.~Lyzhov, A.~Parrish, A.~Prabhu, A.~Mueller, N.~Kim, S.~Bowman,
  and E.~Perez, ``The inverse scaling prize,'' 2022. [Online]. Available:
  \url{https://github.com/inverse-scaling/prize}
\BIBentrySTDinterwordspacing

\bibitem{Huberman-AI-1987-phase}
B.~A. Huberman and T.~Hogg, ``Phase transitions in artificial intelligence
  systems,'' \emph{Artificial Intelligence}, vol.~33, no.~2, pp. 155--171,
  1987.

\bibitem{Rae-arxiv-2021-Scaling}
J.~W. Rae, S.~Borgeaud, T.~Cai, K.~Millican, J.~Hoffmann, H.~F. Song,
  J.~Aslanides, S.~Henderson, R.~Ring, S.~Young, E.~Rutherford, T.~Hennigan,
  J.~Menick, A.~Cassirer, R.~Powell, G.~van~den Driessche, L.~A. Hendricks,
  M.~Rauh, P.~Huang, A.~Glaese, J.~Welbl, S.~Dathathri, S.~Huang, J.~Uesato,
  J.~Mellor, I.~Higgins, A.~Creswell, N.~McAleese, A.~Wu, E.~Elsen, S.~M.
  Jayakumar, E.~Buchatskaya, D.~Budden, E.~Sutherland, K.~Simonyan,
  M.~Paganini, L.~Sifre, L.~Martens, X.~L. Li, A.~Kuncoro, A.~Nematzadeh,
  E.~Gribovskaya, D.~Donato, A.~Lazaridou, A.~Mensch, J.~Lespiau,
  M.~Tsimpoukelli, N.~Grigorev, D.~Fritz, T.~Sottiaux, M.~Pajarskas, T.~Pohlen,
  Z.~Gong, D.~Toyama, C.~de~Masson~d'Autume, Y.~Li, T.~Terzi, V.~Mikulik,
  I.~Babuschkin, A.~Clark, D.~de~Las~Casas, A.~Guy, C.~Jones, J.~Bradbury,
  M.~J. Johnson, B.~A. Hechtman, L.~Weidinger, I.~Gabriel, W.~S. Isaac,
  E.~Lockhart, S.~Osindero, L.~Rimell, C.~Dyer, O.~Vinyals, K.~Ayoub,
  J.~Stanway, L.~Bennett, D.~Hassabis, K.~Kavukcuoglu, and G.~Irving, ``Scaling
  language models: Methods, analysis {\&} insights from training gopher,''
  \emph{CoRR}, vol. abs/2112.11446, 2021.

\bibitem{Dai-arxiv-2022-Why}
D.~Dai, Y.~Sun, L.~Dong, Y.~Hao, Z.~Sui, and F.~Wei, ``Why can {GPT} learn
  in-context? language models secretly perform gradient descent as
  meta-optimizers,'' \emph{CoRR}, vol. abs/2212.10559, 2022.

\bibitem{Ouyang-arxiv-2022-Training}
L.~Ouyang, J.~Wu, X.~Jiang, D.~Almeida, C.~L. Wainwright, P.~Mishkin, C.~Zhang,
  S.~Agarwal, K.~Slama, A.~Ray, J.~Schulman, J.~Hilton, F.~Kelton, L.~Miller,
  M.~Simens, A.~Askell, P.~Welinder, P.~F. Christiano, J.~Leike, and R.~Lowe,
  ``Training language models to follow instructions with human feedback,''
  \emph{CoRR}, vol. abs/2203.02155, 2022.

\bibitem{Wei-ICLR-2022-Finetuned}
J.~Wei, M.~Bosma, V.~Y. Zhao, K.~Guu, A.~W. Yu, B.~Lester, N.~Du, A.~M. Dai,
  and Q.~V. Le, ``Finetuned language models are zero-shot learners,'' in
  \emph{The Tenth International Conference on Learning Representations, {ICLR}
  2022, Virtual Event, April 25-29, 2022}.\hskip 1em plus 0.5em minus
  0.4em\relax OpenReview.net, 2022.

\bibitem{Thoppilan-CoRR-2022-LaMDA}
R.~Thoppilan, D.~D. Freitas, J.~Hall, N.~Shazeer, A.~Kulshreshtha, H.~Cheng,
  A.~Jin, T.~Bos, L.~Baker, Y.~Du, Y.~Li, H.~Lee, H.~S. Zheng, A.~Ghafouri,
  M.~Menegali, Y.~Huang, M.~Krikun, D.~Lepikhin, J.~Qin, D.~Chen, Y.~Xu,
  Z.~Chen, A.~Roberts, M.~Bosma, Y.~Zhou, C.~Chang, I.~Krivokon, W.~Rusch,
  M.~Pickett, K.~S. Meier{-}Hellstern, M.~R. Morris, T.~Doshi, R.~D. Santos,
  T.~Duke, J.~Soraker, B.~Zevenbergen, V.~Prabhakaran, M.~Diaz, B.~Hutchinson,
  K.~Olson, A.~Molina, E.~Hoffman{-}John, J.~Lee, L.~Aroyo, R.~Rajakumar,
  A.~Butryna, M.~Lamm, V.~Kuzmina, J.~Fenton, A.~Cohen, R.~Bernstein,
  R.~Kurzweil, B.~Aguera{-}Arcas, C.~Cui, M.~Croak, E.~H. Chi, and Q.~Le,
  ``Lamda: Language models for dialog applications,'' \emph{CoRR}, vol.
  abs/2201.08239, 2022.

\bibitem{Chung-arxiv-2022-Scaling}
H.~W. Chung, L.~Hou, S.~Longpre, B.~Zoph, Y.~Tay, W.~Fedus, E.~Li, X.~Wang,
  M.~Dehghani, S.~Brahma, A.~Webson, S.~S. Gu, Z.~Dai, M.~Suzgun, X.~Chen,
  A.~Chowdhery, S.~Narang, G.~Mishra, A.~Yu, V.~Y. Zhao, Y.~Huang, A.~M. Dai,
  H.~Yu, S.~Petrov, E.~H. Chi, J.~Dean, J.~Devlin, A.~Roberts, D.~Zhou, Q.~V.
  Le, and J.~Wei, ``Scaling instruction-finetuned language models,''
  \emph{CoRR}, vol. abs/2210.11416, 2022.

\bibitem{Srivastava-arxiv-2022-Beyond}
A.~Srivastava, A.~Rastogi, A.~Rao, A.~A.~M. Shoeb, A.~Abid, A.~Fisch, A.~R.
  Brown, A.~Santoro, A.~Gupta, A.~Garriga{-}Alonso, A.~Kluska, A.~Lewkowycz,
  A.~Agarwal, A.~Power, A.~Ray, A.~Warstadt, A.~W. Kocurek, A.~Safaya,
  A.~Tazarv, A.~Xiang, A.~Parrish, A.~Nie, A.~Hussain, A.~Askell, A.~Dsouza,
  A.~Rahane, A.~S. Iyer, A.~Andreassen, A.~Santilli, A.~Stuhlm{\"{u}}ller,
  A.~M. Dai, A.~La, A.~K. Lampinen, A.~Zou, A.~Jiang, A.~Chen, A.~Vuong,
  A.~Gupta, A.~Gottardi, A.~Norelli, A.~Venkatesh, A.~Gholamidavoodi,
  A.~Tabassum, A.~Menezes, A.~Kirubarajan, A.~Mullokandov, A.~Sabharwal,
  A.~Herrick, A.~Efrat, A.~Erdem, A.~Karakas, and et~al., ``Beyond the
  imitation game: Quantifying and extrapolating the capabilities of language
  models,'' \emph{CoRR}, vol. abs/2206.04615, 2022.

\bibitem{Schaeffer-arXiv-2023-mirage}
R.~Schaeffer, B.~Miranda, and S.~Koyejo, ``Are emergent abilities of large
  language models a mirage?'' \emph{arXiv preprint arXiv:2304.15004}, 2023.

\bibitem{Hu-arXiv-2023-unlock}
S.~Hu, X.~Liu, X.~Han, X.~Zhang, C.~He, W.~Zhao, Y.~Lin, N.~Ding, Z.~Ou,
  G.~Zeng, Z.~Liu, and M.~Sun, ``Unlock predictable scaling from emergent
  abilities,'' 2023.

\bibitem{Power-arxiv-2022-grokking}
A.~Power, Y.~Burda, H.~Edwards, I.~Babuschkin, and V.~Misra, ``Grokking:
  Generalization beyond overfitting on small algorithmic datasets,''
  \emph{arXiv preprint arXiv:2201.02177}, 2022.

\bibitem{Rasley-KDD-2020-DeepSpeed}
J.~Rasley, S.~Rajbhandari, O.~Ruwase, and Y.~He, ``Deepspeed: System
  optimizations enable training deep learning models with over 100 billion
  parameters,'' in \emph{{KDD}}, 2020, pp. 3505--3506.

\bibitem{Shoeybi-arXiv-2019-Megatron}
M.~Shoeybi, M.~Patwary, R.~Puri, P.~LeGresley, J.~Casper, and B.~Catanzaro,
  ``Megatron-lm: Training multi-billion parameter language models using model
  parallelism,'' \emph{CoRR}, vol. abs/1909.08053, 2019.

\bibitem{Narayanan-ACM-2021-Efficient}
D.~Narayanan, M.~Shoeybi, J.~Casper, P.~LeGresley, M.~Patwary, V.~Korthikanti,
  D.~Vainbrand, P.~Kashinkunti, J.~Bernauer, B.~Catanzaro, A.~Phanishayee, and
  M.~Zaharia, ``Efficient large-scale language model training on {GPU} clusters
  using megatron-lm,'' in \emph{International Conference for High Performance
  Computing, Networking, Storage and Analysis, {SC} 2021, St. Louis, Missouri,
  USA, November 14-19, 2021}.\hskip 1em plus 0.5em minus 0.4em\relax {ACM},
  2021, p.~58.

\bibitem{Korthikanti-arxiv-2022-reducing}
V.~Korthikanti, J.~Casper, S.~Lym, L.~McAfee, M.~Andersch, M.~Shoeybi, and
  B.~Catanzaro, ``Reducing activation recomputation in large transformer
  models,'' \emph{CoRR}, vol. abs/2205.05198, 2022.

\bibitem{Scao-arxiv-2022-BLOOM}
T.~L. Scao, A.~Fan, C.~Akiki, E.~Pavlick, S.~Ilic, D.~Hesslow,
  R.~Castagn{\'{e}}, A.~S. Luccioni, F.~Yvon, M.~Gall{\'{e}}, J.~Tow, A.~M.
  Rush, S.~Biderman, A.~Webson, P.~S. Ammanamanchi, T.~Wang, B.~Sagot,
  N.~Muennighoff, A.~V. del Moral, O.~Ruwase, R.~Bawden, S.~Bekman,
  A.~McMillan{-}Major, I.~Beltagy, H.~Nguyen, L.~Saulnier, S.~Tan, P.~O.
  Suarez, V.~Sanh, H.~Lauren{\c{c}}on, Y.~Jernite, J.~Launay, M.~Mitchell,
  C.~Raffel, A.~Gokaslan, A.~Simhi, A.~Soroa, A.~F. Aji, A.~Alfassy, A.~Rogers,
  A.~K. Nitzav, C.~Xu, C.~Mou, C.~Emezue, C.~Klamm, C.~Leong, D.~van Strien,
  D.~I. Adelani, and et~al., ``{BLOOM:} {A} 176b-parameter open-access
  multilingual language model,'' \emph{CoRR}, vol. abs/2211.05100, 2022.

\bibitem{Christiano-NeurIPS-2017-Deep}
P.~F. Christiano, J.~Leike, T.~B. Brown, M.~Martic, S.~Legg, and D.~Amodei,
  ``Deep reinforcement learning from human preferences,'' in \emph{Advances in
  Neural Information Processing Systems 30: Annual Conference on Neural
  Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
  {USA}}, I.~Guyon, U.~von Luxburg, S.~Bengio, H.~M. Wallach, R.~Fergus,
  S.~V.~N. Vishwanathan, and R.~Garnett, Eds., 2017, pp. 4299--4307.

\bibitem{Schick-arxiv-2023-Toolformer}
T.~Schick, J.~Dwivedi{-}Yu, R.~Dess{\`{\i}}, R.~Raileanu, M.~Lomeli,
  L.~Zettlemoyer, N.~Cancedda, and T.~Scialom, ``Toolformer: Language models
  can teach themselves to use tools,'' \emph{CoRR}, vol. abs/2302.04761, 2023.

\bibitem{Nakano-arxiv-2021-WebGPT}
R.~Nakano, J.~Hilton, S.~Balaji, J.~Wu, L.~Ouyang, C.~Kim, C.~Hesse, S.~Jain,
  V.~Kosaraju, W.~Saunders, X.~Jiang, K.~Cobbe, T.~Eloundou, G.~Krueger,
  K.~Button, M.~Knight, B.~Chess, and J.~Schulman, ``Webgpt: Browser-assisted
  question-answering with human feedback,'' \emph{CoRR}, vol. abs/2112.09332,
  2021.

\bibitem{Raffel-JMLR-2020-Exploring}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, and P.~J. Liu, ``Exploring the limits of transfer learning with a
  unified text-to-text transformer,'' \emph{J. Mach. Learn. Res.}, pp.
  140:1--140:67, 2020.

\bibitem{Xue-NAACL-2021-mT5}
L.~Xue, N.~Constant, A.~Roberts, M.~Kale, R.~Al{-}Rfou, A.~Siddhant, A.~Barua,
  and C.~Raffel, ``mt5: {A} massively multilingual pre-trained text-to-text
  transformer,'' in \emph{Proceedings of the 2021 Conference of the North
  American Chapter of the Association for Computational Linguistics: Human
  Language Technologies, {NAACL-HLT} 2021, Online, June 6-11, 2021}, 2021, pp.
  483--498.

\bibitem{Zeng-arxiv-2021-PanGualpha}
W.~Zeng, X.~Ren, T.~Su, H.~Wang, Y.~Liao, Z.~Wang, X.~Jiang, Z.~Yang, K.~Wang,
  X.~Zhang, C.~Li, Z.~Gong, Y.~Yao, X.~Huang, J.~Wang, J.~Yu, Q.~Guo, Y.~Yu,
  Y.~Zhang, J.~Wang, H.~Tao, D.~Yan, Z.~Yi, F.~Peng, F.~Jiang, H.~Zhang,
  L.~Deng, Y.~Zhang, Z.~Lin, C.~Zhang, S.~Zhang, M.~Guo, S.~Gu, G.~Fan,
  Y.~Wang, X.~Jin, Q.~Liu, and Y.~Tian, ``Pangu-{\(\alpha\)}: Large-scale
  autoregressive pretrained chinese language models with auto-parallel
  computation,'' \emph{CoRR}, vol. abs/2104.12369, 2021.

\bibitem{Zhang-arXiv-2021-CPM-2}
Z.~Zhang, Y.~Gu, X.~Han, S.~Chen, C.~Xiao, Z.~Sun, Y.~Yao, F.~Qi, J.~Guan,
  P.~Ke, Y.~Cai, G.~Zeng, Z.~Tan, Z.~Liu, M.~Huang, W.~Han, Y.~Liu, X.~Zhu, and
  M.~Sun, ``{CPM-2:} large-scale cost-effective pre-trained language models,''
  \emph{CoRR}, vol. abs/2106.10715, 2021.

\bibitem{nijkamp-arxiv-2022-Codegen}
E.~Nijkamp, B.~Pang, H.~Hayashi, L.~Tu, H.~Wang, Y.~Zhou, S.~Savarese, and
  C.~Xiong, ``Codegen: An open large language model for code with mtulti-turn
  program synthesis,'' \emph{arXiv preprint arXiv:2203.13474}, 2022.

\bibitem{Black-CoRR-2022-GPT}
S.~Black, S.~Biderman, E.~Hallahan, Q.~Anthony, L.~Gao, L.~Golding, H.~He,
  C.~Leahy, K.~McDonell, J.~Phang, M.~Pieler, U.~S. Prashanth, S.~Purohit,
  L.~Reynolds, J.~Tow, B.~Wang, and S.~Weinbach, ``Gpt-neox-20b: An open-source
  autoregressive language model,'' \emph{CoRR}, vol. abs/2204.06745, 2022.

\bibitem{Wang-EMNLP-2022-Super}
Y.~Wang, S.~Mishra, P.~Alipoormolabashi, Y.~Kordi, A.~Mirzaei, A.~Naik,
  A.~Ashok, A.~S. Dhanasekaran, A.~Arunkumar, D.~Stap, E.~Pathak,
  G.~Karamanolakis, H.~G. Lai, I.~Purohit, I.~Mondal, J.~Anderson, K.~Kuznia,
  K.~Doshi, K.~K. Pal, M.~Patel, M.~Moradshahi, M.~Parmar, M.~Purohit,
  N.~Varshney, P.~R. Kaza, P.~Verma, R.~S. Puri, R.~Karia, S.~Doshi, S.~K.
  Sampat, S.~Mishra, S.~R. A, S.~Patro, T.~Dixit, and X.~Shen,
  ``Super-naturalinstructions: Generalization via declarative instructions on
  1600+ {NLP} tasks,'' in \emph{Proceedings of the 2022 Conference on Empirical
  Methods in Natural Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab
  Emirates, December 7-11, 2022}, 2022, pp. 5085--5109.

\bibitem{Tay-arxiv-2022-UL2}
Y.~Tay, M.~Dehghani, V.~Q. Tran, X.~Garc{\'i}a, J.~Wei, X.~Wang, H.~W. Chung,
  D.~Bahri, T.~Schuster, H.~Zheng, D.~Zhou, N.~Houlsby, and D.~Metzler, ``Ul2:
  Unifying language learning paradigms,'' 2022.

\bibitem{Zhang-arxiv-2022-OPT}
S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~T.
  Diab, X.~Li, X.~V. Lin, T.~Mihaylov, M.~Ott, S.~Shleifer, K.~Shuster,
  D.~Simig, P.~S. Koura, A.~Sridhar, T.~Wang, and L.~Zettlemoyer, ``{OPT:} open
  pre-trained transformer language models,'' \emph{CoRR}, vol. abs/2205.01068,
  2022.

\bibitem{Marta-arxiv-2022-NLLB}
M.~R. Costa{-}juss{\`{a}}, J.~Cross, O.~{\c{C}}elebi, M.~Elbayad, K.~Heafield,
  K.~Heffernan, E.~Kalbassi, J.~Lam, D.~Licht, J.~Maillard, A.~Sun, S.~Wang,
  G.~Wenzek, A.~Youngblood, B.~Akula, L.~Barrault, G.~M. Gonzalez, P.~Hansanti,
  J.~Hoffman, S.~Jarrett, K.~R. Sadagopan, D.~Rowe, S.~Spruit, C.~Tran,
  P.~Andrews, N.~F. Ayan, S.~Bhosale, S.~Edunov, A.~Fan, C.~Gao, V.~Goswami,
  F.~Guzm{\'{a}}n, P.~Koehn, A.~Mourachko, C.~Ropers, S.~Saleem, H.~Schwenk,
  and J.~Wang, ``No language left behind: Scaling human-centered machine
  translation,'' \emph{CoRR}, vol. abs/2207.04672, 2022.

\bibitem{Zheng-arXiv-2023-CodeGeex}
Q.~Zheng, X.~Xia, X.~Zou, Y.~Dong, S.~Wang, Y.~Xue, Z.~Wang, L.~Shen, A.~Wang,
  Y.~Li \emph{et~al.}, ``Codegeex: A pre-trained model for code generation with
  multilingual evaluations on humaneval-x,'' \emph{arXiv preprint
  arXiv:2303.17568}, 2023.

\bibitem{Zeng-arxiv-2022-GLM}
A.~Zeng, X.~Liu, Z.~Du, Z.~Wang, H.~Lai, M.~Ding, Z.~Yang, Y.~Xu, W.~Zheng,
  X.~Xia, W.~L. Tam, Z.~Ma, Y.~Xue, J.~Zhai, W.~Chen, P.~Zhang, Y.~Dong, and
  J.~Tang, ``{GLM-130B:} an open bilingual pre-trained model,'' vol.
  abs/2210.02414, 2022.

\bibitem{Muennighoff-2022-arxiv-Crosslingual}
N.~Muennighoff, T.~Wang, L.~Sutawika, A.~Roberts, S.~Biderman, T.~L. Scao,
  M.~S. Bari, S.~Shen, Z.~X. Yong, H.~Schoelkopf, X.~Tang, D.~Radev, A.~F. Aji,
  K.~Almubarak, S.~Albanie, Z.~Alyafeai, A.~Webson, E.~Raff, and C.~Raffel,
  ``Crosslingual generalization through multitask finetuning,'' \emph{CoRR},
  vol. abs/2211.01786, 2022.

\bibitem{Iyer-arxiv-2022-OPT}
S.~Iyer, X.~V. Lin, R.~Pasunuru, T.~Mihaylov, D.~Simig, P.~Yu, K.~Shuster,
  T.~Wang, Q.~Liu, P.~S. Koura, X.~Li, B.~O'Horo, G.~Pereyra, J.~Wang,
  C.~Dewan, A.~Celikyilmaz, L.~Zettlemoyer, and V.~Stoyanov, ``{OPT-IML:}
  scaling language model instruction meta learning through the lens of
  generalization,'' \emph{CoRR}, vol. abs/2212.12017, 2022.

\bibitem{Biderman-arxiv-2023-Pythia}
S.~Biderman, H.~Schoelkopf, Q.~Anthony, H.~Bradley, K.~O'Brien, E.~Hallahan,
  M.~A. Khan, S.~Purohit, U.~S. Prashanth, E.~Raff \emph{et~al.}, ``Pythia: A
  suite for analyzing large language models across training and scaling,''
  \emph{arXiv preprint arXiv:2304.01373}, 2023.

\bibitem{Nijkamp-2023-codegen2-arxiv}
E.~Nijkamp, H.~Hayashi, C.~Xiong, S.~Savarese, and Y.~Zhou, ``Codegen2: Lessons
  for training llms on programming and natural languages,'' \emph{CoRR}, vol.
  abs/2305.02309, 2023.

\bibitem{Li-2023-arxiv-Starcoder}
\BIBentryALTinterwordspacing
R.~Li, L.~B. Allal, Y.~Zi, N.~Muennighoff, D.~Kocetkov, C.~Mou, M.~Marone,
  C.~Akiki, J.~Li, J.~Chim, Q.~Liu, E.~Zheltonozhskii, T.~Y. Zhuo, T.~Wang,
  O.~Dehaene, M.~Davaadorj, J.~Lamy{-}Poirier, J.~Monteiro, O.~Shliazhko,
  N.~Gontier, N.~Meade, A.~Zebaze, M.~Yee, L.~K. Umapathi, J.~Zhu, B.~Lipkin,
  M.~Oblokulov, Z.~Wang, R.~M. V, J.~Stillerman, S.~S. Patel, D.~Abulkhanov,
  M.~Zocca, M.~Dey, Z.~Zhang, N.~Fahmy, U.~Bhattacharyya, W.~Yu, S.~Singh,
  S.~Luccioni, P.~Villegas, M.~Kunakov, F.~Zhdanov, M.~Romero, T.~Lee,
  N.~Timor, J.~Ding, C.~Schlesinger, H.~Schoelkopf, J.~Ebert, T.~Dao,
  M.~Mishra, A.~Gu, J.~Robinson, C.~J. Anderson, B.~Dolan{-}Gavitt,
  D.~Contractor, S.~Reddy, D.~Fried, D.~Bahdanau, Y.~Jernite, C.~M. Ferrandis,
  S.~Hughes, T.~Wolf, A.~Guha, L.~von Werra, and H.~de~Vries, ``Starcoder: may
  the source be with you!'' \emph{CoRR}, vol. abs/2305.06161, 2023. [Online].
  Available: \url{https://doi.org/10.48550/arXiv.2305.06161}
\BIBentrySTDinterwordspacing

\bibitem{Touvron-2023-llama2-arxiv}
H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei,
  N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale \emph{et~al.}, ``Llama 2:
  Open foundation and fine-tuned chat models,'' \emph{arXiv preprint
  arXiv:2307.09288}, 2023.

\bibitem{yang-2023-baichuan2}
A.~Yang, B.~Xiao, B.~Wang, B.~Zhang, C.~Yin, C.~Lv, D.~Pan, D.~Wang, D.~Yan,
  F.~Yang \emph{et~al.}, ``Baichuan 2: Open large-scale language models,''
  \emph{arXiv preprint arXiv:2309.10305}, 2023.

\bibitem{bai-2023-qwen}
J.~Bai, S.~Bai, Y.~Chu, Z.~Cui, K.~Dang, X.~Deng, Y.~Fan, W.~Ge, Y.~Han,
  F.~Huang \emph{et~al.}, ``Qwen technical report,'' \emph{arXiv preprint
  arXiv:2309.16609}, 2023.

\bibitem{Li-arxiv-2023-FLM}
X.~Li, Y.~Yao, X.~Jiang, X.~Fang, X.~Meng, S.~Fan, P.~Han, J.~Li, L.~Du, B.~Qin
  \emph{et~al.}, ``Flm-101b: An open llm and how to train it with \$100 k
  budget,'' \emph{arXiv preprint arXiv:2309.03852}, 2023.

\bibitem{wei-2023-skywork}
T.~Wei, L.~Zhao, L.~Zhang, B.~Zhu, L.~Wang, H.~Yang, B.~Li, C.~Cheng,
  W.~L{\"u}, R.~Hu \emph{et~al.}, ``Skywork: A more open bilingual foundation
  model,'' \emph{arXiv preprint arXiv:2310.19341}, 2023.

\bibitem{Lepikhin-ILR-2021-GShard}
D.~Lepikhin, H.~Lee, Y.~Xu, D.~Chen, O.~Firat, Y.~Huang, M.~Krikun, N.~Shazeer,
  and Z.~Chen, ``Gshard: Scaling giant models with conditional computation and
  automatic sharding,'' in \emph{9th International Conference on Learning
  Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}, 2021.

\bibitem{Chen-arxiv-2021-evaluating}
M.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. de~Oliveira~Pinto, J.~Kaplan,
  H.~Edwards, Y.~Burda, N.~Joseph, G.~Brockman, A.~Ray, R.~Puri, G.~Krueger,
  M.~Petrov, H.~Khlaaf, G.~Sastry, P.~Mishkin, B.~Chan, S.~Gray, N.~Ryder,
  M.~Pavlov, A.~Power, L.~Kaiser, M.~Bavarian, C.~Winter, P.~Tillet, F.~P.
  Such, D.~Cummings, M.~Plappert, F.~Chantzis, E.~Barnes, A.~Herbert{-}Voss,
  W.~H. Guss, A.~Nichol, A.~Paino, N.~Tezak, J.~Tang, I.~Babuschkin, S.~Balaji,
  S.~Jain, W.~Saunders, C.~Hesse, A.~N. Carr, J.~Leike, J.~Achiam, V.~Misra,
  E.~Morikawa, A.~Radford, M.~Knight, M.~Brundage, M.~Murati, K.~Mayer,
  P.~Welinder, B.~McGrew, D.~Amodei, S.~McCandlish, I.~Sutskever, and
  W.~Zaremba, ``Evaluating large language models trained on code,''
  \emph{CoRR}, vol. abs/2107.03374, 2021.

\bibitem{Sun-arXiv-2021-ERNIE3.0}
Y.~Sun, S.~Wang, S.~Feng, S.~Ding, C.~Pang, J.~Shang, J.~Liu, X.~Chen, Y.~Zhao,
  Y.~Lu, W.~Liu, Z.~Wu, W.~Gong, J.~Liang, Z.~Shang, P.~Sun, W.~Liu, X.~Ouyang,
  D.~Yu, H.~Tian, H.~Wu, and H.~Wang, ``{ERNIE} 3.0: Large-scale knowledge
  enhanced pre-training for language understanding and generation,''
  \emph{CoRR}, vol. abs/2107.02137, 2021.

\bibitem{lieber-2021-jurassic}
O.~Lieber, O.~Sharir, B.~Lenz, and Y.~Shoham, ``Jurassic-1: Technical details
  and evaluation,'' \emph{White Paper. AI21 Labs}, vol.~1, 2021.

\bibitem{Kim-EMNLP-2021-HyperCLOVA}
B.~Kim, H.~Kim, S.~Lee, G.~Lee, D.~Kwak, D.~H. Jeon, S.~Park, S.~Kim, S.~Kim,
  D.~Seo, H.~Lee, M.~Jeong, S.~Lee, M.~Kim, S.~Ko, S.~Kim, T.~Park, J.~Kim,
  S.~Kang, N.~Ryu, K.~M. Yoo, M.~Chang, S.~Suh, S.~In, J.~Park, K.~Kim, H.~Kim,
  J.~Jeong, Y.~G. Yeo, D.~Ham, D.~Park, M.~Y. Lee, J.~Kang, I.~Kang, J.~Ha,
  W.~Park, and N.~Sung, ``What changes can large-scale language models bring?
  intensive study on hyperclova: Billions-scale korean generative pretrained
  transformers,'' in \emph{Proceedings of the 2021 Conference on Empirical
  Methods in Natural Language Processing, {EMNLP} 2021, Virtual Event / Punta
  Cana, Dominican Republic, 7-11 November, 2021}.\hskip 1em plus 0.5em minus
  0.4em\relax Association for Computational Linguistics, 2021.

\bibitem{Wu-arxiv-2021-Yuan}
S.~Wu, X.~Zhao, T.~Yu, R.~Zhang, C.~Shen, H.~Liu, F.~Li, H.~Zhu, J.~Luo, L.~Xu
  \emph{et~al.}, ``Yuan 1.0: Large-scale pre-trained language model in
  zero-shot and few-shot learning,'' \emph{arXiv preprint arXiv:2110.04725},
  2021.

\bibitem{Askell-arxiv-2021-Anthropic}
A.~Askell, Y.~Bai, A.~Chen, D.~Drain, D.~Ganguli, T.~Henighan, A.~Jones,
  N.~Joseph, B.~Mann, N.~DasSarma, N.~Elhage, Z.~Hatfield{-}Dodds,
  D.~Hernandez, J.~Kernion, K.~Ndousse, C.~Olsson, D.~Amodei, T.~B. Brown,
  J.~Clark, S.~McCandlish, C.~Olah, and J.~Kaplan, ``A general language
  assistant as a laboratory for alignment,'' \emph{CoRR}, vol. abs/2112.00861,
  2021.

\bibitem{Wang-arxiv-2021-ERNIE}
S.~Wang, Y.~Sun, Y.~Xiang, Z.~Wu, S.~Ding, W.~Gong, S.~Feng, J.~Shang, Y.~Zhao,
  C.~Pang, J.~Liu, X.~Chen, Y.~Lu, W.~Liu, X.~Wang, Y.~Bai, Q.~Chen, L.~Zhao,
  S.~Li, P.~Sun, D.~Yu, Y.~Ma, H.~Tian, H.~Wu, T.~Wu, W.~Zeng, G.~Li, W.~Gao,
  and H.~Wang, ``{ERNIE} 3.0 titan: Exploring larger-scale knowledge enhanced
  pre-training for language understanding and generation,'' \emph{CoRR}, vol.
  abs/2112.12731, 2021.

\bibitem{Du-ICML-2022-GLaM}
N.~Du, Y.~Huang, A.~M. Dai, S.~Tong, D.~Lepikhin, Y.~Xu, M.~Krikun, Y.~Zhou,
  A.~W. Yu, O.~Firat, B.~Zoph, L.~Fedus, M.~P. Bosma, Z.~Zhou, T.~Wang, Y.~E.
  Wang, K.~Webster, M.~Pellat, K.~Robinson, K.~S. Meier{-}Hellstern, T.~Duke,
  L.~Dixon, K.~Zhang, Q.~V. Le, Y.~Wu, Z.~Chen, and C.~Cui, ``Glam: Efficient
  scaling of language models with mixture-of-experts,'' in \emph{International
  Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore,
  Maryland, {USA}}, 2022, pp. 5547--5569.

\bibitem{Smith-CoRR-2022-Using}
S.~Smith, M.~Patwary, B.~Norick, P.~LeGresley, S.~Rajbhandari, J.~Casper,
  Z.~Liu, S.~Prabhumoye, G.~Zerveas, V.~Korthikanti, E.~Zheng, R.~Child, R.~Y.
  Aminabadi, J.~Bernauer, X.~Song, M.~Shoeybi, Y.~He, M.~Houston, S.~Tiwary,
  and B.~Catanzaro, ``Using deepspeed and megatron to train megatron-turing
  {NLG} 530b, {A} large-scale generative language model,'' \emph{CoRR}, vol.
  abs/2201.11990, 2022.

\bibitem{Li-Science-2022-AlphaCode}
Y.~Li, D.~H. Choi, J.~Chung, N.~Kushman, J.~Schrittwieser, R.~Leblond,
  T.~Eccles, J.~Keeling, F.~Gimeno, A.~D. Lago, T.~Hubert, P.~Choy,
  C.~de~Masson~d'Autume, I.~Babuschkin, X.~Chen, P.~Huang, J.~Welbl, S.~Gowal,
  A.~Cherepanov, J.~Molloy, D.~J. Mankowitz, E.~S. Robson, P.~Kohli,
  N.~de~Freitas, K.~Kavukcuoglu, and O.~Vinyals, ``Competition-level code
  generation with alphacode,'' \emph{Science}, 2022.

\bibitem{Soltan-arxiv-2022-AlexaTM20B}
S.~Soltan, S.~Ananthakrishnan, J.~FitzGerald, R.~Gupta, W.~Hamza, H.~Khan,
  C.~Peris, S.~Rawls, A.~Rosenbaum, A.~Rumshisky, C.~S. Prakash, M.~Sridhar,
  F.~Triefenbach, A.~Verma, G.~T{\"{u}}r, and P.~Natarajan, ``Alexatm 20b:
  Few-shot learning using a large-scale multilingual seq2seq model,''
  \emph{CoRR}, vol. abs/2208.01448, 2022.

\bibitem{Glaese-arxiv-2022-Improving}
A.~Glaese, N.~McAleese, M.~Trebacz, J.~Aslanides, V.~Firoiu, T.~Ewalds,
  M.~Rauh, L.~Weidinger, M.~Chadwick, P.~Thacker, L.~Campbell{-}Gillingham,
  J.~Uesato, P.~Huang, R.~Comanescu, F.~Yang, A.~See, S.~Dathathri, R.~Greig,
  C.~Chen, D.~Fritz, J.~S. Elias, R.~Green, S.~Mokr{\'{a}}, N.~Fernando, B.~Wu,
  R.~Foley, S.~Young, I.~Gabriel, W.~Isaac, J.~Mellor, D.~Hassabis,
  K.~Kavukcuoglu, L.~A. Hendricks, and G.~Irving, ``Improving alignment of
  dialogue agents via targeted human judgements,'' \emph{CoRR}, vol.
  abs/2209.14375, 2022.

\bibitem{Su-arxiv-2022-WeLM}
H.~Su, X.~Zhou, H.~Yu, Y.~Chen, Z.~Zhu, Y.~Yu, and J.~Zhou, ``Welm: {A}
  well-read pre-trained language model for chinese,'' \emph{CoRR}, vol.
  abs/2209.10372, 2022.

\bibitem{Tay-arxiv-2022-Transcending}
Y.~Tay, J.~Wei, H.~W. Chung, V.~Q. Tran, D.~R. So, S.~Shakeri, X.~Garcia, H.~S.
  Zheng, J.~Rao, A.~Chowdhery, D.~Zhou, D.~Metzler, S.~Petrov, N.~Houlsby,
  Q.~V. Le, and M.~Dehghani, ``Transcending scaling laws with 0.1{\%} extra
  compute,'' \emph{CoRR}, vol. abs/2210.11399, 2022.

\bibitem{Ren-arXiv-2023-PanGusigma}
X.~Ren, P.~Zhou, X.~Meng, X.~Huang, Y.~Wang, W.~Wang, P.~Li, X.~Zhang,
  A.~Podolskiy, G.~Arshinov, A.~Bout, I.~Piontkovskaya, J.~Wei, X.~Jiang,
  T.~Su, Q.~Liu, and J.~Yao, ``Pangu-{\(\Sigma\)}: Towards trillion parameter
  language model with sparse heterogeneous computing,'' \emph{CoRR}, vol.
  abs/2303.10845, 2023.

\bibitem{Anil-arxiv-2023-palm2}
R.~Anil, A.~M. Dai, O.~Firat, M.~Johnson, D.~Lepikhin, A.~Passos, S.~Shakeri,
  E.~Taropa, P.~Bailey, Z.~Chen \emph{et~al.}, ``Palm 2 technical report,''
  \emph{arXiv preprint arXiv:2305.10403}, 2023.

\bibitem{Radford-CoRR-2017-Learning}
A.~Radford, R.~J{\'{o}}zefowicz, and I.~Sutskever, ``Learning to generate
  reviews and discovering sentiment,'' \emph{CoRR}, vol. abs/1704.01444, 2017.

\bibitem{radford-openai-2018-improving}
A.~Radford, K.~Narasimhan, T.~Salimans, I.~Sutskever \emph{et~al.}, ``Improving
  language understanding by generative pre-training,'' 2018.

\bibitem{McCann-CoRR-2018-The}
B.~McCann, N.~S. Keskar, C.~Xiong, and R.~Socher, ``The natural language
  decathlon: Multitask learning as question answering,'' \emph{CoRR}, vol.
  abs/1806.08730, 2018.

\bibitem{Zhang-ACL-2020-DIALOGPT}
Y.~Zhang, S.~Sun, M.~Galley, Y.~Chen, C.~Brockett, X.~Gao, J.~Gao, J.~Liu, and
  B.~Dolan, ``{DIALOGPT} : Large-scale generative pre-training for
  conversational response generation,'' in \emph{Proceedings of the 58th Annual
  Meeting of the Association for Computational Linguistics: System
  Demonstrations, {ACL} 2020, Online, July 5-10, 2020}, A.~Celikyilmaz and
  T.~Wen, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2020, pp. 270--278.

\bibitem{Ham-ACL-2020-End}
D.~Ham, J.~Lee, Y.~Jang, and K.~Kim, ``End-to-end neural pipeline for
  goal-oriented dialogue systems using {GPT-2},'' in \emph{Proceedings of the
  58th Annual Meeting of the Association for Computational Linguistics, {ACL}
  2020, Online, July 5-10, 2020}.\hskip 1em plus 0.5em minus 0.4em\relax
  Association for Computational Linguistics, 2020, pp. 583--592.

\bibitem{Drori-CoRR-2021-A}
I.~Drori, S.~Tran, R.~Wang, N.~Cheng, K.~Liu, L.~Tang, E.~Ke, N.~Singh, T.~L.
  Patti, J.~Lynch, A.~Shporer, N.~Verma, E.~Wu, and G.~Strang, ``A neural
  network solves and generates mathematics problems by program synthesis:
  Calculus, differential equations, linear algebra, and more,'' \emph{CoRR},
  vol. abs/2112.15594, 2021.

\bibitem{Neelakantan-CoRR-2022-Text}
A.~Neelakantan, T.~Xu, R.~Puri, A.~Radford, J.~M. Han, J.~Tworek, Q.~Yuan,
  N.~Tezak, J.~W. Kim, C.~Hallacy, J.~Heidecke, P.~Shyam, B.~Power, T.~E.
  Nekoul, G.~Sastry, G.~Krueger, D.~Schnurr, F.~P. Such, K.~Hsu, M.~Thompson,
  T.~Khan, T.~Sherbakov, J.~Jang, P.~Welinder, and L.~Weng, ``Text and code
  embeddings by contrastive pre-training,'' \emph{CoRR}, vol. abs/2201.10005,
  2022.

\bibitem{schulman-arxiv-2017-proximal}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov, ``Proximal
  policy optimization algorithms,'' \emph{arXiv preprint arXiv:1707.06347},
  2017.

\bibitem{Stiennon-arxiv-2020-learning}
N.~Stiennon, L.~Ouyang, J.~Wu, D.~M. Ziegler, R.~Lowe, C.~Voss, A.~Radford,
  D.~Amodei, and P.~F. Christiano, ``Learning to summarize from human
  feedback,'' \emph{CoRR}, vol. abs/2009.01325, 2020.

\bibitem{OpenAI-blog-2022-alignment}
OpenAI, ``Our approach to alignment research,'' \emph{OpenAI Blog}, August
  2022.

\bibitem{OpenAI-blog-2022-ChatGPT}
------, ``Introducing chatgpt,'' \emph{OpenAI Blog}, November 2022.

\bibitem{Ganguli-arxiv-2022-Red}
D.~Ganguli, L.~Lovitt, J.~Kernion, A.~Askell, Y.~Bai, S.~Kadavath, B.~Mann,
  E.~Perez, N.~Schiefer, K.~Ndousse, A.~Jones, S.~Bowman, A.~Chen, T.~Conerly,
  N.~DasSarma, D.~Drain, N.~Elhage, S.~E. Showk, S.~Fort, Z.~Hatfield{-}Dodds,
  T.~Henighan, D.~Hernandez, T.~Hume, J.~Jacobson, S.~Johnston, S.~Kravec,
  C.~Olsson, S.~Ringer, E.~Tran{-}Johnson, D.~Amodei, T.~Brown, N.~Joseph,
  S.~McCandlish, C.~Olah, J.~Kaplan, and J.~Clark, ``Red teaming language
  models to reduce harms: Methods, scaling behaviors, and lessons learned,''
  \emph{CoRR}, vol. abs/2209.07858, 2022.

\bibitem{OpenAI-OpenAI-2023-GPT-4v}
OpenAI, ``Gpt-4v(ision) system card,'' \emph{OpenAI}, 2023.

\bibitem{OpenAI-blog-2022-lessons}
------, ``Lessons learned on language model safety and misuse,'' \emph{OpenAI
  blog}, 2022.

\bibitem{Falcon40b}
E.~Almazrouei, H.~Alobeidli, A.~Alshamsi, A.~Cappelli, R.~Cojocaru, M.~Debbah,
  E.~Goffinet, D.~Heslow, J.~Launay, Q.~Malartic, B.~Noune, B.~Pannier, and
  G.~Penedo, ``{Falcon-40B}: an open large language model with state-of-the-art
  performance,'' 2023.

\bibitem{Huawei-Springer-2022-MindSpore}
L.~Huawei Technologies~Co., ``Huawei mindspore ai development framework,'' in
  \emph{Artificial Intelligence Technology}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, 2022, pp. 137--162.

\bibitem{alpaca}
R.~Taori, I.~Gulrajani, T.~Zhang, Y.~Dubois, X.~Li, C.~Guestrin, P.~Liang, and
  T.~B. Hashimoto, ``Stanford alpaca: An instruction-following llama model,''
  \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem{vicuna2023}
\BIBentryALTinterwordspacing
W.-L. Chiang, Z.~Li, Z.~Lin, Y.~Sheng, Z.~Wu, H.~Zhang, L.~Zheng, S.~Zhuang,
  Y.~Zhuang, J.~E. Gonzalez, I.~Stoica, and E.~P. Xing, ``Vicuna: An
  open-source chatbot impressing gpt-4 with 90\%* chatgpt quality,'' 2023.
  [Online]. Available: \url{https://vicuna.lmsys.org}
\BIBentrySTDinterwordspacing

\bibitem{ChatLLaMA}
\BIBentryALTinterwordspacing
2023. [Online]. Available:
  \url{https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama}
\BIBentrySTDinterwordspacing

\bibitem{ColossalChat}
\BIBentryALTinterwordspacing
Y.~You, ``Colossalchat: An open-source solution for cloning chatgpt with a
  complete rlhf pipeline,'' 2023. [Online]. Available:
  \url{https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b}
\BIBentrySTDinterwordspacing

\bibitem{Penedo-2023-arxiv-Refinedweb}
G.~Penedo, Q.~Malartic, D.~Hesslow, R.~Cojocaru, A.~Cappelli, H.~Alobeidli,
  B.~Pannier, E.~Almazrouei, and J.~Launay, ``The {R}efined{W}eb dataset for
  {F}alcon {LLM}: outperforming curated corpora with web data, and web data
  only,'' \emph{arXiv preprint arXiv:2306.01116}, 2023.

\bibitem{Taori-github-2023-Stanford}
R.~Taori, I.~Gulrajani, T.~Zhang, Y.~Dubois, X.~Li, C.~Guestrin, P.~Liang, and
  T.~B. Hashimoto, ``Stanford alpaca: An instruction-following llama model,''
  \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem{Wang-arXiv-2022-Self}
Y.~Wang, Y.~Kordi, S.~Mishra, A.~Liu, N.~A. Smith, D.~Khashabi, and
  H.~Hajishirzi, ``Self-instruct: Aligning language model with self generated
  instructions,'' \emph{CoRR}, vol. abs/2212.10560, 2022.

\bibitem{Alpaca-LoRA}
Alpaca-LoRA, ``Instruct-tune llama on consumer hardware,''
  \url{https://github.com/tloen/alpaca-lora}, 2023.

\bibitem{Hu-ICLR-2022-LoRA}
E.~J. Hu, Y.~Shen, P.~Wallis, Z.~Allen{-}Zhu, Y.~Li, S.~Wang, L.~Wang, and
  W.~Chen, ``Lora: Low-rank adaptation of large language models,'' in \emph{The
  Tenth International Conference on Learning Representations, {ICLR} 2022,
  Virtual Event, April 25-29, 2022}.\hskip 1em plus 0.5em minus 0.4em\relax
  OpenReview.net, 2022.

\bibitem{koala_blogpost_2023}
X.~Geng, A.~Gudibande, H.~Liu, E.~Wallace, P.~Abbeel, S.~Levine, and D.~Song,
  ``Koala: A dialogue model for academic research,'' Blog post, April 2023.

\bibitem{BELLE}
Y.~Ji, Y.~Deng, Y.~Gong, Y.~Peng, Q.~Niu, B.~Ma, and X.~Li, ``Belle: Be
  everyone's large language model engine,''
  \url{https://github.com/LianjiaTech/BELLE}, 2023.

\bibitem{ShareGPT}
D.~Eccleston, ``Sharegpt,'' \url{https://sharegpt.com/}, 2023.

\bibitem{Liu-arxiv-2023-Visual}
H.~Liu, C.~Li, Q.~Wu, and Y.~J. Lee, ``Visual instruction tuning,''
  \emph{CoRR}, vol. abs/2304.08485, 2023.

\bibitem{Zhu-arxiv-2023-MiniGPT-4}
D.~Zhu, J.~Chen, X.~Shen, X.~Li, and M.~Elhoseiny, ``Minigpt-4: Enhancing
  vision-language understanding with advanced large language models,''
  \emph{CoRR}, vol. abs/2304.10592, 2023.

\bibitem{Dai-2023-arxiv-InstructBLIP}
W.~Dai, J.~Li, D.~Li, A.~M.~H. Tiong, J.~Zhao, W.~Wang, B.~Li, P.~Fung, and
  S.~C.~H. Hoi, ``Instructblip: Towards general-purpose vision-language models
  with instruction tuning,'' \emph{CoRR}, vol. abs/2305.06500, 2023.

\bibitem{su-2023-arxiv-pandagpt}
Y.~Su, T.~Lan, H.~Li, J.~Xu, Y.~Wang, and D.~Cai, ``Pandagpt: One model to
  instruction-follow them all,'' 2023.

\bibitem{Zhu-ICCV-2015-Aligning}
Y.~Zhu, R.~Kiros, R.~S. Zemel, R.~Salakhutdinov, R.~Urtasun, A.~Torralba, and
  S.~Fidler, ``Aligning books and movies: Towards story-like visual
  explanations by watching movies and reading books,'' in \emph{2015 {IEEE}
  International Conference on Computer Vision, {ICCV} 2015, Santiago, Chile,
  December 7-13, 2015}.\hskip 1em plus 0.5em minus 0.4em\relax {IEEE} Computer
  Society, 2015, pp. 19--27.

\bibitem{Gutenberg}
\BIBentryALTinterwordspacing
``Project gutenberg.'' [Online]. Available: \url{https://www.gutenberg.org/}
\BIBentrySTDinterwordspacing

\bibitem{Trinh-CoRR-2018-A}
T.~H. Trinh and Q.~V. Le, ``A simple method for commonsense reasoning,''
  \emph{CoRR}, vol. abs/1806.02847, 2018.

\bibitem{Zellers-NeurIPS-2019-Defending}
R.~Zellers, A.~Holtzman, H.~Rashkin, Y.~Bisk, A.~Farhadi, F.~Roesner, and
  Y.~Choi, ``Defending against neural fake news,'' in \emph{Advances in Neural
  Information Processing Systems 32: Annual Conference on Neural Information
  Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
  Canada}, H.~M. Wallach, H.~Larochelle, A.~Beygelzimer,
  F.~d'Alch{\'{e}}{-}Buc, E.~B. Fox, and R.~Garnett, Eds., 2019, pp.
  9051--9062.

\bibitem{Gokaslan2019OpenWeb}
A.~Gokaslan, V.~C.~E. Pavlick, and S.~Tellex, ``Openwebtext corpus,''
  \url{http://Skylion007.github.io/OpenWebTextCorpus}, 2019.

\bibitem{Baumgartner-AAAI-2020-The}
J.~Baumgartner, S.~Zannettou, B.~Keegan, M.~Squire, and J.~Blackburn, ``The
  pushshift reddit dataset,'' in \emph{Proceedings of the Fourteenth
  International {AAAI} Conference on Web and Social Media, {ICWSM} 2020, Held
  Virtually, Original Venue: Atlanta, Georgia, USA, June 8-11, 2020}.\hskip 1em
  plus 0.5em minus 0.4em\relax {AAAI} Press, 2020, pp. 830--839.

\bibitem{Wikipedia}
\BIBentryALTinterwordspacing
``Wikipedia.'' [Online]. Available:
  \url{https://en.wikipedia.org/wiki/Main_Page}
\BIBentrySTDinterwordspacing

\bibitem{bigquery-google}
\BIBentryALTinterwordspacing
``Bigquery dataset.'' [Online]. Available:
  \url{https://cloud.google.com/bigquery?hl=zh-cn}
\BIBentrySTDinterwordspacing

\bibitem{Gao-arxiv-2021-Pile}
L.~Gao, S.~Biderman, S.~Black, L.~Golding, T.~Hoppe, C.~Foster, J.~Phang,
  H.~He, A.~Thite, N.~Nabeshima, S.~Presser, and C.~Leahy, ``The pile: An 800gb
  dataset of diverse text for language modeling,'' \emph{CoRR}, vol.
  abs/2101.00027, 2021.

\bibitem{Laurencon-NIPS-2022-The}
H.~Lauren{\c{c}}on, L.~Saulnier, T.~Wang, C.~Akiki, A.~V. del Moral,
  T.~Le~Scao, L.~Von~Werra, C.~Mou, E.~G. Ponferrada, H.~Nguyen \emph{et~al.},
  ``The bigscience roots corpus: A 1.6 tb composite multilingual dataset,'' in
  \emph{Thirty-sixth Conference on Neural Information Processing Systems
  Datasets and Benchmarks Track}, 2022.

\bibitem{commoncrawl}
\BIBentryALTinterwordspacing
``Common crawl.'' [Online]. Available: \url{https://commoncrawl.org/}
\BIBentrySTDinterwordspacing

\bibitem{CC-Stories-R}
\BIBentryALTinterwordspacing
``A reproduction version of cc-stories on hugging face.'' [Online]. Available:
  \url{https://huggingface.co/datasets/spacemanidol/cc-stories}
\BIBentrySTDinterwordspacing

\bibitem{Wang-GitHub-2021-GPT-J}
B.~Wang and A.~Komatsuzaki, ``{GPT-J-6B: A 6 Billion Parameter Autoregressive
  Language Model},'' \url{https://github.com/kingoflolz/mesh-transformer-jax},
  2021.

\bibitem{Mishra-ACL-2022-Cross}
S.~Mishra, D.~Khashabi, C.~Baral, and H.~Hajishirzi, ``Cross-task
  generalization via natural language crowdsourcing instructions,'' in
  \emph{Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin,
  Ireland, May 22-27, 2022}, S.~Muresan, P.~Nakov, and A.~Villavicencio, Eds.,
  2022, pp. 3470--3487.

\bibitem{Bach-ACL-2022-PromptSource}
S.~H. Bach, V.~Sanh, Z.~X. Yong, A.~Webson, C.~Raffel, N.~V. Nayak, A.~Sharma,
  T.~Kim, M.~S. Bari, T.~F{\'{e}}vry, Z.~Alyafeai, M.~Dey, A.~Santilli, Z.~Sun,
  S.~Ben{-}David, C.~Xu, G.~Chhablani, H.~Wang, J.~A. Fries, M.~S. AlShaibani,
  S.~Sharma, U.~Thakker, K.~Almubarak, X.~Tang, D.~R. Radev, M.~T. Jiang, and
  A.~M. Rush, ``Promptsource: An integrated development environment and
  repository for natural language prompts,'' in \emph{{ACL} (demo)}.\hskip 1em
  plus 0.5em minus 0.4em\relax Association for Computational Linguistics, 2022,
  pp. 93--104.

\bibitem{Tang-arxiv-2022-MVP}
T.~Tang, J.~Li, W.~X. Zhao, and J.~Wen, ``{MVP:} multi-task supervised
  pre-training for natural language generation,'' \emph{CoRR}, vol.
  abs/2206.12131, 2022.

\bibitem{Nguyen-laion-2023-The}
H.~Nguyen, S.~Suri, K.~Tsui, Shahules786, T.~team, and C.~Schuhmann, ``The oig
  dataset,'' \url{https://laion.ai/blog/oig-dataset/}, 2023.

\bibitem{Bai-arxiv-2022-Training}
\BIBentryALTinterwordspacing
Y.~Bai, A.~Jones, K.~Ndousse, A.~Askell, A.~Chen, N.~DasSarma, D.~Drain,
  S.~Fort, D.~Ganguli, T.~Henighan, N.~Joseph, S.~Kadavath, J.~Kernion,
  T.~Conerly, S.~E. Showk, N.~Elhage, Z.~Hatfield{-}Dodds, D.~Hernandez,
  T.~Hume, S.~Johnston, S.~Kravec, L.~Lovitt, N.~Nanda, C.~Olsson, D.~Amodei,
  T.~B. Brown, J.~Clark, S.~McCandlish, C.~Olah, B.~Mann, and J.~Kaplan,
  ``Training a helpful and harmless assistant with reinforcement learning from
  human feedback,'' \emph{CoRR}, vol. abs/2204.05862, 2022. [Online].
  Available: \url{https://doi.org/10.48550/arXiv.2204.05862}
\BIBentrySTDinterwordspacing

\bibitem{guo-arxiv-2023-how}
B.~Guo, X.~Zhang, Z.~Wang, M.~Jiang, J.~Nie, Y.~Ding, J.~Yue, and Y.~Wu, ``How
  close is chatgpt to human experts? comparison corpus, evaluation, and
  detection,'' \emph{arXiv preprint arXiv:2301.07597}, 2023.

\bibitem{Conover-2023-arxiv-Dolly}
M.~Conover, M.~Hayes, A.~Mathur, J.~Xie, J.~Wan, S.~Shah, A.~Ghodsi,
  P.~Wendell, M.~Zaharia, and R.~Xin. (2023) Free dolly: Introducing the
  world's first truly open instruction-tuned llm.

\bibitem{kopf-arxiv-2023-openassistant}
A.~K{\"o}pf, Y.~Kilcher, D.~von R{\"u}tte, S.~Anagnostidis, Z.-R. Tam,
  K.~Stevens, A.~Barhoum, N.~M. Duc, O.~Stanley, R.~Nagyfi \emph{et~al.},
  ``Openassistant conversations--democratizing large language model
  alignment,'' \emph{arXiv preprint arXiv:2304.07327}, 2023.

\bibitem{Cheung-2023-Guanaco}
J.~Cheung, ``Guanaco - generative universal assistant for natural-language
  adaptive context-aware omnilingual outputs,''
  \url{https://guanaco-model.github.io/}, 2023.

\bibitem{xu-arxiv-2023-baize}
C.~Xu, D.~Guo, N.~Duan, and J.~McAuley, ``Baize: An open-source chat model with
  parameter-efficient tuning on self-chat data,'' \emph{arXiv preprint
  arXiv:2304.01196}, 2023.

\bibitem{ji-arxiv-2023-towards}
Y.~Ji, Y.~Gong, Y.~Deng, Y.~Peng, Q.~Niu, B.~Ma, and X.~Li, ``Towards better
  instruction following language models for chinese: Investigating the impact
  of training data and evaluation,'' \emph{arXiv preprint arXiv:2304.07854},
  2023.

\bibitem{Ethayarajh-ICLM-2022-Understanding}
K.~Ethayarajh, Y.~Choi, and S.~Swayamdipta, ``Understanding dataset difficulty
  with $\mathcal{V}$-usable information,'' in \emph{Proceedings of the 39th
  International Conference on Machine Learning}, 2022, pp. 5988--6008.

\bibitem{Lambert-2023-StackH4}
\BIBentryALTinterwordspacing
N.~Lambert, L.~Tunstall, N.~Rajani, and T.~Thrush. (2023) Huggingface h4 stack
  exchange preference dataset. [Online]. Available:
  \url{https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences}
\BIBentrySTDinterwordspacing

\bibitem{Liu-arxiv-2023-training}
R.~Liu, R.~Yang, C.~Jia, G.~Zhang, D.~Zhou, A.~M. Dai, D.~Yang, and
  S.~Vosoughi, ``Training socially aligned language models in simulated human
  society,'' \emph{CoRR}, vol. abs/2305.16960, 2023.

\bibitem{Xu-2023-arxiv-CValues}
G.~Xu, J.~Liu, M.~Yan, H.~Xu, J.~Si, Z.~Zhou, P.~Yi, X.~Gao, J.~Sang, R.~Zhang,
  J.~Zhang, C.~Peng, F.~Huang, and J.~Zhou, ``Cvalues: Measuring the values of
  chinese large language models from safety to responsibility,'' 2023.

\bibitem{Dai-arxiv-2023-SafeRLHF}
J.~Dai, X.~Pan, R.~Sun, J.~Ji, X.~Xu, M.~Liu, Y.~Wang, and Y.~Yang, ``Safe
  rlhf: Safe reinforcement learning from human feedback,'' \emph{arXiv preprint
  arXiv:2310.12773}, 2023.

\bibitem{Sanh-2022-ICLR-P3}
V.~Sanh, A.~Webson, C.~Raffel, S.~H. Bach, L.~Sutawika, Z.~Alyafeai,
  A.~Chaffin, A.~Stiegler, A.~Raja, M.~Dey, M.~S. Bari, C.~Xu, U.~Thakker,
  S.~S. Sharma, E.~Szczechla, T.~Kim, G.~Chhablani, N.~V. Nayak, D.~Datta,
  J.~Chang, M.~T. Jiang, H.~Wang, M.~Manica, S.~Shen, Z.~X. Yong, H.~Pandey,
  R.~Bawden, T.~Wang, T.~Neeraj, J.~Rozen, A.~Sharma, A.~Santilli,
  T.~F{\'{e}}vry, J.~A. Fries, R.~Teehan, T.~L. Scao, S.~Biderman, L.~Gao,
  T.~Wolf, and A.~M. Rush, ``Multitask prompted training enables zero-shot task
  generalization,'' in \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.\hskip 1em
  plus 0.5em minus 0.4em\relax OpenReview.net, 2022.

\bibitem{Longpre-2023-arxiv-Flan_v2}
S.~Longpre, L.~Hou, T.~Vu, A.~Webson, H.~W. Chung, Y.~Tay, D.~Zhou, Q.~V. Le,
  B.~Zoph, J.~Wei \emph{et~al.}, ``The flan collection: Designing data and
  methods for effective instruction tuning,'' \emph{arXiv preprint
  arXiv:2301.13688}, 2023.

\bibitem{Cobbe-arxiv-2021-Training}
K.~Cobbe, V.~Kosaraju, M.~Bavarian, J.~Hilton, R.~Nakano, C.~Hesse, and
  J.~Schulman, ``Training verifiers to solve math word problems,'' \emph{CoRR},
  vol. abs/2110.14168, 2021.

\bibitem{Geva-tacl-2021-Did}
M.~Geva, D.~Khashabi, E.~Segal, T.~Khot, D.~Roth, and J.~Berant, ``Did
  aristotle use a laptop? {A} question answering benchmark with implicit
  reasoning strategies,'' \emph{Trans. Assoc. Comput. Linguistics}, vol.~9, pp.
  346--361, 2021.

\bibitem{Camburu-2020-ACL-Make}
O.~Camburu, B.~Shillingford, P.~Minervini, T.~Lukasiewicz, and P.~Blunsom,
  ``Make up your mind! adversarial generation of inconsistent natural language
  explanations,'' in \emph{Proceedings of the 58th Annual Meeting of the
  Association for Computational Linguistics, {ACL} 2020, Online, July 5-10,
  2020}, D.~Jurafsky, J.~Chai, N.~Schluter, and J.~R. Tetreault, Eds.\hskip 1em
  plus 0.5em minus 0.4em\relax Association for Computational Linguistics, 2020,
  pp. 4157--4165.

\bibitem{Wolf-EMNLP-2020-Transformers}
T.~Wolf, L.~Debut, V.~Sanh, J.~Chaumond, C.~Delangue, A.~Moi, P.~Cistac,
  T.~Rault, R.~Louf, M.~Funtowicz, J.~Davison, S.~Shleifer, P.~von Platen,
  C.~Ma, Y.~Jernite, J.~Plu, C.~Xu, T.~L. Scao, S.~Gugger, M.~Drame, Q.~Lhoest,
  and A.~M. Rush, ``Transformers: State-of-the-art natural language
  processing,'' in \emph{Proceedings of the 2020 Conference on Empirical
  Methods in Natural Language Processing: System Demonstrations, {EMNLP} 2020 -
  Demos, Online, November 16-20, 2020}.\hskip 1em plus 0.5em minus 0.4em\relax
  Association for Computational Linguistics, 2020, pp. 38--45.

\bibitem{Bradbury-github-2018-jax}
\BIBentryALTinterwordspacing
J.~Bradbury, R.~Frostig, P.~Hawkins, M.~J. Johnson, C.~Leary, D.~Maclaurin,
  G.~Necula, A.~Paszke, J.~Vander{P}las, S.~Wanderman-{M}ilne, and Q.~Zhang,
  ``{JAX}: composable transformations of {P}ython+{N}um{P}y programs,'' 2018.
  [Online]. Available: \url{http://github.com/google/jax}
\BIBentrySTDinterwordspacing

\bibitem{Bian-CoRR-2021-Colossal-AI}
Z.~Bian, H.~Liu, B.~Wang, H.~Huang, Y.~Li, C.~Wang, F.~Cui, and Y.~You,
  ``Colossal-ai: {A} unified deep learning system for large-scale parallel
  training,'' \emph{CoRR}, vol. abs/2110.14883, 2021.

\bibitem{Fang-arxiv-2021-PatrickStar}
J.~Fang, Y.~Yu, S.~Li, Y.~You, and J.~Zhou, ``Patrickstar: Parallel training of
  pre-trained models via a chunk-based memory management,'' \emph{CoRR}, vol.
  abs/2108.05818, 2021.

\bibitem{BMTrain}
\BIBentryALTinterwordspacing
``Bmtrain: Effient training for big models.'' [Online]. Available:
  \url{https://github.com/OpenBMB/BMTrain}
\BIBentrySTDinterwordspacing

\bibitem{He-arXiv-2021-FastMoE}
J.~He, J.~Qiu, A.~Zeng, Z.~Yang, J.~Zhai, and J.~Tang, ``Fastmoe: {A} fast
  mixture-of-expert training system,'' \emph{CoRR}, vol. abs/2103.13262, 2021.

\bibitem{kwon-2023-SIGOPS-efficient}
W.~Kwon, Z.~Li, S.~Zhuang, Y.~Sheng, L.~Zheng, C.~H. Yu, J.~E. Gonzalez,
  H.~Zhang, and I.~Stoica, ``Efficient memory management for large language
  model serving with pagedattention,'' in \emph{Proceedings of the ACM SIGOPS
  29th Symposium on Operating Systems Principles}, 2023.

\bibitem{DeepSpeed-MII}
\BIBentryALTinterwordspacing
(2023) Deepspeed-mii. [Online]. Available:
  \url{https://github.com/microsoft/DeepSpeed-MII}
\BIBentrySTDinterwordspacing

\bibitem{jiang-2023-arxiv-mistral}
A.~Q. Jiang, A.~Sablayrolles, A.~Mensch, C.~Bamford, D.~S. Chaplot, D.~de~las
  Casas, F.~Bressand, G.~Lengyel, G.~Lample, L.~Saulnier, L.~R. Lavaud, M.-A.
  Lachaux, P.~Stock, T.~L. Scao, T.~Lavril, T.~Wang, T.~Lacroix, and W.~E.
  Sayed, ``Mistral 7b,'' 2023.

\bibitem{yao-2023-arxiv-dschat}
Z.~Yao, R.~Y. Aminabadi, O.~Ruwase, S.~Rajbhandari, X.~Wu, A.~A. Awan,
  J.~Rasley, M.~Zhang, C.~Li, C.~Holmes, Z.~Zhou, M.~Wyatt, M.~Smith,
  L.~Kurilenko, H.~Qin, M.~Tanaka, S.~Che, S.~L. Song, and Y.~He,
  ``{DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like
  Models at All Scales},'' \emph{arXiv preprint arXiv:2308.01320}, 2023.

\bibitem{Paszke-NeurIPS-2019-Pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~K{\"{o}}pf, E.~Z. Yang,
  Z.~DeVito, M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang,
  J.~Bai, and S.~Chintala, ``Pytorch: An imperative style, high-performance
  deep learning library,'' in \emph{Advances in Neural Information Processing
  Systems 32: Annual Conference on Neural Information Processing Systems 2019,
  NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada}, H.~M. Wallach,
  H.~Larochelle, A.~Beygelzimer, F.~d'Alch{\'{e}}{-}Buc, E.~B. Fox, and
  R.~Garnett, Eds., 2019, pp. 8024--8035.

\bibitem{Abadi-OSDI-2016-TensorFlow}
M.~Abadi, P.~Barham, J.~Chen, Z.~Chen, A.~Davis, J.~Dean, M.~Devin,
  S.~Ghemawat, G.~Irving, M.~Isard, M.~Kudlur, J.~Levenberg, R.~Monga,
  S.~Moore, D.~G. Murray, B.~Steiner, P.~A. Tucker, V.~Vasudevan, P.~Warden,
  M.~Wicke, Y.~Yu, and X.~Zheng, ``Tensorflow: {A} system for large-scale
  machine learning,'' in \emph{12th {USENIX} Symposium on Operating Systems
  Design and Implementation, {OSDI} 2016, Savannah, GA, USA, November 2-4,
  2016}, K.~Keeton and T.~Roscoe, Eds.\hskip 1em plus 0.5em minus 0.4em\relax
  {USENIX} Association, 2016, pp. 265--283.

\bibitem{Chen-arxiv-2015-MXNet}
T.~Chen, M.~Li, Y.~Li, M.~Lin, N.~Wang, M.~Wang, T.~Xiao, B.~Xu, C.~Zhang, and
  Z.~Zhang, ``Mxnet: {A} flexible and efficient machine learning library for
  heterogeneous distributed systems,'' \emph{CoRR}, vol. abs/1512.01274, 2015.

\bibitem{Ma-fodc-2019-PaddlePaddle}
Y.~Ma, D.~Yu, T.~Wu, and H.~Wang, ``Paddlepaddle: An open-source deep learning
  platform from industrial practice,'' \emph{Frontiers of Data and Domputing},
  vol.~1, no.~1, p. 105, 2019.

\bibitem{Yuan-arXiv-2021-OneFlow}
J.~Yuan, X.~Li, C.~Cheng, J.~Liu, R.~Guo, S.~Cai, C.~Yao, F.~Yang, X.~Yi,
  C.~Wu, H.~Zhang, and J.~Zhao, ``Oneflow: Redesign the distributed deep
  learning framework from scratch,'' \emph{CoRR}, vol. abs/2110.15032, 2021.

\bibitem{Roller-ACL-2021-Recipes}
S.~Roller, E.~Dinan, N.~Goyal, D.~Ju, M.~Williamson, Y.~Liu, J.~Xu, M.~Ott,
  E.~M. Smith, Y.~Boureau, and J.~Weston, ``Recipes for building an open-domain
  chatbot,'' in \emph{Proceedings of the 16th Conference of the European
  Chapter of the Association for Computational Linguistics: Main Volume, {EACL}
  2021, Online, April 19 - 23, 2021}, 2021, pp. 300--325.

\bibitem{Lewkowycz-arxiv-2022-Solving}
A.~Lewkowycz, A.~Andreassen, D.~Dohan, E.~Dyer, H.~Michalewski, V.~V. Ramasesh,
  A.~Slone, C.~Anil, I.~Schlag, T.~Gutman{-}Solo, Y.~Wu, B.~Neyshabur,
  G.~Gur{-}Ari, and V.~Misra, ``Solving quantitative reasoning problems with
  language models,'' \emph{CoRR}, vol. abs/2206.14858, 2022.

\bibitem{Saier-arxiv-2023-unarXive}
T.~Saier, J.~Krause, and M.~F{\"a}rber, ``unarxive 2022: All arxiv publications
  pre-processed for nlp, including structured full-text and citation network,''
  \emph{arXiv preprint arXiv:2303.14957}, 2023.

\bibitem{Simon-JACM-1963-Experiments}
H.~A. Simon, ``Experiments with a heuristic compiler,'' \emph{J. {ACM}},
  vol.~10, no.~4, pp. 493--506, 1963.

\bibitem{Manna-CommunACM-1971-Toward}
Z.~Manna and R.~J. Waldinger, ``Toward automatic program synthesis,''
  \emph{Commun. {ACM}}, vol.~14, no.~3, pp. 151--165, 1971.

\bibitem{Feng-EMNLPFindings-2020-CodeBERT}
Z.~Feng, D.~Guo, D.~Tang, N.~Duan, X.~Feng, M.~Gong, L.~Shou, B.~Qin, T.~Liu,
  D.~Jiang, and M.~Zhou, ``Codebert: {A} pre-trained model for programming and
  natural languages,'' in \emph{Findings of {EMNLP}}, 2020.

\bibitem{Austin-arxiv-2021-Program}
J.~Austin, A.~Odena, M.~I. Nye, M.~Bosma, H.~Michalewski, D.~Dohan, E.~Jiang,
  C.~J. Cai, M.~Terry, Q.~V. Le, and C.~Sutton, ``Program synthesis with large
  language models,'' \emph{CoRR}, vol. abs/2108.07732, 2021.

\bibitem{Black-GitHub-2021-GPT-Neo}
S.~Black, L.~Gao, P.~Wang, C.~Leahy, and S.~Biderman, ``{GPT-Neo: Large Scale
  Autoregressive Language Modeling with Mesh-Tensorflow},'' 2021.

\bibitem{Xu-SIGPLAN-2022-Systematic}
F.~F. Xu, U.~Alon, G.~Neubig, and V.~J. Hellendoorn, ``A systematic evaluation
  of large language models of code,'' in \emph{{MAPS@PLDI}}, 2022.

\bibitem{Madaan-emnlp-2022-Language}
A.~Madaan, S.~Zhou, U.~Alon, Y.~Yang, and G.~Neubig, ``Language models of code
  are few-shot commonsense learners,'' in \emph{Proceedings of the 2022
  Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2022,
  Abu Dhabi, United Arab Emirates, December 7-11, 2022}, Y.~Goldberg,
  Z.~Kozareva, and Y.~Zhang, Eds.\hskip 1em plus 0.5em minus 0.4em\relax
  Association for Computational Linguistics, 2022, pp. 1384--1403.

\bibitem{Longpre-arxiv-2023-pretrainer}
S.~Longpre, G.~Yauney, E.~Reif, K.~Lee, A.~Roberts, B.~Zoph, D.~Zhou, J.~Wei,
  K.~Robinson, D.~Mimno \emph{et~al.}, ``A pretrainer's guide to training data:
  Measuring the effects of data age, domain coverage, quality, \& toxicity,''
  \emph{arXiv preprint arXiv:2305.13169}, 2023.

\bibitem{Chen-2023-arxiv-Data}
D.~Chen, Y.~Huang, Z.~Ma, H.~Chen, X.~Pan, C.~Ge, D.~Gao, Y.~Xie, Z.~Liu,
  J.~Gao, Y.~Li, B.~Ding, and J.~Zhou, ``Data-juicer: A one-stop data
  processing system for large language models,'' 2023.

\bibitem{Hernandez-arxiv-2022-Scaling}
D.~Hernandez, T.~B. Brown, T.~Conerly, N.~DasSarma, D.~Drain, S.~E. Showk,
  N.~Elhage, Z.~Hatfield{-}Dodds, T.~Henighan, T.~Hume, S.~Johnston, B.~Mann,
  C.~Olah, C.~Olsson, D.~Amodei, N.~Joseph, J.~Kaplan, and S.~McCandlish,
  ``Scaling laws and interpretability of learning from repeated data,''
  \emph{CoRR}, vol. abs/2205.10487, 2022.

\bibitem{Holtzman-2019-ICLR-The}
A.~Holtzman, J.~Buys, L.~Du, M.~Forbes, and Y.~Choi, ``The curious case of
  neural text degeneration,'' in \emph{8th International Conference on Learning
  Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30,
  2020}.\hskip 1em plus 0.5em minus 0.4em\relax OpenReview.net, 2020.

\bibitem{Lee-ACL-2022-Deduplicating}
K.~Lee, D.~Ippolito, A.~Nystrom, C.~Zhang, D.~Eck, C.~Callison{-}Burch, and
  N.~Carlini, ``Deduplicating training data makes language models better,'' in
  \emph{Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin,
  Ireland, May 22-27, 2022}, 2022, pp. 8424--8445.

\bibitem{Carlini-arxiv-2022-Quantifying}
N.~Carlini, D.~Ippolito, M.~Jagielski, K.~Lee, F.~Tram{\`{e}}r, and C.~Zhang,
  ``Quantifying memorization across neural language models,'' \emph{CoRR},
  2022.

\bibitem{Carlini-USENIX-2021-Extracting}
N.~Carlini, F.~Tram{\`{e}}r, E.~Wallace, M.~Jagielski, A.~Herbert{-}Voss,
  K.~Lee, A.~Roberts, T.~B. Brown, D.~Song, {\'{U}}.~Erlingsson, A.~Oprea, and
  C.~Raffel, ``Extracting training data from large language models,'' in
  \emph{30th {USENIX} Security Symposium, {USENIX} Security 2021, August 11-13,
  2021}, 2021, pp. 2633--2650.

\bibitem{Kandpal-ICML-2022-Deduplicating}
N.~Kandpal, E.~Wallace, and C.~Raffel, ``Deduplicating training data mitigates
  privacy risks in language models,'' in \emph{International Conference on
  Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland,
  {USA}}.\hskip 1em plus 0.5em minus 0.4em\relax {PMLR}, 2022, pp.
  10\,697--10\,707.

\bibitem{Lafferty-ICML-2001}
J.~D. Lafferty, A.~McCallum, and F.~C.~N. Pereira, ``Conditional random fields:
  Probabilistic models for segmenting and labeling sequence data,'' in
  \emph{Proceedings of the Eighteenth International Conference on Machine
  Learning {(ICML} 2001), Williams College, Williamstown, MA, USA, June 28 -
  July 1, 2001}, C.~E. Brodley and A.~P. Danyluk, Eds.\hskip 1em plus 0.5em
  minus 0.4em\relax Morgan Kaufmann, 2001, pp. 282--289.

\bibitem{Philip-1994-BPE}
P.~Gage, ``A new algorithm for data compression,'' \emph{C Users Journal},
  vol.~12, no.~2, pp. 23--38, 1994.

\bibitem{Sennrich-ACL-2016-nueral}
R.~Sennrich, B.~Haddow, and A.~Birch, ``Neural machine translation of rare
  words with subword units,'' in \emph{Proceedings of the 54th Annual Meeting
  of the Association for Computational Linguistics, {ACL} 2016, August 7-12,
  2016, Berlin, Germany, Volume 1: Long Papers}.\hskip 1em plus 0.5em minus
  0.4em\relax The Association for Computer Linguistics, 2016.

\bibitem{Mike-ICASSP-2012-Japanese}
M.~Schuster and K.~Nakajima, ``Japanese and korean voice search,'' in
  \emph{2012 IEEE international conference on acoustics, speech and signal
  processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2012, pp.
  5149--5152.

\bibitem{Wu-CoRR-2016}
Y.~Wu, M.~Schuster, Z.~Chen, Q.~V. Le, M.~Norouzi, W.~Macherey, M.~Krikun,
  Y.~Cao, Q.~Gao, K.~Macherey, J.~Klingner, A.~Shah, M.~Johnson, X.~Liu,
  L.~Kaiser, S.~Gouws, Y.~Kato, T.~Kudo, H.~Kazawa, K.~Stevens, G.~Kurian,
  N.~Patil, W.~Wang, C.~Young, J.~Smith, J.~Riesa, A.~Rudnick, O.~Vinyals,
  G.~Corrado, M.~Hughes, and J.~Dean, ``Google's neural machine translation
  system: Bridging the gap between human and machine translation,''
  \emph{CoRR}, vol. abs/1609.08144, 2016.

\bibitem{Kudo-ACL-2018-Subword}
T.~Kudo, ``Subword regularization: Improving neural network translation models
  with multiple subword candidates,'' in \emph{Proceedings of the 56th Annual
  Meeting of the Association for Computational Linguistics, {ACL} 2018,
  Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers}, I.~Gurevych
  and Y.~Miyao, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2018, pp. 66--75.

\bibitem{Kudo-EMNLP-2018-SentencePiece}
T.~Kudo and J.~Richardson, ``Sentencepiece: {A} simple and language independent
  subword tokenizer and detokenizer for neural text processing,'' in
  \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural
  Language Processing, {EMNLP} 2018: System Demonstrations, Brussels, Belgium,
  October 31 - November 4, 2018}, E.~Blanco and W.~Lu, Eds.\hskip 1em plus
  0.5em minus 0.4em\relax Association for Computational Linguistics, 2018.

\bibitem{Davis-arxiv-2001-Unicode}
M.~Davis and M.~D{\"u}rst, ``Unicode normalization forms,'' 2001.

\bibitem{Nakkiran-ICLR-2020-Deep}
P.~Nakkiran, G.~Kaplun, Y.~Bansal, T.~Yang, B.~Barak, and I.~Sutskever, ``Deep
  double descent: Where bigger models and more data hurt,'' in \emph{8th
  International Conference on Learning Representations, {ICLR} 2020, Addis
  Ababa, Ethiopia, April 26-30, 2020}.\hskip 1em plus 0.5em minus 0.4em\relax
  OpenReview.net, 2020.

\bibitem{Tirumala-2023-arXiv-D4}
K.~Tirumala, D.~Simig, A.~Aghajanyan, and A.~S. Morcos, ``D4: Improving llm
  pretraining via document de-duplication and diversification,'' \emph{arXiv
  preprint arXiv:2308.12284}, 2023.

\bibitem{Shen-2023-arXiv-SlimPajamaDC}
Z.~Shen, T.~Tao, L.~Ma, W.~Neiswanger, J.~Hestness, N.~Vassilieva, D.~Soboleva,
  and E.~Xing, ``Slimpajama-dc: Understanding data combinations for llm
  training,'' \emph{arXiv preprint arXiv:2309.10818}, 2023.

\bibitem{Xie-arxiv-2023-DSIR}
S.~M. Xie, S.~Santurkar, T.~Ma, and P.~Liang, ``Data selection for language
  models via importance resampling,'' \emph{arXiv preprint arXiv:2302.03169},
  2023.

\bibitem{Wang-2023-arXiv-farewell}
X.~Wang, W.~Zhou, Q.~Zhang, J.~Zhou, S.~Gao, J.~Wang, M.~Zhang, X.~Gao,
  Y.~Chen, and T.~Gui, ``Farewell to aimless large-scale pretraining:
  Influential subset selection for language model,'' \emph{arXiv preprint
  arXiv:2305.12816}, 2023.

\bibitem{Paperno-ACL-2016-LAMBADA}
D.~Paperno, G.~Kruszewski, A.~Lazaridou, Q.~N. Pham, R.~Bernardi, S.~Pezzelle,
  M.~Baroni, G.~Boleda, and R.~Fern{\'{a}}ndez, ``The {LAMBADA} dataset: Word
  prediction requiring a broad discourse context,'' in \emph{{ACL}
  {(1)}}.\hskip 1em plus 0.5em minus 0.4em\relax The Association for Computer
  Linguistics, 2016.

\bibitem{Chen-2023-arXiv-skill}
M.~F. Chen, N.~Roberts, K.~Bhatia, J.~Wang, C.~Zhang, F.~Sala, and C.~R{\'e},
  ``Skill-it! a data-driven skills framework for understanding and training
  language models,'' \emph{arXiv preprint arXiv:2307.14430}, 2023.

\bibitem{Roziere-arxiv-2023-codellama}
B.~Rozi{\`{e}}re, J.~Gehring, F.~Gloeckle, S.~Sootla, I.~Gat, X.~E. Tan,
  Y.~Adi, J.~Liu, T.~Remez, J.~Rapin, A.~Kozhevnikov, I.~Evtimov, J.~Bitton,
  M.~Bhatt, C.~Canton{-}Ferrer, A.~Grattafiori, W.~Xiong, A.~D{\'{e}}fossez,
  J.~Copet, F.~Azhar, H.~Touvron, L.~Martin, N.~Usunier, T.~Scialom, and
  G.~Synnaeve, ``Code llama: Open foundation models for code,'' \emph{CoRR},
  vol. abs/2308.12950, 2023.

\bibitem{Bengio-2009-arXiv-curriculum}
Y.~Bengio, J.~Louradour, R.~Collobert, and J.~Weston, ``Curriculum learning,''
  in \emph{{ICML}}, 2009, pp. 41--48.

\bibitem{Xu-2023-arXiv-contrastive}
C.~Xu, C.~Rosset, L.~Del~Corro, S.~Mahajan, J.~McAuley, J.~Neville, A.~H.
  Awadallah, and N.~Rao, ``Contrastive post-training large language models on
  data curriculum,'' \emph{arXiv preprint arXiv:2310.02263}, 2023.

\bibitem{Tworkowski-arxiv-2023-Focused}
S.~Tworkowski, K.~Staniszewski, M.~Pacek, Y.~Wu, H.~Michalewski, and P.~Milos,
  ``Focused transformer: Contrastive training for context scaling,''
  \emph{CoRR}, vol. abs/2307.03170, 2023.

\bibitem{Azerbayev-arxiv-2023-llemma}
Z.~Azerbayev, H.~Schoelkopf, K.~Paster, M.~D. Santos, S.~McAleer, A.~Q. Jiang,
  J.~Deng, S.~Biderman, and S.~Welleck, ``Llemma: An open language model for
  mathematics,'' \emph{arXiv preprint arXiv:2310.10631}, 2023.

\bibitem{Chen-arxiv-2023-Extending}
S.~Chen, S.~Wong, L.~Chen, and Y.~Tian, ``Extending context window of large
  language models via positional interpolation,'' \emph{CoRR}, vol.
  abs/2306.15595, 2023.

\bibitem{Wenzek-2020-LREC-CCNet}
G.~Wenzek, M.-A. Lachaux, A.~Conneau, V.~Chaudhary, F.~Guzm{\'a}n, A.~Joulin,
  and {\'E}.~Grave, ``Ccnet: Extracting high quality monolingual datasets from
  web crawl data,'' in \emph{Proceedings of the Twelfth Language Resources and
  Evaluation Conference}, 2020, pp. 4003--4012.

\bibitem{Joulin-2017-EACL-fasttext}
A.~Joulin, E.~Grave, P.~Bojanowski, and T.~Mikolov, ``Bag of tricks for
  efficient text classification,'' in \emph{{EACL}}, 2017, pp. 427--431.

\bibitem{chen-2023-arXiv-DataJuicer}
D.~Chen, Y.~Huang, Z.~Ma, H.~Chen, X.~Pan, C.~Ge, D.~Gao, Y.~Xie, Z.~Liu,
  J.~Gao \emph{et~al.}, ``Data-juicer: A one-stop data processing system for
  large language models,'' \emph{arXiv preprint arXiv:2309.02033}, 2023.

\bibitem{Zhang-ICML-2022-Examining}
B.~Zhang, B.~Ghorbani, A.~Bapna, Y.~Cheng, X.~Garcia, J.~Shen, and O.~Firat,
  ``Examining scaling and transfer of language model architectures for machine
  translation,'' in \emph{International Conference on Machine Learning, {ICML}
  2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, 2022, pp.
  26\,176--26\,192.

\bibitem{Dong-NIPS-2019-Unified}
L.~Dong, N.~Yang, W.~Wang, F.~Wei, X.~Liu, Y.~Wang, J.~Gao, M.~Zhou, and
  H.~Hon, ``Unified language model pre-training for natural language
  understanding and generation,'' in \emph{Advances in Neural Information
  Processing Systems 32: Annual Conference on Neural Information Processing
  Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada},
  2019, pp. 13\,042--13\,054.

\bibitem{Clark-ICML-2022-Unified}
A.~Clark, D.~de~Las~Casas, A.~Guy, A.~Mensch, M.~Paganini, J.~Hoffmann,
  B.~Damoc, B.~A. Hechtman, T.~Cai, S.~Borgeaud, G.~van~den Driessche,
  E.~Rutherford, T.~Hennigan, M.~J. Johnson, A.~Cassirer, C.~Jones,
  E.~Buchatskaya, D.~Budden, L.~Sifre, S.~Osindero, O.~Vinyals, M.~Ranzato,
  J.~W. Rae, E.~Elsen, K.~Kavukcuoglu, and K.~Simonyan, ``Unified scaling laws
  for routed language models,'' in \emph{International Conference on Machine
  Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, 2022,
  pp. 4057--4086.

\bibitem{gu-2022-iclr-efficiently}
\BIBentryALTinterwordspacing
A.~Gu, K.~Goel, and C.~R{\'{e}}, ``Efficiently modeling long sequences with
  structured state spaces,'' in \emph{The Tenth International Conference on
  Learning Representations, {ICLR} 2022, Virtual Event, April 25-29,
  2022}.\hskip 1em plus 0.5em minus 0.4em\relax OpenReview.net, 2022. [Online].
  Available: \url{https://openreview.net/forum?id=uYLFoz1vlAC}
\BIBentrySTDinterwordspacing

\bibitem{Mehta-2022-arxiv-long}
\BIBentryALTinterwordspacing
H.~Mehta, A.~Gupta, A.~Cutkosky, and B.~Neyshabur, ``Long range language
  modeling via gated state spaces,'' \emph{CoRR}, vol. abs/2206.13947, 2022.
  [Online]. Available: \url{https://doi.org/10.48550/arXiv.2206.13947}
\BIBentrySTDinterwordspacing

\bibitem{dao-2022-arxiv-hungry}
\BIBentryALTinterwordspacing
T.~Dao, D.~Y. Fu, K.~K. Saab, A.~W. Thomas, A.~Rudra, and C.~R{\'{e}}, ``Hungry
  hungry hippos: Towards language modeling with state space models,''
  \emph{CoRR}, vol. abs/2212.14052, 2022. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2212.14052}
\BIBentrySTDinterwordspacing

\bibitem{poli-2023-icml-hyena}
M.~Poli, S.~Massaroli, E.~Nguyen, D.~Y. Fu, T.~Dao, S.~Baccus, Y.~Bengio,
  S.~Ermon, and C.~R{\'e}, ``Hyena hierarchy: Towards larger convolutional
  language models,'' in \emph{{ICML}}, 2023.

\bibitem{peng-2023-arxiv-rwkv}
\BIBentryALTinterwordspacing
B.~Peng, E.~Alcaide, Q.~Anthony, A.~Albalak, S.~Arcadinho, H.~Cao, X.~Cheng,
  M.~Chung, M.~Grella, K.~K.~G. V., X.~He, H.~Hou, P.~Kazienko, J.~Kocon,
  J.~Kong, B.~Koptyra, H.~Lau, K.~S.~I. Mantri, F.~Mom, A.~Saito, X.~Tang,
  B.~Wang, J.~S. Wind, S.~Wozniak, R.~Zhang, Z.~Zhang, Q.~Zhao, P.~Zhou,
  J.~Zhu, and R.~Zhu, ``{RWKV:} reinventing rnns for the transformer era,''
  \emph{CoRR}, vol. abs/2305.13048, 2023. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2305.13048}
\BIBentrySTDinterwordspacing

\bibitem{sun-2023-arxiv-retnet}
Y.~Sun, L.~Dong, S.~Huang, S.~Ma, Y.~Xia, J.~Xue, J.~Wang, and F.~Wei,
  ``Retentive network: A successor to transformer for large language models,''
  \emph{arXiv preprint arXiv:2307.08621}, 2023.

\bibitem{smith-2023-iclr-s5}
J.~T. Smith, A.~Warrington, and S.~Linderman, ``Simplified state space layers
  for sequence modeling,'' in \emph{{ICLR}}, 2023.

\bibitem{orvieto-2023-icml-lru}
A.~Orvieto, S.~L. Smith, A.~Gu, A.~Fernando, C.~Gulcehre, R.~Pascanu, and
  S.~De, ``Resurrecting recurrent neural networks for long sequences,'' in
  \emph{{ICML}}, 2023.

\bibitem{Ding-NIPS-2021-CogView}
M.~Ding, Z.~Yang, W.~Hong, W.~Zheng, C.~Zhou, D.~Yin, J.~Lin, X.~Zou, Z.~Shao,
  H.~Yang, and J.~Tang, ``Cogview: Mastering text-to-image generation via
  transformers,'' in \emph{Advances in Neural Information Processing Systems
  34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS
  2021, December 6-14, 2021, virtual}, 2021, pp. 19\,822--19\,835.

\bibitem{Jimmy-arxiv-2016-Layer}
L.~J. Ba, J.~R. Kiros, and G.~E. Hinton, ``Layer normalization,'' vol.
  abs/1607.06450, 2016.

\bibitem{Zhang-NIPS-2019-Root}
B.~Zhang and R.~Sennrich, ``Root mean square layer normalization,'' in
  \emph{Advances in Neural Information Processing Systems 32: Annual Conference
  on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14,
  2019, Vancouver, BC, Canada}, 2019, pp. 12\,360--12\,371.

\bibitem{Wang-arxiv-2022-DeepNet}
H.~Wang, S.~Ma, L.~Dong, S.~Huang, D.~Zhang, and F.~Wei, ``Deepnet: Scaling
  transformers to 1, 000 layers,'' vol. abs/2203.00555, 2022.

\bibitem{Vinod-ICML-2010-Rectified}
V.~Nair and G.~E. Hinton, ``Rectified linear units improve restricted boltzmann
  machines,'' in \emph{Proceedings of the 27th international conference on
  machine learning (ICML-10)}, 2010, pp. 807--814.

\bibitem{Wang-EMNLP-2018-GLUE}
A.~Wang, A.~Singh, J.~Michael, F.~Hill, O.~Levy, and S.~R. Bowman, ``{GLUE:}
  {A} multi-task benchmark and analysis platform for natural language
  understanding,'' in \emph{Proceedings of the Workshop: Analyzing and
  Interpreting Neural Networks for NLP, BlackboxNLP@EMNLP 2018, Brussels,
  Belgium, November 1, 2018}, T.~Linzen, G.~Chrupala, and A.~Alishahi,
  Eds.\hskip 1em plus 0.5em minus 0.4em\relax Association for Computational
  Linguistics, 2018, pp. 353--355.

\bibitem{Ramachandran-arXiv-2017-searching}
P.~Ramachandran, B.~Zoph, and Q.~V. Le, ``Searching for activation functions,''
  \emph{arXiv preprint arXiv:1710.05941}, 2017.

\bibitem{Shazeer-arxiv-2020-GLU}
N.~Shazeer, ``{GLU} variants improve transformer,'' vol. abs/2002.05202, 2020.

\bibitem{Su-arxiv-2021-Roformer}
J.~Su, Y.~Lu, S.~Pan, B.~Wen, and Y.~Liu, ``Roformer: Enhanced transformer with
  rotary position embedding,'' vol. abs/2104.09864, 2021.

\bibitem{Press-ICLR-2022-Train}
O.~Press, N.~A. Smith, and M.~Lewis, ``Train short, test long: Attention with
  linear biases enables input length extrapolation,'' in \emph{The Tenth
  International Conference on Learning Representations, {ICLR} 2022, Virtual
  Event, April 25-29, 2022}, 2022.

\bibitem{Ioffe-2015-ICML-Batch}
\BIBentryALTinterwordspacing
S.~Ioffe and C.~Szegedy, ``Batch normalization: Accelerating deep network
  training by reducing internal covariate shift,'' in \emph{Proceedings of the
  32nd International Conference on Machine Learning, {ICML} 2015, Lille,
  France, 6-11 July 2015}, ser. {JMLR} Workshop and Conference Proceedings,
  F.~R. Bach and D.~M. Blei, Eds., vol.~37.\hskip 1em plus 0.5em minus
  0.4em\relax JMLR.org, 2015, pp. 448--456. [Online]. Available:
  \url{http://proceedings.mlr.press/v37/ioffe15.html}
\BIBentrySTDinterwordspacing

\bibitem{Narang-EMNLP-2021-Do}
S.~Narang, H.~W. Chung, Y.~Tay, L.~Fedus, T.~F{\'{e}}vry, M.~Matena, K.~Malkan,
  N.~Fiedel, N.~Shazeer, Z.~Lan, Y.~Zhou, W.~Li, N.~Ding, J.~Marcus,
  A.~Roberts, and C.~Raffel, ``Do transformer modifications transfer across
  implementations and applications?'' in \emph{Proceedings of the 2021
  Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2021,
  Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021}, 2021,
  pp. 5758--5773.

\bibitem{Xiong-ICML-2020-On}
R.~Xiong, Y.~Yang, D.~He, K.~Zheng, S.~Zheng, C.~Xing, H.~Zhang, Y.~Lan,
  L.~Wang, and T.~Liu, ``On layer normalization in the transformer
  architecture,'' in \emph{{ICML}}, 2020.

\bibitem{Baevski-2019-ICLR-Adaptive}
A.~Baevski and M.~Auli, ``Adaptive input representations for neural language
  modeling,'' in \emph{7th International Conference on Learning
  Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}.\hskip 1em
  plus 0.5em minus 0.4em\relax OpenReview.net, 2019.

\bibitem{liu-2020-EMNLP-Understanding}
L.~Liu, X.~Liu, J.~Gao, W.~Chen, and J.~Han, ``Understanding the difficulty of
  training transformers,'' in \emph{Proceedings of the 2020 Conference on
  Empirical Methods in Natural Language Processing, {EMNLP} 2020, Online,
  November 16-20, 2020}.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2020, pp. 5747--5763.

\bibitem{Dan-arxiv-2016-Gaussian}
D.~Hendrycks and K.~Gimpel, ``Gaussian error linear units (gelus),''
  \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem{Dauphin-ICML-2017-Language}
Y.~N. Dauphin, A.~Fan, M.~Auli, and D.~Grangier, ``Language modeling with gated
  convolutional networks,'' in \emph{Proceedings of the 34th International
  Conference on Machine Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11
  August 2017}, 2017, pp. 933--941.

\bibitem{Le-EMNLP-2022-What}
T.~L. Scao, T.~Wang, D.~Hesslow, S.~Bekman, M.~S. Bari, S.~Biderman,
  H.~Elsahar, N.~Muennighoff, J.~Phang, O.~Press, C.~Raffel, V.~Sanh, S.~Shen,
  L.~Sutawika, J.~Tae, Z.~X. Yong, J.~Launay, and I.~Beltagy, ``What language
  model to train if you have one million {GPU} hours?'' in \emph{Findings of
  the Association for Computational Linguistics: {EMNLP} 2022, Abu Dhabi,
  United Arab Emirates, December 7-11, 2022}, 2022, pp. 765--782.

\bibitem{shaw-2018-acl-self}
\BIBentryALTinterwordspacing
P.~Shaw, J.~Uszkoreit, and A.~Vaswani, ``Self-attention with relative position
  representations,'' in \emph{Proceedings of the 2018 Conference of the North
  American Chapter of the Association for Computational Linguistics: Human
  Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6,
  2018, Volume 2 (Short Papers)}, M.~A. Walker, H.~Ji, and A.~Stent, Eds.\hskip
  1em plus 0.5em minus 0.4em\relax Association for Computational Linguistics,
  2018, pp. 464--468. [Online]. Available:
  \url{https://doi.org/10.18653/v1/n18-2074}
\BIBentrySTDinterwordspacing

\bibitem{dai-2019-acl-transformer}
\BIBentryALTinterwordspacing
Z.~Dai, Z.~Yang, Y.~Yang, J.~G. Carbonell, Q.~V. Le, and R.~Salakhutdinov,
  ``Transformer-xl: Attentive language models beyond a fixed-length context,''
  in \emph{Proceedings of the 57th Conference of the Association for
  Computational Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2,
  2019, Volume 1: Long Papers}, A.~Korhonen, D.~R. Traum, and L.~M{\`{a}}rquez,
  Eds.\hskip 1em plus 0.5em minus 0.4em\relax Association for Computational
  Linguistics, 2019, pp. 2978--2988. [Online]. Available:
  \url{https://doi.org/10.18653/v1/p19-1285}
\BIBentrySTDinterwordspacing

\bibitem{Yang-NeurIPS-2019-xlnet}
Z.~Yang, Z.~Dai, Y.~Yang, J.~Carbonell, R.~R. Salakhutdinov, and Q.~V. Le,
  ``Xlnet: Generalized autoregressive pretraining for language understanding,''
  \emph{Advances in neural information processing systems}, vol.~32, 2019.

\bibitem{Peng-arxiv-2023-Yarn}
B.~Peng, J.~Quesnelle, H.~Fan, and E.~Shippole, ``Yarn: Efficient context
  window extension of large language models,'' \emph{CoRR}, vol.
  abs/2309.00071, 2023.

\bibitem{Sun-2022-arxiv-Length}
\BIBentryALTinterwordspacing
Y.~Sun, L.~Dong, B.~Patra, S.~Ma, S.~Huang, A.~Benhaim, V.~Chaudhary, X.~Song,
  and F.~Wei, ``A length-extrapolatable transformer,'' \emph{CoRR}, vol.
  abs/2212.10554, 2022. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2212.10554}
\BIBentrySTDinterwordspacing

\bibitem{Peng-ICLR-2021-Random}
H.~Peng, N.~Pappas, D.~Yogatama, R.~Schwartz, N.~A. Smith, and L.~Kong,
  ``Random feature attention,'' in \emph{9th International Conference on
  Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7,
  2021}.

\bibitem{Zaheer-NIPS-2020-Big}
M.~Zaheer, G.~Guruganesh, K.~A. Dubey, J.~Ainslie, C.~Alberti,
  S.~Onta{\~{n}}{\'{o}}n, P.~Pham, A.~Ravula, Q.~Wang, L.~Yang, and A.~Ahmed,
  ``Big bird: Transformers for longer sequences,'' in \emph{Advances in Neural
  Information Processing Systems 33: Annual Conference on Neural Information
  Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.

\bibitem{Child-arxiv-2019-Generating}
R.~Child, S.~Gray, A.~Radford, and I.~Sutskever, ``Generating long sequences
  with sparse transformers,'' \emph{CoRR}, vol. abs/1904.10509, 2019.

\bibitem{Shazeer-2019-arxiv-Fast}
\BIBentryALTinterwordspacing
N.~Shazeer, ``Fast transformer decoding: One write-head is all you need,''
  \emph{CoRR}, vol. abs/1911.02150, 2019. [Online]. Available:
  \url{http://arxiv.org/abs/1911.02150}
\BIBentrySTDinterwordspacing

\bibitem{Ainslie-2023-arxiv-gqa}
J.~Ainslie, J.~Lee-Thorp, M.~de~Jong, Y.~Zemlyanskiy, F.~Lebr{\'o}n, and
  S.~Sanghai, ``Gqa: Training generalized multi-query transformer models from
  multi-head checkpoints,'' \emph{arXiv preprint arXiv:2305.13245}, 2023.

\bibitem{Dao-NeurIPS-2020-FLASH}
T.~Dao, D.~Y. Fu, S.~Ermon, A.~Rudra, and C.~Re, ``Flashattention: Fast and
  memory-efficient exact attention with {IO}-awareness,'' in \emph{{NeurIPS}},
  2022.

\bibitem{Dao-2023-arxiv-flashattention2}
T.~Dao, ``Flashattention-2: Faster attention with better parallelism and work
  partitioning,'' \emph{arXiv preprint arXiv:2307.08691}, 2023.

\bibitem{vllm-pagedattention}
\BIBentryALTinterwordspacing
``vllm: Easy, fast, and cheap llm serving with pagedattention.'' [Online].
  Available: \url{https://vllm.ai/}
\BIBentrySTDinterwordspacing

\bibitem{Yuan-2022-IUI-Wordcraft}
A.~Yuan, A.~Coenen, E.~Reif, and D.~Ippolito, ``Wordcraft: story writing with
  large language models,'' in \emph{27th International Conference on
  Intelligent User Interfaces}, 2022, pp. 841--852.

\bibitem{kazemnejad-arxiv-2023-impact}
A.~Kazemnejad, I.~Padhi, K.~N. Ramamurthy, P.~Das, and S.~Reddy, ``The impact
  of positional encoding on length generalization in transformers,''
  \emph{CoRR}, vol. abs/2305.19466, 2023.

\bibitem{xiong-arxiv-2023-effective}
W.~Xiong, J.~Liu, I.~Molybog, H.~Zhang, P.~Bhargava, R.~Hou, L.~Martin,
  R.~Rungta, K.~A. Sankararaman, B.~Oguz, M.~Khabsa, H.~Fang, Y.~Mehdad,
  S.~Narang, K.~Malik, A.~Fan, S.~Bhosale, S.~Edunov, M.~Lewis, S.~Wang, and
  H.~Ma, ``Effective long-context scaling of foundation models,'' \emph{CoRR},
  vol. abs/2309.16039, 2023.

\bibitem{kaiokendev-github-2023-Things}
kaiokendev, ``{Things I'm learning while training superhot.}'' 2023.

\bibitem{Dong-arxiv-2023-BAMBOO}
Z.~Dong, T.~Tang, J.~Li, W.~X. Zhao, and J.~Wen, ``{BAMBOO:} {A} comprehensive
  benchmark for evaluating long text modeling capacities of large language
  models,'' \emph{CoRR}, vol. abs/2309.13345, 2023.

\bibitem{su-online-2023-Rerope}
J.~Su. (2023) Transformer upgrade path: 12, infinite extrapolation of rerope?

\bibitem{Liu-arxiv-2023_scaling}
X.~Liu, H.~Yan, S.~Zhang, C.~An, X.~Qiu, and D.~Lin, ``Scaling laws of
  rope-based extrapolation,'' \emph{CoRR}, vol. abs/2310.05209, 2023.

\bibitem{Pal-arxiv-2023-giraffe}
A.~Pal, D.~Karkhanis, M.~Roberts, S.~Dooley, A.~Sundararajan, and S.~Naidu,
  ``Giraffe: Adventures in expanding context lengths in llms,'' \emph{CoRR},
  vol. abs/2308.10882, 2023.

\bibitem{Izacard-EACL-2021-Leveraging}
G.~Izacard and E.~Grave, ``Leveraging passage retrieval with generative models
  for open domain question answering,'' in \emph{Proceedings of the 16th
  Conference of the European Chapter of the Association for Computational
  Linguistics: Main Volume, {EACL} 2021, Online, April 19 - 23, 2021}.\hskip
  1em plus 0.5em minus 0.4em\relax Association for Computational Linguistics,
  2021, pp. 874--880.

\bibitem{Partner-ACL-2023-Parallel}
N.~Ratner, Y.~Levine, Y.~Belinkov, O.~Ram, I.~Magar, O.~Abend, E.~Karpas,
  A.~Shashua, K.~Leyton{-}Brown, and Y.~Shoham, ``Parallel context windows for
  large language models,'' in \emph{Proceedings of the 61st Annual Meeting of
  the Association for Computational Linguistics (Volume 1: Long Papers), {ACL}
  2023, Toronto, Canada, July 9-14, 2023}.\hskip 1em plus 0.5em minus
  0.4em\relax Association for Computational Linguistics, 2023, pp. 6383--6402.

\bibitem{Hao-2022-arxiv-Structured}
Y.~Hao, Y.~Sun, L.~Dong, Z.~Han, Y.~Gu, and F.~Wei, ``Structured prompting:
  Scaling in-context learning to 1, 000 examples,'' \emph{CoRR}, 2022.

\bibitem{Beltagy-arxiv-2020-longformer}
I.~Beltagy, M.~E. Peters, and A.~Cohan, ``Longformer: The long-document
  transformer,'' \emph{CoRR}, vol. abs/2004.05150, 2020.

\bibitem{Xiao-arxiv-2023-Efficient}
G.~Xiao, Y.~Tian, B.~Chen, S.~Han, and M.~Lewis, ``Efficient streaming language
  models with attention sinks,'' \emph{CoRR}, vol. abs/2309.17453, 2023.

\bibitem{Liu-arxiv-2023-Lost}
N.~F. Liu, K.~Lin, J.~Hewitt, A.~Paranjape, M.~Bevilacqua, F.~Petroni, and
  P.~Liang, ``Lost in the middle: How language models use long contexts,''
  \emph{CoRR}, vol. abs/2307.03172, 2023.

\bibitem{Han-arxiv-2023-LMinfinite}
C.~Han, Q.~Wang, W.~Xiong, Y.~Chen, H.~Ji, and S.~Wang, ``Lm-infinite: Simple
  on-the-fly length generalization for large language models,'' \emph{CoRR},
  vol. abs/2308.16137, 2023.

\bibitem{Bertsch-arxiv-2023-Unlimiformer}
A.~Bertsch, U.~Alon, G.~Neubig, and M.~R. Gormley, ``Unlimiformer: Long-range
  transformers with unlimited length input,'' \emph{CoRR}, vol. abs/2305.01625,
  2023.

\bibitem{Wu-ICLR-2022-Memorizing}
Y.~Wu, M.~N. Rabe, D.~Hutchins, and C.~Szegedy, ``Memorizing transformers,'' in
  \emph{The Tenth International Conference on Learning Representations, {ICLR}
  2022, Virtual Event, April 25-29, 2022}.\hskip 1em plus 0.5em minus
  0.4em\relax OpenReview.net, 2022.

\bibitem{Chen-arxiv-2023-Walking}
H.~Chen, R.~Pasunuru, J.~Weston, and A.~Celikyilmaz, ``Walking down the memory
  maze: Beyond context limit through interactive reading,'' \emph{CoRR}, vol.
  abs/2310.05029, 2023.

\bibitem{zhou-arxiv-2023-recurrentgpt}
W.~Zhou, Y.~E. Jiang, P.~Cui, T.~Wang, Z.~Xiao, Y.~Hou, R.~Cotterell, and
  M.~Sachan, ``Recurrentgpt: Interactive generation of (arbitrarily) long
  text,'' \emph{CoRR}, vol. abs/2305.13304, 2023.

\bibitem{Packer-arxiv-2023-MemGPT}
C.~Packer, V.~Fang, S.~G. Patil, K.~Lin, S.~Wooders, and J.~E. Gonzalez,
  ``Memgpt: Towards llms as operating systems,'' \emph{CoRR}, vol.
  abs/2310.08560, 2023.

\bibitem{Xu-arxiv-2023-retrieval}
P.~Xu, W.~Ping, X.~Wu, L.~McAfee, C.~Zhu, Z.~Liu, S.~Subramanian,
  E.~Bakhturina, M.~Shoeybi, and B.~Catanzaro, ``Retrieval meets long context
  large language models,'' \emph{CoRR}, vol. abs/2310.03025, 2023.

\bibitem{Murray-WMT-2018-Correcting}
K.~Murray and D.~Chiang, ``Correcting length bias in neural machine
  translation,'' in \emph{{WMT}}.\hskip 1em plus 0.5em minus 0.4em\relax
  Association for Computational Linguistics, 2018, pp. 212--223.

\bibitem{Holtzman-2020-ICLR-The}
A.~Holtzman, J.~Buys, L.~Du, M.~Forbes, and Y.~Choi, ``The curious case of
  neural text degeneration,'' in \emph{{ICLR}}, 2020.

\bibitem{CMU-book-1977-speech}
C.-M. U. P. P. D. O.~C. SCIENCE, \emph{Speech Understanding Systems. Summary of
  Results of the Five-Year Research Effort at Carnegie-Mellon University},
  1977.

\bibitem{Koehn-ACL-2017-Six}
P.~Koehn and R.~Knowles, ``Six challenges for neural machine translation,'' in
  \emph{NMT@ACL}.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2017, pp. 28--39.

\bibitem{Wu-arxiv-2016-Google}
Y.~Wu, M.~Schuster, Z.~Chen, Q.~V. Le, M.~Norouzi, W.~Macherey, M.~Krikun,
  Y.~Cao, Q.~Gao, K.~Macherey, J.~Klingner, A.~Shah, M.~Johnson, X.~Liu,
  L.~Kaiser, S.~Gouws, Y.~Kato, T.~Kudo, H.~Kazawa, K.~Stevens, G.~Kurian,
  N.~Patil, W.~Wang, C.~Young, J.~Smith, J.~Riesa, A.~Rudnick, O.~Vinyals,
  G.~Corrado, M.~Hughes, and J.~Dean, ``Google's neural machine translation
  system: Bridging the gap between human and machine translation,''
  \emph{CoRR}, vol. abs/1609.08144, 2016.

\bibitem{Paulus-iclr-2018-A}
R.~Paulus, C.~Xiong, and R.~Socher, ``A deep reinforced model for abstractive
  summarization,'' in \emph{{ICLR} (Poster)}.\hskip 1em plus 0.5em minus
  0.4em\relax OpenReview.net, 2018.

\bibitem{Vijayakumar-arxiv-2016-Diverse}
A.~K. Vijayakumar, M.~Cogswell, R.~R. Selvaraju, Q.~Sun, S.~Lee, D.~J.
  Crandall, and D.~Batra, ``Diverse beam search: Decoding diverse solutions
  from neural sequence models,'' \emph{CoRR}, vol. abs/1610.02424, 2016.

\bibitem{Fan-2018-ACL-Hierarchical}
A.~Fan, M.~Lewis, and Y.~N. Dauphin, ``Hierarchical neural story generation,''
  in \emph{{ACL} {(1)}}.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2018, pp. 889--898.

\bibitem{Hewitt-emnlp-2022-Truncation}
J.~Hewitt, C.~D. Manning, and P.~Liang, ``Truncation sampling as language model
  desmoothing,'' in \emph{{EMNLP} (Findings)}.\hskip 1em plus 0.5em minus
  0.4em\relax Association for Computational Linguistics, 2022, pp. 3414--3427.

\bibitem{Su-2022-NIPS-A}
Y.~Su, T.~Lan, Y.~Wang, D.~Yogatama, L.~Kong, and N.~Collier, ``A contrastive
  framework for neural text generation,'' in \emph{NeurIPS}, 2022.

\bibitem{Meister-TACL-2023-Locally}
C.~Meister, T.~Pimentel, G.~Wiher, and R.~Cotterell, ``Locally typical
  sampling,'' \emph{Trans. Assoc. Comput. Linguistics}, 2023.

\bibitem{Li-ACL-2023-Contrastive}
X.~L. Li, A.~Holtzman, D.~Fried, P.~Liang, J.~Eisner, T.~Hashimoto,
  L.~Zettlemoyer, and M.~Lewis, ``Contrastive decoding: Open-ended text
  generation as optimization,'' in \emph{{ACL} {(1)}}.\hskip 1em plus 0.5em
  minus 0.4em\relax Association for Computational Linguistics, 2023, pp.
  12\,286--12\,312.

\bibitem{Chuang-arxiv-2023-DoLa}
Y.~Chuang, Y.~Xie, H.~Luo, Y.~Kim, J.~R. Glass, and P.~He, ``Dola: Decoding by
  contrasting layers improves factuality in large language models,''
  \emph{CoRR}, vol. abs/2309.03883, 2023.

\bibitem{Chen-blog-2023-Dissecting}
\BIBentryALTinterwordspacing
L.~Chen, ``Dissecting batching effects in gpt inference,'' 2023. [Online].
  Available: \url{https://le.qun.ch/en/blog/2023/05/13/transformer-batching/}
\BIBentrySTDinterwordspacing

\bibitem{Sheng-ICML-2023-FlexGen}
Y.~Sheng, L.~Zheng, B.~Yuan, Z.~Li, M.~Ryabinin, B.~Chen, P.~Liang,
  C.~R{\'{e}}, I.~Stoica, and C.~Zhang, ``Flexgen: High-throughput generative
  inference of large language models with a single {GPU},'' in \emph{{ICML}},
  ser. Proceedings of Machine Learning Research, vol. 202.\hskip 1em plus 0.5em
  minus 0.4em\relax {PMLR}, 2023, pp. 31\,094--31\,116.

\bibitem{Dao-stanford-2023-Flash}
T.~Dao, D.~Haziza, F.~Massa, and G.~Sizov, ``Flash-decoding for long-context
  inference,'' \url{https://crfm.stanford.edu/2023/10/12/flashdecoding.html},
  2023.

\bibitem{Yaniv-ICML-2023-Fast}
Y.~Leviathan, M.~Kalman, and Y.~Matias, ``Fast inference from transformers via
  speculative decoding,'' in \emph{International Conference on Machine
  Learning}, 2023.

\bibitem{Chen-arxiv-2023-Accelerating}
C.~Chen, S.~Borgeaud, G.~Irving, J.~Lespiau, L.~Sifre, and J.~Jumper,
  ``Accelerating large language model decoding with speculative sampling,''
  \emph{CoRR}, vol. abs/2302.01318, 2023.

\bibitem{Miao-arxiv-2023-SpecInfer}
X.~Miao, G.~Oliaro, Z.~Zhang, X.~Cheng, Z.~Wang, R.~Y.~Y. Wong, Z.~Chen,
  D.~Arfeen, R.~Abhyankar, and Z.~Jia, ``Specinfer: Accelerating generative
  {LLM} serving with speculative inference and token tree verification,''
  \emph{CoRR}, vol. abs/2305.09781, 2023.

\bibitem{Spector-2023-arxiv-Accelerating}
B.~Spector and C.~R{\'{e}}, ``Accelerating {LLM} inference with staged
  speculative decoding,'' \emph{CoRR}, vol. abs/2308.04623, 2023.

\bibitem{Corro-arxiv-2023-SkipDecode}
L.~D. Corro, A.~D. Giorno, S.~Agarwal, B.~Yu, A.~H. Awadallah, and
  S.~Mukherjee, ``Skipdecode: Autoregressive skip decoding with batching and
  caching for efficient {LLM} inference,'' \emph{CoRR}, vol. abs/2307.02628,
  2023.

\bibitem{Kingma-arXiv-2015-Adam}
D.~P. Kingma and J.~Ba, ``Adam: {A} method for stochastic optimization,'' in
  \emph{3rd International Conference on Learning Representations, {ICLR} 2015,
  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings}, Y.~Bengio
  and Y.~LeCun, Eds., 2015.

\bibitem{Loshchilov-arxiv-2017-Fixing}
I.~Loshchilov and F.~Hutter, ``Fixing weight decay regularization in adam,''
  \emph{CoRR}, vol. abs/1711.05101, 2017.

\bibitem{Shazeer-ICML-2018-Adafactor}
N.~Shazeer and M.~Stern, ``Adafactor: Adaptive learning rates with sublinear
  memory cost,'' in \emph{Proceedings of the 35th International Conference on
  Machine Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden,
  July 10-15, 2018}, ser. Proceedings of Machine Learning Research, J.~G. Dy
  and A.~Krause, Eds., vol.~80.\hskip 1em plus 0.5em minus 0.4em\relax {PMLR},
  2018, pp. 4603--4611.

\bibitem{Huang-NeurIPS-2019-GPipe}
Y.~Huang, Y.~Cheng, A.~Bapna, O.~Firat, D.~Chen, M.~X. Chen, H.~Lee, J.~Ngiam,
  Q.~V. Le, Y.~Wu, and Z.~Chen, ``Gpipe: Efficient training of giant neural
  networks using pipeline parallelism,'' in \emph{Advances in Neural
  Information Processing Systems 32: Annual Conference on Neural Information
  Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
  Canada}, H.~M. Wallach, H.~Larochelle, A.~Beygelzimer,
  F.~d'Alch{\'{e}}{-}Buc, E.~B. Fox, and R.~Garnett, Eds., 2019, pp. 103--112.

\bibitem{Harlap-arXiv-2018-PipeDream}
A.~Harlap, D.~Narayanan, A.~Phanishayee, V.~Seshadri, N.~R. Devanur, G.~R.
  Ganger, and P.~B. Gibbons, ``Pipedream: Fast and efficient pipeline parallel
  {DNN} training,'' \emph{CoRR}, vol. abs/1806.03377, 2018.

\bibitem{Rajbhandari-IEEE-2020-ZeRO}
S.~Rajbhandari, J.~Rasley, O.~Ruwase, and Y.~He, ``Zero: memory optimizations
  toward training trillion parameter models,'' in \emph{Proceedings of the
  International Conference for High Performance Computing, Networking, Storage
  and Analysis, {SC} 2020, Virtual Event / Atlanta, Georgia, USA, November
  9-19, 2020}, C.~Cuicchi, I.~Qualters, and W.~T. Kramer, Eds.\hskip 1em plus
  0.5em minus 0.4em\relax {IEEE/ACM}, 2020, p.~20.

\bibitem{Micikevicius-arXiv-2017-Mixed}
P.~Micikevicius, S.~Narang, J.~Alben, G.~F. Diamos, E.~Elsen, D.~Garc{\'{\i}}a,
  B.~Ginsburg, M.~Houston, O.~Kuchaiev, G.~Venkatesh, and H.~Wu, ``Mixed
  precision training,'' \emph{CoRR}, vol. abs/1710.03740, 2017.

\bibitem{Xu-arXiv-2021-An}
Q.~Xu, S.~Li, C.~Gong, and Y.~You, ``An efficient 2d method for training
  super-large deep learning models,'' \emph{CoRR}, vol. abs/2104.05343, 2021.

\bibitem{Wang-ICPP-2022-Tesseract}
B.~Wang, Q.~Xu, Z.~Bian, and Y.~You, ``Tesseract: Parallelize the tensor
  parallelism efficiently,'' in \emph{Proceedings of the 51st International
  Conference on Parallel Processing, {ICPP} 2022, Bordeaux, France, 29 August
  2022 - 1 September 2022}.\hskip 1em plus 0.5em minus 0.4em\relax {ACM}, 2022.

\bibitem{Bian-arXiv-2021-Maximizing}
Z.~Bian, Q.~Xu, B.~Wang, and Y.~You, ``Maximizing parallelism in distributed
  training for huge neural networks,'' \emph{CoRR}, vol. abs/2105.14450, 2021.

\bibitem{Li-arXiv-2021-Sequence}
S.~Li, F.~Xue, C.~Baranwal, Y.~Li, and Y.~You, ``Sequence parallelism: Long
  sequence training from system perspective,'' \emph{arXiv e-prints}, pp.
  arXiv--2105, 2021.

\bibitem{FairScale2021}
{FairScale authors}, ``Fairscale: A general purpose modular pytorch library for
  high performance and large scale training,''
  \url{https://github.com/facebookresearch/fairscale}, 2021.

\bibitem{Zheng-OSDI-2022-Alpa}
L.~Zheng, Z.~Li, H.~Zhang, Y.~Zhuang, Z.~Chen, Y.~Huang, Y.~Wang, Y.~Xu,
  D.~Zhuo, E.~P. Xing \emph{et~al.}, ``Alpa: Automating inter-and
  $\{$Intra-Operator$\}$ parallelism for distributed deep learning,'' in
  \emph{{OSDI}}, 2022, pp. 559--578.

\bibitem{Chen-arxiv-2016-training}
T.~Chen, B.~Xu, C.~Zhang, and C.~Guestrin, ``Training deep nets with sublinear
  memory cost,'' \emph{CoRR}, vol. abs/1604.06174, 2016.

\bibitem{Lou-arXiv-2023-Is}
R.~Lou, K.~Zhang, and W.~Yin, ``Is prompt all you need? no. {A} comprehensive
  and broader view of instruction learning,'' \emph{CoRR}, vol. abs/2303.10475,
  2023.

\bibitem{Liu-ACL-2019-Multi}
X.~Liu, P.~He, W.~Chen, and J.~Gao, ``Multi-task deep neural networks for
  natural language understanding,'' in \emph{{ACL} {(1)}}.\hskip 1em plus 0.5em
  minus 0.4em\relax Association for Computational Linguistics, 2019, pp.
  4487--4496.

\bibitem{Aghajanyan-EMNLP-2021-Muppet}
A.~Aghajanyan, A.~Gupta, A.~Shrivastava, X.~Chen, L.~Zettlemoyer, and S.~Gupta,
  ``Muppet: Massive multi-task representations with pre-finetuning,'' in
  \emph{{EMNLP} {(1)}}.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2021, pp. 5799--5811.

\bibitem{Longpre-arxiv-2023-The}
S.~Longpre, L.~Hou, T.~Vu, A.~Webson, H.~W. Chung, Y.~Tay, D.~Zhou, Q.~V. Le,
  B.~Zoph, J.~Wei, and A.~Roberts, ``The flan collection: Designing data and
  methods for effective instruction tuning,'' \emph{CoRR}, vol. abs/2301.13688,
  2023.

\bibitem{Xu-arxiv-2023-WizardLM}
\BIBentryALTinterwordspacing
C.~Xu, Q.~Sun, K.~Zheng, X.~Geng, P.~Zhao, J.~Feng, C.~Tao, and D.~Jiang,
  ``Wizardlm: Empowering large language models to follow complex
  instructions,'' \emph{CoRR}, vol. abs/2304.12244, 2023. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2304.12244}
\BIBentrySTDinterwordspacing

\bibitem{Sun-arxiv-2023-Principle}
Z.~Sun, Y.~Shen, Q.~Zhou, H.~Zhang, Z.~Chen, D.~Cox, Y.~Yang, and C.~Gan,
  ``Principle-driven self-alignment of language models from scratch with
  minimal human supervision,'' \emph{arXiv preprint arXiv:2305.03047}, 2023.

\bibitem{Li-arxiv-2023-Self}
X.~Li, P.~Yu, C.~Zhou, T.~Schick, L.~Zettlemoyer, O.~Levy, J.~Weston, and
  M.~Lewis, ``Self-alignment with instruction backtranslation,'' \emph{CoRR},
  vol. abs/2308.06259, 2023.

\bibitem{zhou-arxiv-2023-lima}
C.~Zhou, P.~Liu, P.~Xu, S.~Iyer, J.~Sun, Y.~Mao, X.~Ma, A.~Efrat, P.~Yu, L.~Yu
  \emph{et~al.}, ``Lima: Less is more for alignment,'' \emph{arXiv preprint
  arXiv:2305.11206}, 2023.

\bibitem{Chen-arxiv-2023-AlpaGasus}
L.~Chen, S.~Li, J.~Yan, H.~Wang, K.~Gunaratna, V.~Yadav, Z.~Tang,
  V.~Srinivasan, T.~Zhou, H.~Huang, and H.~Jin, ``Alpagasus: Training {A}
  better alpaca with fewer data,'' \emph{CoRR}, vol. abs/2307.08701, 2023.

\bibitem{Mukherjee-arxiv-2023-Orca}
S.~Mukherjee, A.~Mitra, G.~Jawahar, S.~Agarwal, H.~Palangi, and A.~H.
  Awadallah, ``Orca: Progressive learning from complex explanation traces of
  {GPT-4},'' \emph{CoRR}, vol. abs/2306.02707, 2023.

\bibitem{YuLan-Chat}
YuLan-Chat-Team, ``Yulan-chat: An open-source bilingual chatbot,''
  \url{https://github.com/RUC-GSAI/YuLan-Chat}, 2023.

\bibitem{Wang-arxiv-2023-How}
Y.~Wang, H.~Ivison, P.~Dasigi, J.~Hessel, T.~Khot, K.~R. Chandu, D.~Wadden,
  K.~MacMillan, N.~A. Smith, I.~Beltagy, and H.~Hajishirzi, ``How far can
  camels go? exploring the state of instruction tuning on open resources,''
  \emph{CoRR}, vol. abs/2306.04751, 2023.

\bibitem{Peng-23-arxiv-Instruction}
B.~Peng, C.~Li, P.~He, M.~Galley, and J.~Gao, ``Instruction tuning with
  {GPT-4},'' \emph{CoRR}, vol. abs/2304.03277, 2023.

\bibitem{Krell-2021-arxiv-efficient}
M.~M. Krell, M.~Kosec, S.~P. Perez, and A.~Fitzgibbon, ``Efficient sequence
  packing without cross-contamination: Accelerating large language models
  without impacting performance,'' \emph{arXiv preprint arXiv:2107.02027},
  2021.

\bibitem{singhal-arxiv-2022-large}
K.~Singhal, S.~Azizi, T.~Tu, S.~S. Mahdavi, J.~Wei, H.~W. Chung, N.~Scales,
  A.~Tanwani, H.~Cole-Lewis, S.~Pfohl \emph{et~al.}, ``Large language models
  encode clinical knowledge,'' \emph{arXiv preprint arXiv:2212.13138}, 2022.

\bibitem{Zhang-2023-arxiv-recommendation}
J.~Zhang, R.~Xie, Y.~Hou, W.~X. Zhao, L.~Lin, and J.~Wen, ``Recommendation as
  instruction following: {A} large language model empowered recommendation
  approach,'' \emph{CoRR}, vol. abs/2305.07001, 2023.

\bibitem{wang-arxiv-2023-huatuo}
H.~Wang, C.~Liu, N.~Xi, Z.~Qiang, S.~Zhao, B.~Qin, and T.~Liu, ``Huatuo: Tuning
  llama model with chinese medical knowledge,'' \emph{arXiv preprint
  arXiv:2304.06975}, 2023.

\bibitem{huang-arxiv-2023-lawyer}
Q.~Huang, M.~Tao, Z.~An, C.~Zhang, C.~Jiang, Z.~Chen, Z.~Wu, and Y.~Feng,
  ``Lawyer llama technical report,'' \emph{arXiv preprint arXiv:2305.15062},
  2023.

\bibitem{wu-arxiv-2023-bloomberggpt}
S.~Wu, O.~Irsoy, S.~Lu, V.~Dabravolski, M.~Dredze, S.~Gehrmann, P.~Kambadur,
  D.~Rosenberg, and G.~Mann, ``Bloomberggpt: A large language model for
  finance,'' \emph{arXiv preprint arXiv:2303.17564}, 2023.

\bibitem{liu-arxiv-2023-goat}
T.~Liu and B.~K.~H. Low, ``Goat: Fine-tuned llama outperforms gpt-4 on
  arithmetic tasks,'' \emph{arXiv preprint arXiv:2305.14201}, 2023.

\bibitem{sun2023moss}
T.~Sun, X.~Zhang, Z.~He, P.~Li, Q.~Cheng, H.~Yan, X.~Liu, Y.~Shao, Q.~Tang,
  X.~Zhao, K.~Chen, Y.~Zheng, Z.~Zhou, R.~Li, J.~Zhan, Y.~Zhou, L.~Li, X.~Yang,
  L.~Wu, Z.~Yin, X.~Huang, and X.~Qiu, ``Moss: Training conversational language
  models from synthetic data,'' 2023.

\bibitem{Dubois-arxiv-2023-AlpacaFarm}
\BIBentryALTinterwordspacing
Y.~Dubois, X.~Li, R.~Taori, T.~Zhang, I.~Gulrajani, J.~Ba, C.~Guestrin,
  P.~Liang, and T.~B. Hashimoto, ``Alpacafarm: {A} simulation framework for
  methods that learn from human feedback,'' \emph{CoRR}, vol. abs/2305.14387,
  2023. [Online]. Available: \url{https://doi.org/10.48550/arXiv.2305.14387}
\BIBentrySTDinterwordspacing

\bibitem{Hendrycks-ICLR-2021-Measuring}
D.~Hendrycks, C.~Burns, S.~Basart, A.~Zou, M.~Mazeika, D.~Song, and
  J.~Steinhardt, ``Measuring massive multitask language understanding,'' in
  \emph{{ICLR}}.\hskip 1em plus 0.5em minus 0.4em\relax OpenReview.net, 2021.

\bibitem{Suzgun-arxiv-2022-Challenging}
M.~Suzgun, N.~Scales, N.~Sch{\"{a}}rli, S.~Gehrmann, Y.~Tay, H.~W. Chung,
  A.~Chowdhery, Q.~V. Le, E.~H. Chi, D.~Zhou, and J.~Wei, ``Challenging
  big-bench tasks and whether chain-of-thought can solve them,'' \emph{CoRR},
  vol. abs/2210.09261, 2022.

\bibitem{Kenton-arxiv-2021-Alignment}
Z.~Kenton, T.~Everitt, L.~Weidinger, I.~Gabriel, V.~Mikulik, and G.~Irving,
  ``Alignment of language agents,'' \emph{CoRR}, vol. abs/2103.14659, 2021.

\bibitem{Ziegler-arxiv-2019-Fine-Tuning}
D.~M. Ziegler, N.~Stiennon, J.~Wu, T.~B. Brown, A.~Radford, D.~Amodei, P.~F.
  Christiano, and G.~Irving, ``Fine-tuning language models from human
  preferences,'' \emph{CoRR}, vol. abs/1909.08593, 2019.

\bibitem{Askell-arxiv-2021-A}
A.~Askell, Y.~Bai, A.~Chen, D.~Drain, D.~Ganguli, T.~Henighan, A.~Jones,
  N.~Joseph, B.~Mann, N.~DasSarma, N.~Elhage, Z.~Hatfield{-}Dodds,
  D.~Hernandez, J.~Kernion, K.~Ndousse, C.~Olsson, D.~Amodei, T.~B. Brown,
  J.~Clark, S.~McCandlish, C.~Olah, and J.~Kaplan, ``A general language
  assistant as a laboratory for alignment,'' \emph{CoRR}, vol. abs/2112.00861,
  2021.

\bibitem{Perez-EMNLP-2022-Red}
E.~Perez, S.~Huang, H.~F. Song, T.~Cai, R.~Ring, J.~Aslanides, A.~Glaese,
  N.~McAleese, and G.~Irving, ``Red teaming language models with language
  models,'' in \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab Emirates,
  December 7-11, 2022}, Y.~Goldberg, Z.~Kozareva, and Y.~Zhang, Eds.\hskip 1em
  plus 0.5em minus 0.4em\relax Association for Computational Linguistics, 2022,
  pp. 3419--3448.

\bibitem{Menick-arxiv-2022-teaching}
J.~Menick, M.~Trebacz, V.~Mikulik, J.~Aslanides, H.~F. Song, M.~Chadwick,
  M.~Glaese, S.~Young, L.~Campbell{-}Gillingham, G.~Irving, and N.~McAleese,
  ``Teaching language models to support answers with verified quotes,''
  \emph{CoRR}, vol. abs/2203.11147, 2022.

\bibitem{Bai-arXiv-2022-Constitutional}
\BIBentryALTinterwordspacing
Y.~Bai, S.~Kadavath, S.~Kundu, A.~Askell, J.~Kernion, A.~Jones, A.~Chen,
  A.~Goldie, A.~Mirhoseini, C.~McKinnon, C.~Chen, C.~Olsson, C.~Olah,
  D.~Hernandez, D.~Drain, D.~Ganguli, D.~Li, E.~Tran{-}Johnson, E.~Perez,
  J.~Kerr, J.~Mueller, J.~Ladish, J.~Landau, K.~Ndousse, K.~Lukosiute,
  L.~Lovitt, M.~Sellitto, N.~Elhage, N.~Schiefer, N.~Mercado, N.~DasSarma,
  R.~Lasenby, R.~Larson, S.~Ringer, S.~Johnston, S.~Kravec, S.~E. Showk,
  S.~Fort, T.~Lanham, T.~Telleen{-}Lawton, T.~Conerly, T.~Henighan, T.~Hume,
  S.~R. Bowman, Z.~Hatfield{-}Dodds, B.~Mann, D.~Amodei, N.~Joseph,
  S.~McCandlish, T.~Brown, and J.~Kaplan, ``Constitutional {AI:} harmlessness
  from {AI} feedback,'' \emph{CoRR}, vol. abs/2212.08073, 2022. [Online].
  Available: \url{https://doi.org/10.48550/arXiv.2212.08073}
\BIBentrySTDinterwordspacing

\bibitem{Lee-CoRR-2023-RLAIF}
H.~Lee, S.~Phatale, H.~Mansoor, K.~Lu, T.~Mesnard, C.~Bishop, V.~Carbune, and
  A.~Rastogi, ``{RLAIF:} scaling reinforcement learning from human feedback
  with {AI} feedback,'' \emph{CoRR}, vol. abs/2309.00267, 2023.

\bibitem{Dong-RAFT-2023-arxiv}
\BIBentryALTinterwordspacing
H.~Dong, W.~Xiong, D.~Goyal, R.~Pan, S.~Diao, J.~Zhang, K.~Shum, and T.~Zhang,
  ``{RAFT:} reward ranked finetuning for generative foundation model
  alignment,'' \emph{CoRR}, vol. abs/2304.06767, 2023. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2304.06767}
\BIBentrySTDinterwordspacing

\bibitem{askell2021general}
A.~Askell, Y.~Bai, A.~Chen, D.~Drain, D.~Ganguli, T.~Henighan, A.~Jones,
  N.~Joseph, B.~Mann, N.~DasSarma \emph{et~al.}, ``A general language assistant
  as a laboratory for alignment,'' \emph{arXiv preprint arXiv:2112.00861},
  2021.

\bibitem{zheng2023secrets}
R.~Zheng, S.~Dou, S.~Gao, W.~Shen, B.~Wang, Y.~Liu, S.~Jin, Q.~Liu, L.~Xiong,
  L.~Chen \emph{et~al.}, ``Secrets of rlhf in large language models part i:
  Ppo,'' \emph{arXiv preprint arXiv:2307.04964}, 2023.

\bibitem{Uesato-arxiv-2022-Solving}
J.~Uesato, N.~Kushman, R.~Kumar, H.~F. Song, N.~Y. Siegel, L.~Wang,
  A.~Creswell, G.~Irving, and I.~Higgins, ``Solving math word problems with
  process- and outcome-based feedback,'' \emph{CoRR}, vol. abs/2211.14275,
  2022.

\bibitem{Lightman-arxiv-2023-let}
H.~Lightman, V.~Kosaraju, Y.~Burda, H.~Edwards, B.~Baker, T.~Lee, J.~Leike,
  J.~Schulman, I.~Sutskever, and K.~Cobbe, ``Let's verify step by step,''
  \emph{CoRR}, vol. abs/2305.20050, 2023.

\bibitem{Hendrycks-nips-2021-Measuring}
D.~Hendrycks, S.~Basart, S.~Kadavath, M.~Mazeika, A.~Arora, E.~Guo, C.~Burns,
  S.~Puranik, H.~He, D.~Song, and J.~Steinhardt, ``Measuring coding challenge
  competence with {APPS},'' in \emph{NeurIPS Datasets and Benchmarks}, 2021.

\bibitem{Ma-arxiv-2023-Let}
Q.~Ma, H.~Zhou, T.~Liu, J.~Yuan, P.~Liu, Y.~You, and H.~Yang, ``Let's reward
  step by step: Step-level reward model as the navigators for reasoning,''
  \emph{CoRR}, vol. abs/2310.10080, 2023.

\bibitem{Silver-nat-2017-Mastering}
D.~Silver, J.~Schrittwieser, K.~Simonyan, I.~Antonoglou, A.~Huang, A.~Guez,
  T.~Hubert, L.~Baker, M.~Lai, A.~Bolton, Y.~Chen, T.~P. Lillicrap, F.~Hui,
  L.~Sifre, G.~van~den Driessche, T.~Graepel, and D.~Hassabis, ``Mastering the
  game of go without human knowledge,'' \emph{Nat.}, pp. 354--359, 2017.

\bibitem{Anthony-nips-2017-Thinking}
T.~Anthony, Z.~Tian, and D.~Barber, ``Thinking fast and slow with deep learning
  and tree search,'' in \emph{Advances in Neural Information Processing Systems
  30: Annual Conference on Neural Information Processing Systems 2017, December
  4-9, 2017, Long Beach, CA, {USA}}, 2017, pp. 5360--5370.

\bibitem{Luo-arxiv-2023-WizardMath}
H.~Luo, Q.~Sun, C.~Xu, P.~Zhao, J.~Lou, C.~Tao, X.~Geng, Q.~Lin, S.~Chen, and
  D.~Zhang, ``Wizardmath: Empowering mathematical reasoning for large language
  models via reinforced evol-instruct,'' \emph{CoRR}, vol. abs/2308.09583,
  2023.

\bibitem{Liu-NeurIPS-2022-Second}
R.~Liu, C.~Jia, G.~Zhang, Z.~Zhuang, T.~X. Liu, and S.~Vosoughi, ``Second
  thoughts are best: Learning to re-align with human values from text edits,''
  in \emph{NeurIPS}, 2022.

\bibitem{Lu-nips-2022-quark}
X.~Lu, S.~Welleck, J.~Hessel, L.~Jiang, L.~Qin, P.~West, P.~Ammanabrolu, and
  Y.~Choi, ``{QUARK:} controllable text generation with reinforced
  unlearning,'' in \emph{NeurIPS}, 2022.

\bibitem{Scheurer-arxiv-2023-ILF}
J.~Scheurer, J.~A. Campos, T.~Korbak, J.~S. Chan, A.~Chen, K.~Cho, and
  E.~Perez, ``Training language models with language feedback at scale,''
  \emph{CoRR}, vol. abs/2303.16755, 2023.

\bibitem{Guo-arxiv-2023-Beyond}
G.~Guo, R.~Zhao, T.~Tang, W.~X. Zhao, and J.-R. Wen, ``Beyond imitation:
  Leveraging fine-grained quality signals for alignment,'' \emph{arXiv preprint
  arXiv:2311.04072}, 2023.

\bibitem{Krishna-PNAS-2022-Socially}
\BIBentryALTinterwordspacing
R.~Krishna, D.~Lee, L.~Fei-Fei, and M.~S. Bernstein, ``Socially situated
  artificial intelligence enables learning from human interaction,''
  \emph{Proceedings of the National Academy of Sciences of the United States of
  America}, vol. 119, 2022. [Online]. Available:
  \url{https://api.semanticscholar.org/CorpusID:252381954}
\BIBentrySTDinterwordspacing

\bibitem{Liu-arxiv-2023-Chain}
H.~Liu, C.~Sferrazza, and P.~Abbeel, ``Chain of hindsight aligns language
  models with feedback,'' \emph{CoRR}, vol. abs/2302.02676, 2023.

\bibitem{Rafailov-arxiv-2023-Direct}
\BIBentryALTinterwordspacing
R.~Rafailov, A.~Sharma, E.~Mitchell, S.~Ermon, C.~D. Manning, and C.~Finn,
  ``Direct preference optimization: Your language model is secretly a reward
  model,'' \emph{CoRR}, vol. abs/2305.18290, 2023. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2305.18290}
\BIBentrySTDinterwordspacing

\bibitem{Yuan-RRHF-2023-arxiv}
\BIBentryALTinterwordspacing
Z.~Yuan, H.~Yuan, C.~Tan, W.~Wang, S.~Huang, and F.~Huang, ``{RRHF:} rank
  responses to align language models with human feedback without tears,''
  \emph{CoRR}, vol. abs/2304.05302, 2023. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2304.05302}
\BIBentrySTDinterwordspacing

\bibitem{Zhao-arxiv-2023-slichf}
Y.~Zhao, R.~Joshi, T.~Liu, M.~Khalman, M.~Saleh, and P.~J. Liu, ``Slic-hf:
  Sequence likelihood calibration with human feedback,'' \emph{CoRR}, vol.
  abs/2305.10425, 2023.

\bibitem{Zhang-arxiv-2023-The}
\BIBentryALTinterwordspacing
T.~Zhang, F.~Liu, J.~Wong, P.~Abbeel, and J.~E. Gonzalez, ``The wisdom of
  hindsight makes language models better instruction followers,'' \emph{CoRR},
  vol. abs/2302.05206, 2023. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2302.05206}
\BIBentrySTDinterwordspacing

\bibitem{Ahmed-ACM-2017-Imitation}
\BIBentryALTinterwordspacing
A.~Hussein, M.~M. Gaber, E.~Elyan, and C.~Jayne, ``Imitation learning: A survey
  of learning methods,'' \emph{ACM Comput. Surv.}, vol.~50, no.~2, apr 2017.
  [Online]. Available: \url{https://doi.org/10.1145/3054912}
\BIBentrySTDinterwordspacing

\bibitem{Levine-youtube-2022-Imitate}
\BIBentryALTinterwordspacing
S.~Levine, ``Should i imitate or reinforce,'' 2022. [Online]. Available:
  \url{https://www.youtube.com/watch?v=sVPm7zOrBxM}
\BIBentrySTDinterwordspacing

\bibitem{John-youtube-2023-RLHF}
\BIBentryALTinterwordspacing
J.~Schulman, ``Reinforcement learning from human feedback: Progress and
  challenges,'' 2023. [Online]. Available:
  \url{https://www.youtube.com/watch?v=hhiLw5Q_UFg}
\BIBentrySTDinterwordspacing

\bibitem{Li-ACL-2021-prefix}
X.~L. Li and P.~Liang, ``Prefix-tuning: Optimizing continuous prompts for
  generation,'' in \emph{Proceedings of the 59th Annual Meeting of the
  Association for Computational Linguistics and the 11th International Joint
  Conference on Natural Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long
  Papers), Virtual Event, August 1-6, 2021}, C.~Zong, F.~Xia, W.~Li, and
  R.~Navigli, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2021, pp. 4582--4597.

\bibitem{Lester-ACL-2021-The}
B.~Lester, R.~Al{-}Rfou, and N.~Constant, ``The power of scale for
  parameter-efficient prompt tuning,'' in \emph{Proceedings of the 2021
  Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2021,
  Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021},
  M.~Moens, X.~Huang, L.~Specia, and S.~W. Yih, Eds.\hskip 1em plus 0.5em minus
  0.4em\relax Association for Computational Linguistics, 2021, pp. 3045--3059.

\bibitem{Houlsby-ICML-2019-Parameter}
N.~Houlsby, A.~Giurgiu, S.~Jastrzebski, B.~Morrone, Q.~de~Laroussilhe,
  A.~Gesmundo, M.~Attariyan, and S.~Gelly, ``Parameter-efficient transfer
  learning for {NLP},'' in \emph{Proceedings of the 36th International
  Conference on Machine Learning, {ICML} 2019, 9-15 June 2019, Long Beach,
  California, {USA}}, 2019, pp. 2790--2799.

\bibitem{Hu-arXiv-2023}
Z.~Hu, Y.~Lan, L.~Wang, W.~Xu, E.~Lim, R.~K. Lee, L.~Bing, and S.~Poria,
  ``Llm-adapters: An adapter family for parameter-efficient fine-tuning of
  large language models,'' \emph{CoRR}, vol. abs/2304.01933, 2023.

\bibitem{He-ICLR-2022-towards}
J.~He, C.~Zhou, X.~Ma, T.~Berg{-}Kirkpatrick, and G.~Neubig, ``Towards a
  unified view of parameter-efficient transfer learning,'' in \emph{The Tenth
  International Conference on Learning Representations, {ICLR} 2022, Virtual
  Event, April 25-29, 2022}.\hskip 1em plus 0.5em minus 0.4em\relax
  OpenReview.net, 2022.

\bibitem{Liu-arXiv-2021-P-tuning}
X.~Liu, K.~Ji, Y.~Fu, Z.~Du, Z.~Yang, and J.~Tang, ``P-tuning v2: Prompt tuning
  can be comparable to fine-tuning universally across scales and tasks,''
  \emph{CoRR}, vol. abs/2110.07602, 2021.

\bibitem{Liu-arXiv-2021-GPT}
X.~Liu, Y.~Zheng, Z.~Du, M.~Ding, Y.~Qian, Z.~Yang, and J.~Tang, ``{GPT}
  understands, too,'' \emph{CoRR}, vol. abs/2103.10385, 2021.

\bibitem{gu-ACL-2022-ppt}
Y.~Gu, X.~Han, Z.~Liu, and M.~Huang, ``Ppt: Pre-trained prompt tuning for
  few-shot learning,'' in \emph{Proceedings of the 60th Annual Meeting of the
  Association for Computational Linguistics (Volume 1: Long Papers)}, 2022, pp.
  8410--8423.

\bibitem{jiang-TACL-2020-how}
Z.~Jiang, F.~F. Xu, J.~Araki, and G.~Neubig, ``How can we know what language
  models know?'' \emph{Transactions of the Association for Computational
  Linguistics}, vol.~8, pp. 423--438, 2020.

\bibitem{shin-EMNLP-2020-autoprompt}
T.~Shin, Y.~Razeghi, R.~L. Logan~IV, E.~Wallace, and S.~Singh, ``Autoprompt:
  Eliciting knowledge from language models with automatically generated
  prompts,'' in \emph{Proceedings of the 2020 Conference on Empirical Methods
  in Natural Language Processing (EMNLP)}, 2020, pp. 4222--4235.

\bibitem{Zhang-arXiv-2023-Adaptive}
\BIBentryALTinterwordspacing
Q.~Zhang, M.~Chen, A.~Bukharin, P.~He, Y.~Cheng, W.~Chen, and T.~Zhao,
  ``Adaptive budget allocation for parameter-efficient fine-tuning,''
  \emph{CoRR}, vol. abs/2303.10512, 2023. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2303.10512}
\BIBentrySTDinterwordspacing

\bibitem{Valipour-arXiv-2022-DyLoRA}
\BIBentryALTinterwordspacing
M.~Valipour, M.~Rezagholizadeh, I.~Kobyzev, and A.~Ghodsi, ``Dylora: Parameter
  efficient tuning of pre-trained models using dynamic search-free low-rank
  adaptation,'' \emph{CoRR}, vol. abs/2210.07558, 2022. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2210.07558}
\BIBentrySTDinterwordspacing

\bibitem{Ding-NMI-2023-Parameter}
N.~Ding, Y.~Qin, G.~Yang, F.~Wei, Y.~Zonghan, Y.~Su, S.~Hu, Y.~Chen, C.-M.
  Chan, W.~Chen, J.~Yi, W.~Zhao, X.~Wang, Z.~Liu, H.-T. Zheng, J.~Chen, Y.~Liu,
  J.~Tang, J.~Li, and M.~Sun, ``Parameter-efficient fine-tuning of large-scale
  pre-trained language models,'' \emph{Nature Machine Intelligence}, vol.~5,
  pp. 1--16, 03 2023.

\bibitem{Zhang-arXiv-2023-LLaMA-Adapter}
R.~Zhang, J.~Han, A.~Zhou, X.~Hu, S.~Yan, P.~Lu, H.~Li, P.~Gao, and Y.~Qiao,
  ``Llama-adapter: Efficient fine-tuning of language models with zero-init
  attention,'' \emph{CoRR}, vol. abs/2303.16199, 2023.

\bibitem{Pfeiffer-EMNLP-2022-MAD-X}
J.~Pfeiffer, I.~Vulic, I.~Gurevych, and S.~Ruder, ``{MAD-X:} an adapter-based
  framework for multi-task cross-lingual transfer,'' in \emph{Proceedings of
  the 2020 Conference on Empirical Methods in Natural Language Processing,
  {EMNLP} 2020, Online, November 16-20, 2020}, B.~Webber, T.~Cohn, Y.~He, and
  Y.~Liu, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2020, pp. 7654--7673.

\bibitem{peft-github-2022}
S.~Mangrulkar, S.~Gugger, L.~Debut, Y.~Belkada, and S.~Paul, ``Peft:
  State-of-the-art parameter-efficient fine-tuning methods,''
  \url{https://github.com/huggingface/peft}, 2022.

\bibitem{Gholami-CoRR-2022-A}
\BIBentryALTinterwordspacing
A.~Gholami, S.~Kim, Z.~Dong, Z.~Yao, M.~W. Mahoney, and K.~Keutzer, ``A survey
  of quantization methods for efficient neural network inference,''
  \emph{CoRR}, vol. abs/2103.13630, 2021. [Online]. Available:
  \url{https://arxiv.org/abs/2103.13630}
\BIBentrySTDinterwordspacing

\bibitem{Dettmers-arxiv-2022-LLM}
T.~Dettmers, M.~Lewis, Y.~Belkada, and L.~Zettlemoyer, ``Llm.int8(): 8-bit
  matrix multiplication for transformers at scale,'' \emph{CoRR}, vol.
  abs/2208.07339, 2022.

\bibitem{Xiao-CoRR-2022-SmoothQuant}
\BIBentryALTinterwordspacing
G.~Xiao, J.~Lin, M.~Seznec, J.~Demouth, and S.~Han, ``Smoothquant: Accurate and
  efficient post-training quantization for large language models,''
  \emph{CoRR}, vol. abs/2211.10438, 2022. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2211.10438}
\BIBentrySTDinterwordspacing

\bibitem{Yao-NeurlPS-2022-ZeroQuant}
Z.~Yao, R.~Y. Aminabadi, M.~Zhang, X.~Wu, C.~Li, and Y.~He, ``Zeroquant:
  Efficient and affordable post-training quantization for large-scale
  transformers,'' in \emph{NeurIPS}, 2022.

\bibitem{Lin-arXiv-2023-AWQ}
J.~Lin, J.~Tang, H.~Tang, S.~Yang, X.~Dang, and S.~Han, ``Awq: Activation-aware
  weight quantization for llm compression and acceleration,'' 2023.

\bibitem{frantar-arxiv-2022-gptq}
E.~Frantar, S.~Ashkboos, T.~Hoefler, and D.~Alistarh, ``Gptq: Accurate
  post-training quantization for generative pre-trained transformers,''
  \emph{arXiv preprint arXiv:2210.17323}, 2022.

\bibitem{Frantar-NeurIPS-2022-Optimal}
E.~Frantar and D.~Alistarh, ``Optimal brain compression: {A} framework for
  accurate post-training quantization and pruning,'' in \emph{NeurIPS}, 2022.

\bibitem{Dettmers-CoRR-2023-QLoRA}
T.~Dettmers, A.~Pagnoni, A.~Holtzman, and L.~Zettlemoyer, ``Qlora: Efficient
  finetuning of quantized llms,'' \emph{arXiv preprint arXiv:2305.14314}, 2023.

\bibitem{liu-2023-arxiv-LLM-QAT}
Z.~Liu, B.~Oguz, C.~Zhao, E.~Chang, P.~Stock, Y.~Mehdad, Y.~Shi,
  R.~Krishnamoorthi, and V.~Chandra, ``Llm-qat: Data-free quantization aware
  training for large language models,'' 2023.

\bibitem{Yao-CoRR-2023-ZeroQuant-V2}
Z.~Yao, X.~Wu, C.~Li, S.~Youn, and Y.~He, ``Zeroquant-v2: Exploring
  post-training quantization in llms from comprehensive study to low rank
  compensation,'' 2023.

\bibitem{Dettmers-2022-arxiv-case}
T.~Dettmers and L.~Zettlemoyer, ``The case for 4-bit precision: k-bit inference
  scaling laws,'' \emph{CoRR}, vol. abs/2212.09720, 2022.

\bibitem{Liu-2023-arxiv-Do_emergent}
L.~Peiyu, L.~Zikang, G.~Ze-Feng, G.~Dawei, Z.~W. Xin, L.~Yaliang, D.~Bolin, and
  W.~Ji-Rong, ``Do emergent abilities exist in quantized large language models:
  An empirical study,'' \emph{arXiv preprint arXiv:2307.08072}, 2023.

\bibitem{Dettmers-CoRR-2022-LLM.int8}
\BIBentryALTinterwordspacing
T.~Dettmers, M.~Lewis, Y.~Belkada, and L.~Zettlemoyer, ``Llm.int8(): 8-bit
  matrix multiplication for transformers at scale,'' \emph{CoRR}, vol.
  abs/2208.07339, 2022. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2208.07339}
\BIBentrySTDinterwordspacing

\bibitem{wei-arxiv-2023-zero}
X.~Wei, X.~Cui, N.~Cheng, X.~Wang, X.~Zhang, S.~Huang, P.~Xie, J.~Xu, Y.~Chen,
  M.~Zhang \emph{et~al.}, ``Zero-shot information extraction via chatting with
  chatgpt,'' \emph{arXiv preprint arXiv:2302.10205}, 2023.

\bibitem{Dettmers-ICLR-2022-8bit}
T.~Dettmers, M.~Lewis, S.~Shleifer, and L.~Zettlemoyer, ``8-bit optimizers via
  block-wise quantization,'' \emph{9th International Conference on Learning
  Representations, ICLR}, 2022.

\bibitem{Tao-ACL-2022-Compression}
C.~Tao, L.~Hou, W.~Zhang, L.~Shang, X.~Jiang, Q.~Liu, P.~Luo, and N.~Wong,
  ``Compression of generative pre-trained language models via quantization,''
  in \emph{Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin,
  Ireland, May 22-27, 2022}, S.~Muresan, P.~Nakov, and A.~Villavicencio,
  Eds.\hskip 1em plus 0.5em minus 0.4em\relax Association for Computational
  Linguistics, 2022, pp. 4821--4836.

\bibitem{Liu-ACL-2022-What}
J.~Liu, D.~Shen, Y.~Zhang, B.~Dolan, L.~Carin, and W.~Chen, ``What makes good
  in-context examples for gpt-3?'' in \emph{Proceedings of Deep Learning Inside
  Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep
  Learning Architectures, DeeLIO@ACL 2022, Dublin, Ireland and Online, May 27,
  2022}, 2022, pp. 100--114.

\bibitem{Rubin-NAACL-2022-Learning}
O.~Rubin, J.~Herzig, and J.~Berant, ``Learning to retrieve prompts for
  in-context learning,'' in \emph{Proceedings of the 2022 Conference of the
  North American Chapter of the Association for Computational Linguistics:
  Human Language Technologies, {NAACL} 2022, Seattle, WA, United States, July
  10-15, 2022}, 2022, pp. 2655--2671.

\bibitem{Kim-2022-arxiv-Self}
H.~J. Kim, H.~Cho, J.~Kim, T.~Kim, K.~M. Yoo, and S.~Lee, ``Self-generated
  in-context learning: Leveraging auto-regressive language models as a
  demonstration generator,'' \emph{CoRR}, vol. abs/2206.08082, 2022.

\bibitem{Zhou-2023-ICLR-Large}
Y.~Zhou, A.~I. Muresanu, Z.~Han, K.~Paster, S.~Pitis, H.~Chan, and J.~Ba,
  ``Large language models are human-level prompt engineers,'' in \emph{Proc. of
  ICLR}, 2023.

\bibitem{Lu-ACL-2022-Fantasically}
Y.~Lu, M.~Bartolo, A.~Moore, S.~Riedel, and P.~Stenetorp, ``Fantastically
  ordered prompts and where to find them: Overcoming few-shot prompt order
  sensitivity,'' in \emph{Proceedings of the 60th Annual Meeting of the
  Association for Computational Linguistics (Volume 1: Long Papers), {ACL}
  2022, Dublin, Ireland, May 22-27, 2022}, S.~Muresan, P.~Nakov, and
  A.~Villavicencio, Eds., 2022, pp. 8086--8098.

\bibitem{Fu-arxiv-2022-Complexity}
Y.~Fu, H.~Peng, A.~Sabharwal, P.~Clark, and T.~Khot, ``Complexity-based
  prompting for multi-step reasoning,'' \emph{CoRR}, vol. abs/2210.00720, 2022.

\bibitem{Zhang-arxiv-2022-Automatic}
Z.~Zhang, A.~Zhang, M.~Li, and A.~Smola, ``Automatic chain of thought prompting
  in large language models,'' \emph{CoRR}, vol. abs/2210.03493, 2022.

\bibitem{Creswell-2022-arXiv-selection}
A.~Creswell, M.~Shanahan, and I.~Higgins, ``Selection-inference: Exploiting
  large language models for interpretable logical reasoning,'' \emph{CoRR},
  vol. abs/2205.09712, 2022.

\bibitem{Wang-arxiv-2022-Self-Consistency}
X.~Wang, J.~Wei, D.~Schuurmans, Q.~V. Le, E.~H. Chi, and D.~Zhou,
  ``Self-consistency improves chain of thought reasoning in language models,''
  \emph{CoRR}, vol. abs/2203.11171, 2022.

\bibitem{Li-arxiv-2022-On}
Y.~Li, Z.~Lin, S.~Zhang, Q.~Fu, B.~Chen, J.~Lou, and W.~Chen, ``On the advance
  of making language models better reasoners,'' \emph{CoRR}, vol.
  abs/2206.02336, 2022.

\bibitem{Wang-arxiv-2022-Rationale}
X.~Wang, J.~Wei, D.~Schuurmans, Q.~V. Le, E.~H. Chi, and D.~Zhou,
  ``Rationale-augmented ensembles in language models,'' \emph{CoRR}, 2022.

\bibitem{Zhou-arxiv-2022-Least}
D.~Zhou, N.~Sch{\"{a}}rli, L.~Hou, J.~Wei, N.~Scales, X.~Wang, D.~Schuurmans,
  O.~Bousquet, Q.~Le, and E.~H. Chi, ``Least-to-most prompting enables complex
  reasoning in large language models,'' \emph{CoRR}, vol. abs/2205.10625, 2022.

\bibitem{Khot-2022-arXiv-Decomposed}
\BIBentryALTinterwordspacing
T.~Khot, H.~Trivedi, M.~Finlayson, Y.~Fu, K.~Richardson, P.~Clark, and
  A.~Sabharwal, ``Decomposed prompting: {A} modular approach for solving
  complex tasks,'' \emph{CoRR}, vol. abs/2210.02406, 2022. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2210.02406}
\BIBentrySTDinterwordspacing

\bibitem{Wang-arXiv-2023-Plan}
\BIBentryALTinterwordspacing
L.~Wang, W.~Xu, Y.~Lan, Z.~Hu, Y.~Lan, R.~K. Lee, and E.~Lim, ``Plan-and-solve
  prompting: Improving zero-shot chain-of-thought reasoning by large language
  models,'' \emph{CoRR}, vol. abs/2305.04091, 2023. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2305.04091}
\BIBentrySTDinterwordspacing

\bibitem{Lyu-arxiv-2023-Faithful}
Q.~Lyu, S.~Havaldar, A.~Stein, L.~Zhang, D.~Rao, E.~Wong, M.~Apidianaki, and
  C.~Callison{-}Burch, ``Faithful chain-of-thought reasoning,'' \emph{CoRR},
  vol. abs/2301.13379, 2023.

\bibitem{Gao-arxiv-2022-PAL}
L.~Gao, A.~Madaan, S.~Zhou, U.~Alon, P.~Liu, Y.~Yang, J.~Callan, and G.~Neubig,
  ``{PAL:} program-aided language models,'' \emph{CoRR}, vol. abs/2211.10435,
  2022.

\bibitem{Shen-2023-arXiv-Hugginggpt}
Y.~Shen, K.~Song, X.~Tan, D.~Li, W.~Lu, and Y.~Zhuang, ``Hugginggpt: Solving ai
  tasks with chatgpt and its friends in huggingface,'' \emph{arXiv preprint
  arXiv:2303.17580}, 2023.

\bibitem{Sun-2023-arXiv-adaplanner}
H.~Sun, Y.~Zhuang, L.~Kong, B.~Dai, and C.~Zhang, ``Adaplanner: Adaptive
  planning from feedback with language models,'' \emph{arXiv preprint
  arXiv:2305.16653}, 2023.

\bibitem{Lu-2023-arXiv-multimodal}
Y.~Lu, P.~Lu, Z.~Chen, W.~Zhu, X.~E. Wang, and W.~Y. Wang, ``Multimodal
  procedural planning via dual text-image prompting,'' \emph{CoRR}, vol.
  abs/2305.01795, 2023.

\bibitem{Hao-2023-arXiv-reasoning}
S.~Hao, Y.~Gu, H.~Ma, J.~J. Hong, Z.~Wang, D.~Z. Wang, and Z.~Hu, ``Reasoning
  with language model is planning with world model,'' \emph{CoRR}, vol.
  abs/2305.14992, 2023.

\bibitem{Chen-2023-arXiv-chatcot}
Z.~Chen, K.~Zhou, B.~Zhang, Z.~Gong, W.~X. Zhao, and J.~Wen, ``Chatcot:
  Tool-augmented chain-of-thought reasoning on chat-based large language
  models,'' \emph{CoRR}, vol. abs/2305.14323, 2023.

\bibitem{Yao-2022-arXiv-react}
S.~Yao, J.~Zhao, D.~Yu, N.~Du, I.~Shafran, K.~Narasimhan, and Y.~Cao, ``React:
  Synergizing reasoning and acting in language models,'' \emph{CoRR}, vol.
  abs/2210.03629, 2022.

\bibitem{Shinn-2023-arXiv-Reflexion}
N.~Shinn, F.~Cassano, B.~Labash, A.~Gopinath, K.~Narasimhan, and S.~Yao,
  ``Reflexion: Language agents with verbal reinforcement learning,'' 2023.

\bibitem{Yao-arxiv-2023-Tree}
S.~Yao, D.~Yu, J.~Zhao, I.~Shafran, T.~L. Griffiths, Y.~Cao, and K.~Narasimhan,
  ``Tree of thoughts: Deliberate problem solving with large language models,''
  \emph{CoRR}, vol. abs/2305.10601, 2023.

\bibitem{Liu-arxiv-2022-Design}
V.~Liu and L.~B. Chilton, ``Design guidelines for prompt engineering
  text-to-image generative models,'' in \emph{Proceedings of the 2022 CHI
  Conference on Human Factors in Computing Systems}, 2022, pp. 1--23.

\bibitem{White-arxiv-2023-Prompt}
J.~White, Q.~Fu, S.~Hays, M.~Sandborn, C.~Olea, H.~Gilbert, A.~Elnashar,
  J.~Spencer-Smith, and D.~C. Schmidt, ``A prompt pattern catalog to enhance
  prompt engineering with chatgpt,'' \emph{arXiv preprint arXiv:2302.11382},
  2023.

\bibitem{Santu-arxiv-2023-TELeR}
\BIBentryALTinterwordspacing
S.~K.~K. Santu and D.~Feng, ``Teler: {A} general taxonomy of {LLM} prompts for
  benchmarking complex tasks,'' \emph{CoRR}, vol. abs/2305.11430, 2023.
  [Online]. Available: \url{https://doi.org/10.48550/arXiv.2305.11430}
\BIBentrySTDinterwordspacing

\bibitem{OpenAI-OpenAI-2023-PromptGuide}
\BIBentryALTinterwordspacing
OpenAI, ``Gpt best practices,'' \emph{OpenAI}, 2023. [Online]. Available:
  \url{https://platform.openai.com/docs/guides/gpt-best-practices}
\BIBentrySTDinterwordspacing

\bibitem{Contributors-AIShort-2023-AIShort}
\BIBentryALTinterwordspacing
Contributors, ``Ai short,'' 2023. [Online]. Available:
  \url{https://www.aishort.top/}
\BIBentrySTDinterwordspacing

\bibitem{Contributors-Github-2023-Awesome}
\BIBentryALTinterwordspacing
------, ``Awesome chatgpt prompts,'' \emph{Github}, 2023. [Online]. Available:
  \url{https://github.com/f/awesome-chatgpt-prompts/}
\BIBentrySTDinterwordspacing

\bibitem{Jiang-2023-arxiv-StructGPT}
J.~Jiang, K.~Zhou, Z.~Dong, K.~Ye, W.~X. Zhao, and J.~Wen, ``Structgpt: {A}
  general framework for large language model to reason over structured data,''
  \emph{CoRR}, vol. abs/2305.09645, 2023.

\bibitem{Beurer-arxiv-2023-Prompting}
L.~Beurer-Kellner, M.~Fischer, and M.~Vechev, ``Prompting is programming: A
  query language for large language models,'' \emph{Proceedings of the ACM on
  Programming Languages}, vol.~7, no. PLDI, pp. 1946--1969, 2023.

\bibitem{Lu-arxiv-2023-Chameleon}
P.~Lu, B.~Peng, H.~Cheng, M.~Galley, K.-W. Chang, Y.~N. Wu, S.-C. Zhu, and
  J.~Gao, ``Chameleon: Plug-and-play compositional reasoning with large
  language models,'' \emph{arXiv preprint arXiv:2304.09842}, 2023.

\bibitem{Ren-arxiv-2023-Investigating}
R.~Ren, Y.~Wang, Y.~Qu, W.~X. Zhao, J.~Liu, H.~Tian, H.~Wu, J.-R. Wen, and
  H.~Wang, ``Investigating the factual knowledge boundary of large language
  models with retrieval augmentation,'' \emph{arXiv preprint arXiv:2307.11019},
  2023.

\bibitem{Hou-2023-arxiv-large}
Y.~Hou, J.~Zhang, Z.~Lin, H.~Lu, R.~Xie, J.~J. McAuley, and W.~X. Zhao, ``Large
  language models are zero-shot rankers for recommender systems,'' \emph{CoRR},
  vol. abs/2305.08845, 2023.

\bibitem{Chang-arxiv-2023-How}
\BIBentryALTinterwordspacing
S.~Chang and E.~Fosler{-}Lussier, ``How to prompt llms for text-to-sql: {A}
  study in zero-shot, single-domain, and cross-domain settings,'' \emph{CoRR},
  vol. abs/2305.11853, 2023. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2305.11853}
\BIBentrySTDinterwordspacing

\bibitem{Wen-CoRR-2023-Hard}
\BIBentryALTinterwordspacing
Y.~Wen, N.~Jain, J.~Kirchenbauer, M.~Goldblum, J.~Geiping, and T.~Goldstein,
  ``Hard prompts made easy: Gradient-based discrete optimization for prompt
  tuning and discovery,'' \emph{CoRR}, vol. abs/2302.03668, 2023. [Online].
  Available: \url{https://doi.org/10.48550/arXiv.2302.03668}
\BIBentrySTDinterwordspacing

\bibitem{Gao-ACL-2021-Making}
T.~Gao, A.~Fisch, and D.~Chen, ``Making pre-trained language models better
  few-shot learners,'' in \emph{Proceedings of the 59th Annual Meeting of the
  Association for Computational Linguistics and the 11th International Joint
  Conference on Natural Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long
  Papers), Virtual Event, August 1-6, 2021}, C.~Zong, F.~Xia, W.~Li, and
  R.~Navigli, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2021, pp. 3816--3830.

\bibitem{Zhou-CoRR-2023-InstructZero}
\BIBentryALTinterwordspacing
L.~Chen, J.~Chen, T.~Goldstein, H.~Huang, and T.~Zhou, ``Instructzero:
  Efficient instruction optimization for black-box large language models,''
  \emph{CoRR}, vol. abs/2306.03082, 2023. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2306.03082}
\BIBentrySTDinterwordspacing

\bibitem{Deng-EMNLP-2022-RLPrompt}
M.~Deng, J.~Wang, C.~Hsieh, Y.~Wang, H.~Guo, T.~Shu, M.~Song, E.~P. Xing, and
  Z.~Hu, ``Rlprompt: Optimizing discrete text prompts with reinforcement
  learning,'' in \emph{Proceedings of the 2022 Conference on Empirical Methods
  in Natural Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab
  Emirates, December 7-11, 2022}, Y.~Goldberg, Z.~Kozareva, and Y.~Zhang,
  Eds.\hskip 1em plus 0.5em minus 0.4em\relax Association for Computational
  Linguistics, 2022, pp. 3369--3391.

\bibitem{Zhang-ICLR-2023-TEMPERA}
T.~Zhang, X.~Wang, D.~Zhou, D.~Schuurmans, and J.~E. Gonzalez, ``{TEMPERA:}
  test-time prompt editing via reinforcement learning,'' in \emph{The Eleventh
  International Conference on Learning Representations, {ICLR} 2023, Kigali,
  Rwanda, May 1-5, 2023}.\hskip 1em plus 0.5em minus 0.4em\relax
  OpenReview.net, 2023.

\bibitem{Xu-EMNLP-2022-GPS}
H.~Xu, Y.~Chen, Y.~Du, N.~Shao, Y.~Wang, H.~Li, and Z.~Yang, ``{GPS:} genetic
  prompt search for efficient few-shot learning,'' in \emph{Proceedings of the
  2022 Conference on Empirical Methods in Natural Language Processing, {EMNLP}
  2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022}, Y.~Goldberg,
  Z.~Kozareva, and Y.~Zhang, Eds.\hskip 1em plus 0.5em minus 0.4em\relax
  Association for Computational Linguistics, 2022, pp. 8162--8171.

\bibitem{Prasad-EACL-2023-GrIPS}
A.~Prasad, P.~Hase, X.~Zhou, and M.~Bansal, ``Grips: Gradient-free, edit-based
  instruction search for prompting large language models,'' in
  \emph{Proceedings of the 17th Conference of the European Chapter of the
  Association for Computational Linguistics, {EACL} 2023, Dubrovnik, Croatia,
  May 2-6, 2023}, A.~Vlachos and I.~Augenstein, Eds.\hskip 1em plus 0.5em minus
  0.4em\relax Association for Computational Linguistics, 2023, pp. 3827--3846.

\bibitem{Zhou-ICLR-2023-Large}
Y.~Zhou, A.~I. Muresanu, Z.~Han, K.~Paster, S.~Pitis, H.~Chan, and J.~Ba,
  ``Large language models are human-level prompt engineers,'' in \emph{The
  Eleventh International Conference on Learning Representations, {ICLR} 2023,
  Kigali, Rwanda, May 1-5, 2023}.\hskip 1em plus 0.5em minus 0.4em\relax
  OpenReview.net, 2023.

\bibitem{Pryzant-CoRR-2023-Automatic}
\BIBentryALTinterwordspacing
R.~Pryzant, D.~Iter, J.~Li, Y.~T. Lee, C.~Zhu, and M.~Zeng, ``Automatic prompt
  optimization with "gradient descent" and beam search,'' \emph{CoRR}, vol.
  abs/2305.03495, 2023. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2305.03495}
\BIBentrySTDinterwordspacing

\bibitem{Yang-CoRR-2023-Large}
\BIBentryALTinterwordspacing
C.~Yang, X.~Wang, Y.~Lu, H.~Liu, Q.~V. Le, D.~Zhou, and X.~Chen, ``Large
  language models as optimizers,'' \emph{CoRR}, vol. abs/2309.03409, 2023.
  [Online]. Available: \url{https://doi.org/10.48550/arXiv.2309.03409}
\BIBentrySTDinterwordspacing

\bibitem{Wang-CoRR-2023-PromptAgent}
\BIBentryALTinterwordspacing
X.~Wang, C.~Li, Z.~Wang, F.~Bai, H.~Luo, J.~Zhang, N.~Jojic, E.~P. Xing, and
  Z.~Hu, ``Promptagent: Strategic planning with language models enables
  expert-level prompt optimization,'' \emph{CoRR}, vol. abs/2310.16427, 2023.
  [Online]. Available: \url{https://doi.org/10.48550/arXiv.2310.16427}
\BIBentrySTDinterwordspacing

\bibitem{Tang-COLING-2022-Context}
T.~Tang, J.~Li, W.~X. Zhao, and J.~Wen, ``Context-tuning: Learning
  contextualized prompts for natural language generation,'' in
  \emph{Proceedings of the 29th International Conference on Computational
  Linguistics, {COLING} 2022, Gyeongju, Republic of Korea, October 12-17,
  2022}, N.~Calzolari, C.~Huang, H.~Kim, J.~Pustejovsky, L.~Wanner, K.~Choi,
  P.~Ryu, H.~Chen, L.~Donatelli, H.~Ji, S.~Kurohashi, P.~Paggio, N.~Xue,
  S.~Kim, Y.~Hahm, Z.~He, T.~K. Lee, E.~Santus, F.~Bond, and S.~Na, Eds.\hskip
  1em plus 0.5em minus 0.4em\relax International Committee on Computational
  Linguistics, 2022, pp. 6340--6354.

\bibitem{Vu-ACL-2022-SPoT}
T.~Vu, B.~Lester, N.~Constant, R.~Al{-}Rfou', and D.~Cer, ``Spot: Better frozen
  model adaptation through soft prompt transfer,'' in \emph{Proceedings of the
  60th Annual Meeting of the Association for Computational Linguistics (Volume
  1: Long Papers), {ACL} 2022, Dublin, Ireland, May 22-27, 2022}, S.~Muresan,
  P.~Nakov, and A.~Villavicencio, Eds.\hskip 1em plus 0.5em minus 0.4em\relax
  Association for Computational Linguistics, 2022, pp. 5039--5059.

\bibitem{Li-NAACL-2022-Learning}
J.~Li, T.~Tang, J.~Nie, J.~Wen, and X.~Zhao, ``Learning to transfer prompts for
  text generation,'' in \emph{Proceedings of the 2022 Conference of the North
  American Chapter of the Association for Computational Linguistics: Human
  Language Technologies, {NAACL} 2022, Seattle, WA, United States, July 10-15,
  2022}, M.~Carpuat, M.~de~Marneffe, and I.~V.~M. Ru{\'{\i}}z, Eds.\hskip 1em
  plus 0.5em minus 0.4em\relax Association for Computational Linguistics, 2022,
  pp. 3506--3518.

\bibitem{Min-EMNLP-2022-Rethinking}
S.~Min, X.~Lyu, A.~Holtzman, M.~Artetxe, M.~Lewis, H.~Hajishirzi, and
  L.~Zettlemoyer, ``Rethinking the role of demonstrations: What makes
  in-context learning work?'' in \emph{Proceedings of the 2022 Conference on
  Empirical Methods in Natural Language Processing, {EMNLP} 2022, Abu Dhabi,
  United Arab Emirates, December 7-11, 2022}.\hskip 1em plus 0.5em minus
  0.4em\relax Association for Computational Linguistics, 2022, pp.
  11\,048--11\,064.

\bibitem{Zhao-ICML-2021-Calibrate}
Z.~Zhao, E.~Wallace, S.~Feng, D.~Klein, and S.~Singh, ``Calibrate before use:
  Improving few-shot performance of language models,'' in \emph{Proceedings of
  the 38th International Conference on Machine Learning, {ICML} 2021, 18-24
  July 2021, Virtual Event}, M.~Meila and T.~Zhang, Eds., 2021, pp.
  12\,697--12\,706.

\bibitem{Lee-COLING-2022-Does}
Y.~Lee, C.~Lim, and H.~Choi, ``Does {GPT-3} generate empathetic dialogues? {A}
  novel in-context example selection method and automatic evaluation metric for
  empathetic dialogue generation,'' in \emph{Proceedings of the 29th
  International Conference on Computational Linguistics, {COLING} 2022,
  Gyeongju, Republic of Korea, October 12-17, 2022}, N.~Calzolari, C.~Huang,
  H.~Kim, J.~Pustejovsky, L.~Wanner, K.~Choi, P.~Ryu, H.~Chen, L.~Donatelli,
  H.~Ji, S.~Kurohashi, P.~Paggio, N.~Xue, S.~Kim, Y.~Hahm, Z.~He, T.~K. Lee,
  E.~Santus, F.~Bond, and S.~Na, Eds.\hskip 1em plus 0.5em minus 0.4em\relax
  International Committee on Computational Linguistics, 2022, pp. 669--683.

\bibitem{Levy-arxiv-2022-Diverse}
I.~Levy, B.~Bogin, and J.~Berant, ``Diverse demonstrations improve in-context
  compositional generalization,'' \emph{CoRR}, vol. abs/2212.06800, 2022.

\bibitem{Su-arxiv-2022-selective}
H.~Su, J.~Kasai, C.~H. Wu, W.~Shi, T.~Wang, J.~Xin, R.~Zhang, M.~Ostendorf,
  L.~Zettlemoyer, N.~A. Smith, and T.~Yu, ``Selective annotation makes language
  models better few-shot learners,'' \emph{CoRR}, 2022.

\bibitem{Ye-arxiv-2022-Complementary}
X.~Ye, S.~Iyer, A.~Celikyilmaz, V.~Stoyanov, G.~Durrett, and R.~Pasunuru,
  ``Complementary explanations for effective in-context learning,''
  \emph{CoRR}, 2022.

\bibitem{Li-arxiv-2023-Finding}
X.~Li and X.~Qiu, ``Finding supporting examples for in-context learning,''
  \emph{CoRR}, 2023.

\bibitem{Zhang-EMNLP-2022-Active}
Y.~Zhang, S.~Feng, and C.~Tan, ``Active example selection for in-context
  learning,'' in \emph{Proceedings of the 2022 Conference on Empirical Methods
  in Natural Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab
  Emirates, December 7-11, 2022}, 2022, pp. 9134--9148.

\bibitem{Gilardi-arXiv-2023-Crowd}
F.~Gilardi, M.~Alizadeh, and M.~Kubli, ``Chatgpt outperforms crowd-workers for
  text-annotation tasks,'' 2023.

\bibitem{Kim-arxiv-2022-Self-Generated}
H.~J. Kim, H.~Cho, J.~Kim, T.~Kim, K.~M. Yoo, and S.~Lee, ``Self-generated
  in-context learning: Leveraging auto-regressive language models as a
  demonstration generator,'' \emph{CoRR}, vol. abs/2206.08082, 2022.

\bibitem{Michael-ICLR-2022-An}
S.~M. Xie, A.~Raghunathan, P.~Liang, and T.~Ma, ``An explanation of in-context
  learning as implicit bayesian inference,'' in \emph{The Tenth International
  Conference on Learning Representations, {ICLR} 2022, Virtual Event, April
  25-29, 2022}, 2022.

\bibitem{Wu-arxiv-2022-Self}
Z.~Wu, Y.~Wang, J.~Ye, and L.~Kong, ``Self-adaptive in-context learning,''
  \emph{CoRR}, vol. abs/2212.10375, 2022.

\bibitem{Gu-arXiv-2023-Pre}
Y.~Gu, L.~Dong, F.~Wei, and M.~Huang, ``Pre-training to learn in context,''
  \emph{CoRR}, vol. abs/2305.09137, 2023.

\bibitem{Min-NAACL-2022-MetaICL}
S.~Min, M.~Lewis, L.~Zettlemoyer, and H.~Hajishirzi, ``Metaicl: Learning to
  learn in context,'' in \emph{Proceedings of the 2022 Conference of the North
  American Chapter of the Association for Computational Linguistics: Human
  Language Technologies, {NAACL} 2022, Seattle, WA, United States, July 10-15,
  2022}, M.~Carpuat, M.~de~Marneffe, and I.~V.~M. Ru{\'{\i}}z, Eds., 2022, pp.
  2791--2809.

\bibitem{Hahn-2023-arXiv-a}
M.~Hahn and N.~Goyal, ``A theory of emergent in-context learning as implicit
  structure induction,'' \emph{CoRR}, vol. abs/2303.07971, 2023.

\bibitem{Pan-2023-arXiv-what}
J.~Pan, T.~Gao, H.~Chen, and D.~Chen, ``What in-context learning "learns"
  in-context: Disentangling task recognition and task learning,'' \emph{CoRR},
  vol. abs/2305.09731, 2023.

\bibitem{Wies-2023-arXiv-the}
N.~Wies, Y.~Levine, and A.~Shashua, ``The learnability of in-context
  learning,'' \emph{CoRR}, vol. abs/2303.07895, 2023.

\bibitem{Webson-2022-NAACL-do}
A.~Webson and E.~Pavlick, ``Do prompt-based models really understand the
  meaning of their prompts?'' in \emph{Proceedings of the 2022 Conference of
  the North American Chapter of the Association for Computational Linguistics:
  Human Language Technologies, {NAACL} 2022, Seattle, WA, United States, July
  10-15, 2022}, 2022, pp. 2300--2344.

\bibitem{Oswald-arxiv-2022-Transformers}
J.~von Oswald, E.~Niklasson, E.~Randazzo, J.~Sacramento, A.~Mordvintsev,
  A.~Zhmoginov, and M.~Vladymyrov, ``Transformers learn in-context by gradient
  descent,'' \emph{CoRR}, vol. abs/2212.07677, 2022.

\bibitem{Olsson-arxiv-2022-In}
C.~Olsson, N.~Elhage, N.~Nanda, N.~Joseph, N.~DasSarma, T.~Henighan, B.~Mann,
  A.~Askell, Y.~Bai, A.~Chen, T.~Conerly, D.~Drain, D.~Ganguli,
  Z.~Hatfield{-}Dodds, D.~Hernandez, S.~Johnston, A.~Jones, J.~Kernion,
  L.~Lovitt, K.~Ndousse, D.~Amodei, T.~Brown, J.~Clark, J.~Kaplan,
  S.~McCandlish, and C.~Olah, ``In-context learning and induction heads,''
  \emph{CoRR}, vol. abs/2209.11895, 2022.

\bibitem{rek-arxiv-2022-what}
E.~Aky{\"{u}}rek, D.~Schuurmans, J.~Andreas, T.~Ma, and D.~Zhou, ``What
  learning algorithm is in-context learning? investigations with linear
  models,'' \emph{CoRR}, vol. abs/2211.15661, 2022.

\bibitem{Wei-arxiv-2023-Larger}
J.~Wei, J.~Wei, Y.~Tay, D.~Tran, A.~Webson, Y.~Lu, X.~Chen, H.~Liu, D.~Huang,
  D.~Zhou \emph{et~al.}, ``Larger language models do in-context learning
  differently,'' \emph{arXiv preprint arXiv:2303.03846}, 2023.

\bibitem{Forno-2023-arXiv-meta}
J.~Coda{-}Forno, M.~Binz, Z.~Akata, M.~M. Botvinick, J.~X. Wang, and E.~Schulz,
  ``Meta-in-context learning in large language models,'' \emph{CoRR}, vol.
  abs/2305.12907, 2023.

\bibitem{Wei-2023-arXiv-symbol}
J.~W. Wei, L.~Hou, A.~K. Lampinen, X.~Chen, D.~Huang, Y.~Tay, X.~Chen, Y.~Lu,
  D.~Zhou, T.~Ma, and Q.~V. Le, ``Symbol tuning improves in-context learning in
  language models,'' \emph{CoRR}, vol. abs/2305.08298, 2023.

\bibitem{Chu-arxiv-2023-A}
Z.~Chu, J.~Chen, Q.~Chen, W.~Yu, T.~He, H.~Wang, W.~Peng, M.~Liu, B.~Qin, and
  T.~Liu, ``A survey of chain of thought reasoning: Advances, frontiers and
  future,'' \emph{CoRR}, vol. abs/2309.15402, 2023.

\bibitem{Miao-ACL-2020-A}
S.~Miao, C.~Liang, and K.~Su, ``A diverse corpus for evaluating and developing
  english math word problem solvers,'' in \emph{Proceedings of the 58th Annual
  Meeting of the Association for Computational Linguistics, {ACL} 2020, Online,
  July 5-10, 2020}, D.~Jurafsky, J.~Chai, N.~Schluter, and J.~R. Tetreault,
  Eds.\hskip 1em plus 0.5em minus 0.4em\relax Association for Computational
  Linguistics, 2020, pp. 975--984.

\bibitem{Talmor-naacl-2019-CommonsenseQA}
A.~Talmor, J.~Herzig, N.~Lourie, and J.~Berant, ``Commonsenseqa: {A} question
  answering challenge targeting commonsense knowledge,'' in \emph{Proceedings
  of the 2019 Conference of the North American Chapter of the Association for
  Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2019,
  Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)},
  J.~Burstein, C.~Doran, and T.~Solorio, Eds.\hskip 1em plus 0.5em minus
  0.4em\relax Association for Computational Linguistics, 2019, pp. 4149--4158.

\bibitem{Kojima-arxiv-2022-Large}
T.~Kojima, S.~S. Gu, M.~Reid, Y.~Matsuo, and Y.~Iwasawa, ``Large language
  models are zero-shot reasoners,'' \emph{CoRR}, vol. abs/2205.11916, 2022.

\bibitem{Chen-arxiv-2022-Program}
W.~Chen, X.~Ma, X.~Wang, and W.~W. Cohen, ``Program of thoughts prompting:
  Disentangling computation from reasoning for numerical reasoning tasks,''
  \emph{CoRR}, vol. abs/2211.12588, 2022.

\bibitem{Gao-ICML-2023-PAL}
L.~Gao, A.~Madaan, S.~Zhou, U.~Alon, P.~Liu, Y.~Yang, J.~Callan, and G.~Neubig,
  ``{PAL:} program-aided language models,'' in \emph{International Conference
  on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}},
  A.~Krause, E.~Brunskill, K.~Cho, B.~Engelhardt, S.~Sabato, and J.~Scarlett,
  Eds., 2023.

\bibitem{Zhao-arxiv-2023-Automatic}
X.~Zhao, Y.~Xie, K.~Kawaguchi, J.~He, and Q.~Xie, ``Automatic model selection
  with large language models for reasoning,'' \emph{CoRR}, vol. abs/2305.14333,
  2023.

\bibitem{Li-arxiv-2023-Making}
Y.~Li, Z.~Lin, S.~Zhang, Q.~Fu, B.~Chen, J.-G. Lou, and W.~Chen, ``Making large
  language models better reasoners with step-aware verifier,'' 2023.

\bibitem{Yoran-arxiv-2023-Answering}
O.~Yoran, T.~Wolfson, B.~Bogin, U.~Katz, D.~Deutch, and J.~Berant, ``Answering
  questions by meta-reasoning over multiple chains of thought,'' \emph{CoRR},
  vol. abs/2304.13007, 2023.

\bibitem{Ling-arxiv-2023-Deductive}
Z.~Ling, Y.~Fang, X.~Li, Z.~Huang, M.~Lee, R.~Memisevic, and H.~Su, ``Deductive
  verification of chain-of-thought reasoning,'' \emph{CoRR}, vol.
  abs/2306.03872, 2023.

\bibitem{Xue-arxiv-2023-RCOT}
T.~Xue, Z.~Wang, Z.~Wang, C.~Han, P.~Yu, and H.~Ji, ``{RCOT:} detecting and
  rectifying factual inconsistency in reasoning by reversing
  chain-of-thought,'' \emph{CoRR}, vol. abs/2305.11499, 2023.

\bibitem{Weng-arxiv-2023-Large}
Y.~Weng, M.~Zhu, F.~Xia, B.~Li, S.~He, K.~Liu, and J.~Zhao, ``Large language
  models are better reasoners with self-verification,'' \emph{CoRR,
  abs/2212.09561}, 2023.

\bibitem{Jiang-arxiv-2023-Forward}
W.~Jiang, H.~Shi, L.~Yu, Z.~Liu, Y.~Zhang, Z.~Li, and J.~T. Kwok,
  ``Forward-backward reasoning in large language models for mathematical
  verification,'' 2023.

\bibitem{Long-arxiv-2023-Large}
J.~Long, ``Large language model guided tree-of-thought,'' \emph{CoRR}, vol.
  abs/2305.08291, 2023.

\bibitem{Mo-arxiv-2023-Tree}
S.~Mo and M.~Xin, ``Tree of uncertain thoughts reasoning for large language
  models,'' \emph{CoRR}, vol. abs/2309.07694, 2023.

\bibitem{Besta-arxiv-2023-Graph}
M.~Besta, N.~Blach, A.~Kubicek, R.~Gerstenberger, L.~Gianinazzi, J.~Gajda,
  T.~Lehmann, M.~Podstawski, H.~Niewiadomski, P.~Nyczyk, and T.~Hoefler,
  ``Graph of thoughts: Solving elaborate problems with large language models,''
  \emph{CoRR}, vol. abs/2308.09687, 2023.

\bibitem{Lei-arxiv-2023-Boosting}
B.~Lei, P.~Lin, C.~Liao, and C.~Ding, ``Boosting logical reasoning in large
  language models through a new framework: The graph of thought,'' \emph{CoRR},
  vol. abs/2308.08614, 2023.

\bibitem{ding-arxiv-2023-everything}
R.~Ding, C.~Zhang, L.~Wang, Y.~Xu, M.~Ma, W.~Zhang, S.~Qin, S.~Rajmohan,
  Q.~Lin, and D.~Zhang, ``Everything of thoughts: Defying the law of penrose
  triangle for thought generation,'' \emph{arXiv preprint arXiv:2311.04254},
  2023.

\bibitem{Liang-arxiv-2022-Holistic}
P.~Liang, R.~Bommasani, T.~Lee, D.~Tsipras, D.~Soylu, M.~Yasunaga, Y.~Zhang,
  D.~Narayanan, Y.~Wu, A.~Kumar, B.~Newman, B.~Yuan, B.~Yan, C.~Zhang,
  C.~Cosgrove, C.~D. Manning, C.~R{\'{e}}, D.~Acosta{-}Navas, D.~A. Hudson,
  E.~Zelikman, E.~Durmus, F.~Ladhak, F.~Rong, H.~Ren, H.~Yao, J.~Wang,
  K.~Santhanam, L.~J. Orr, L.~Zheng, M.~Y{\"{u}}ksekg{\"{o}}n{\"{u}}l,
  M.~Suzgun, N.~Kim, N.~Guha, N.~S. Chatterji, O.~Khattab, P.~Henderson,
  Q.~Huang, R.~Chi, S.~M. Xie, S.~Santurkar, S.~Ganguli, T.~Hashimoto,
  T.~Icard, T.~Zhang, V.~Chaudhary, W.~Wang, X.~Li, Y.~Mai, Y.~Zhang, and
  Y.~Koreeda, ``Holistic evaluation of language models,'' \emph{CoRR}, vol.
  abs/2211.09110, 2022.

\bibitem{Bi-arxiv-2023-When}
Z.~Bi, N.~Zhang, Y.~Jiang, S.~Deng, G.~Zheng, and H.~Chen, ``When do
  program-of-thoughts work for reasoning?'' \emph{CoRR}, vol. abs/2308.15452,
  2023.

\bibitem{Madaan-arxiv-2022-Text}
A.~Madaan and A.~Yazdanbakhsh, ``Text and patterns: For effective chain of
  thought, it takes two to tango,'' \emph{CoRR}, vol. abs/2209.07686, 2022.

\bibitem{Zhang-arxiv-2022-Multimodal}
Z.~Zhang, A.~Zhang, M.~Li, H.~Zhao, G.~Karypis, and A.~Smola, ``Multimodal
  chain-of-thought reasoning in language models,'' \emph{CoRR}, vol.
  abs/2302.00923, 2023.

\bibitem{Shi-arxiv-2022-Language}
F.~Shi, M.~Suzgun, M.~Freitag, X.~Wang, S.~Srivats, S.~Vosoughi, H.~W. Chung,
  Y.~Tay, S.~Ruder, D.~Zhou, D.~Das, and J.~Wei, ``Language models are
  multilingual chain-of-thought reasoners,'' \emph{CoRR}, vol. abs/2210.03057,
  2022.

\bibitem{Qian-2022-arXiv-limitations}
J.~Qian, H.~Wang, Z.~Li, S.~Li, and X.~Yan, ``Limitations of language models in
  arithmetic and symbolic induction,'' \emph{CoRR}, vol. abs/2208.05051, 2022.

\bibitem{Ning-arxiv-2023-ChatGPT}
N.~{Bian}, X.~{Han}, L.~{Sun}, H.~{Lin}, Y.~{Lu}, and B.~{He}, ``{ChatGPT is a
  Knowledgeable but Inexperienced Solver: An Investigation of Commonsense
  Problem in Large Language Models},'' \emph{CoRR}, 2023.

\bibitem{Yao-2023-arXiv-tree}
S.~Yao, D.~Yu, J.~Zhao, I.~Shafran, T.~L. Griffiths, Y.~Cao, and K.~Narasimhan,
  ``Tree of thoughts: Deliberate problem solving with large language models,''
  \emph{CoRR}, vol. abs/2305.10601, 2023.

\bibitem{Wang-2023-arXiv-voyager}
G.~Wang, Y.~Xie, Y.~Jiang, A.~Mandlekar, C.~Xiao, Y.~Zhu, L.~Fan, and
  A.~Anandkumar, ``Voyager: An open-ended embodied agent with large language
  models,'' \emph{arXiv preprint arXiv:2305.16291}, 2023.

\bibitem{Jiang-arXiv-2023-Self}
\BIBentryALTinterwordspacing
X.~Jiang, Y.~Dong, L.~Wang, Q.~Shang, and G.~Li, ``Self-planning code
  generation with large language model,'' \emph{CoRR}, vol. abs/2303.06689,
  2023. [Online]. Available: \url{https://doi.org/10.48550/arXiv.2303.06689}
\BIBentrySTDinterwordspacing

\bibitem{Singh-arxiv-2022-ProgPrompt}
I.~Singh, V.~Blukis, A.~Mousavian, A.~Goyal, D.~Xu, J.~Tremblay, D.~Fox,
  J.~Thomason, and A.~Garg, ``Progprompt: Generating situated robot task plans
  using large language models,'' \emph{CoRR}, vol. abs/2209.11302, 2022.

\bibitem{Liu-2023-arXiv-LLM+P}
\BIBentryALTinterwordspacing
B.~Liu, Y.~Jiang, X.~Zhang, Q.~Liu, S.~Zhang, J.~Biswas, and P.~Stone,
  ``{LLM+P:} empowering large language models with optimal planning
  proficiency,'' \emph{CoRR}, vol. abs/2304.11477, 2023. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2304.11477}
\BIBentrySTDinterwordspacing

\bibitem{Rombach-2022-CVPR-high}
R.~Rombach, A.~Blattmann, D.~Lorenz, P.~Esser, and B.~Ommer, ``High-resolution
  image synthesis with latent diffusion models,'' in \emph{{IEEE/CVF}
  Conference on Computer Vision and Pattern Recognition, {CVPR} 2022, New
  Orleans, LA, USA, June 18-24, 2022}, 2022, pp. 10\,674--10\,685.

\bibitem{Park-arxiv-2023-Generative}
J.~S. Park, J.~C. O'Brien, C.~J. Cai, M.~R. Morris, P.~Liang, and M.~S.
  Bernstein, ``Generative agents: Interactive simulacra of human behavior,''
  \emph{CoRR}, vol. abs/2304.03442, 2023.

\bibitem{AutoGPT}
\BIBentryALTinterwordspacing
2023. [Online]. Available:
  \url{https://github.com/Significant-Gravitas/Auto-GPT}
\BIBentrySTDinterwordspacing

\bibitem{Wang-2023-arXiv-describe}
Z.~Wang, S.~Cai, A.~Liu, X.~Ma, and Y.~Liang, ``Describe, explain, plan and
  select: Interactive planning with large language models enables open-world
  multi-task agents,'' \emph{CoRR}, vol. abs/2302.01560, 2023.

\bibitem{Wang-2021-ICDM-Milvus}
J.~Wang, X.~Yi, R.~Guo, H.~Jin, P.~Xu, S.~Li, X.~Wang, X.~Guo, C.~Li, X.~Xu
  \emph{et~al.}, ``Milvus: A purpose-built vector data management system,'' in
  \emph{Proceedings of the 2021 International Conference on Management of
  Data}, 2021, pp. 2614--2627.

\bibitem{Zhong-2023-arxiv-MemoryBank}
W.~Zhong, L.~Guo, Q.~Gao, H.~Ye, and Y.~Wang, ``Memorybank: Enhancing large
  language models with long-term memory,'' \emph{CoRR}, vol. abs/2305.10250,
  2023.

\bibitem{Marcus-CL-1993-Building}
M.~P. Marcus, B.~Santorini, and M.~A. Marcinkiewicz, ``Building a large
  annotated corpus of english: The penn treebank,'' \emph{Comput. Linguistics},
  vol.~19, no.~2, pp. 313--330, 1993.

\bibitem{Merity-ICLR-2017-Pointer}
S.~Merity, C.~Xiong, J.~Bradbury, and R.~Socher, ``Pointer sentinel mixture
  models,'' in \emph{{ICLR} (Poster)}.\hskip 1em plus 0.5em minus 0.4em\relax
  OpenReview.net, 2017.

\bibitem{Bojar-WMT-2014-Findings}
O.~Bojar, C.~Buck, C.~Federmann, B.~Haddow, P.~Koehn, J.~Leveling, C.~Monz,
  P.~Pecina, M.~Post, H.~Saint{-}Amand, R.~Soricut, L.~Specia, and A.~Tamchyna,
  ``Findings of the 2014 workshop on statistical machine translation,'' in
  \emph{WMT@ACL}.\hskip 1em plus 0.5em minus 0.4em\relax The Association for
  Computer Linguistics, 2014, pp. 12--58.

\bibitem{Bojar-WMT-2016-Findings}
O.~Bojar, R.~Chatterjee, C.~Federmann, Y.~Graham, B.~Haddow, M.~Huck,
  A.~Jimeno{-}Yepes, P.~Koehn, V.~Logacheva, C.~Monz, M.~Negri,
  A.~N{\'{e}}v{\'{e}}ol, M.~L. Neves, M.~Popel, M.~Post, R.~Rubino, C.~Scarton,
  L.~Specia, M.~Turchi, K.~Verspoor, and M.~Zampieri, ``Findings of the 2016
  conference on machine translation,'' in \emph{{WMT}}.\hskip 1em plus 0.5em
  minus 0.4em\relax The Association for Computer Linguistics, 2016, pp.
  131--198.

\bibitem{Barrault-WMT-2019-Findings}
L.~Barrault, O.~Bojar, M.~R. Costa{-}juss{\`{a}}, C.~Federmann, M.~Fishel,
  Y.~Graham, B.~Haddow, M.~Huck, P.~Koehn, S.~Malmasi, C.~Monz,
  M.~M{\"{u}}ller, S.~Pal, M.~Post, and M.~Zampieri, ``Findings of the 2019
  conference on machine translation {(WMT19)},'' in \emph{Proceedings of the
  Fourth Conference on Machine Translation, {WMT} 2019, Florence, Italy, August
  1-2, 2019 - Volume 2: Shared Task Papers, Day 1}, O.~Bojar, R.~Chatterjee,
  C.~Federmann, M.~Fishel, Y.~Graham, B.~Haddow, M.~Huck, A.~Jimeno{-}Yepes,
  P.~Koehn, A.~Martins, C.~Monz, M.~Negri, A.~N{\'{e}}v{\'{e}}ol, M.~L. Neves,
  M.~Post, M.~Turchi, and K.~Verspoor, Eds.\hskip 1em plus 0.5em minus
  0.4em\relax Association for Computational Linguistics, 2019, pp. 1--61.

\bibitem{Barrault-WMT-2020-Findings}
L.~Barrault, M.~Biesialska, O.~Bojar, M.~R. Costa{-}juss{\`{a}}, C.~Federmann,
  Y.~Graham, R.~Grundkiewicz, B.~Haddow, M.~Huck, E.~Joanis, T.~Kocmi,
  P.~Koehn, C.~Lo, N.~Ljubesic, C.~Monz, M.~Morishita, M.~Nagata, T.~Nakazawa,
  S.~Pal, M.~Post, and M.~Zampieri, ``Findings of the 2020 conference on
  machine translation {(WMT20)},'' in \emph{Proceedings of the Fifth Conference
  on Machine Translation, WMT@EMNLP 2020, Online, November 19-20, 2020},
  L.~Barrault, O.~Bojar, F.~Bougares, R.~Chatterjee, M.~R. Costa{-}juss{\`{a}},
  C.~Federmann, M.~Fishel, A.~Fraser, Y.~Graham, P.~Guzman, B.~Haddow, M.~Huck,
  A.~Jimeno{-}Yepes, P.~Koehn, A.~Martins, M.~Morishita, C.~Monz, M.~Nagata,
  T.~Nakazawa, and M.~Negri, Eds.\hskip 1em plus 0.5em minus 0.4em\relax
  Association for Computational Linguistics, 2020, pp. 1--55.

\bibitem{Akhbardeh-WMT-2021-Findings}
F.~Akhbardeh, A.~Arkhangorodsky, M.~Biesialska, O.~Bojar, R.~Chatterjee,
  V.~Chaudhary, M.~R. Costa{-}juss{\`{a}}, C.~Espa{\~{n}}a{-}Bonet, A.~Fan,
  C.~Federmann, M.~Freitag, Y.~Graham, R.~Grundkiewicz, B.~Haddow, L.~Harter,
  K.~Heafield, C.~Homan, M.~Huck, K.~Amponsah{-}Kaakyire, J.~Kasai,
  D.~Khashabi, K.~Knight, T.~Kocmi, P.~Koehn, N.~Lourie, C.~Monz, M.~Morishita,
  M.~Nagata, A.~Nagesh, T.~Nakazawa, M.~Negri, S.~Pal, A.~A. Tapo, M.~Turchi,
  V.~Vydrin, and M.~Zampieri, ``Findings of the 2021 conference on machine
  translation {(WMT21)},'' in \emph{Proceedings of the Sixth Conference on
  Machine Translation, WMT@EMNLP 2021, Online Event, November 10-11, 2021},
  L.~Barrault, O.~Bojar, F.~Bougares, R.~Chatterjee, M.~R. Costa{-}juss{\`{a}},
  C.~Federmann, M.~Fishel, A.~Fraser, M.~Freitag, Y.~Graham, R.~Grundkiewicz,
  P.~Guzman, B.~Haddow, M.~Huck, A.~Jimeno{-}Yepes, P.~Koehn, T.~Kocmi,
  A.~Martins, M.~Morishita, and C.~Monz, Eds.\hskip 1em plus 0.5em minus
  0.4em\relax Association for Computational Linguistics, 2021, pp. 1--88.

\bibitem{Kocmi-WMT-2022-Findings}
T.~Kocmi, R.~Bawden, O.~Bojar, A.~Dvorkovich, C.~Federmann, M.~Fishel,
  T.~Gowda, Y.~Graham, R.~Grundkiewicz, B.~Haddow, R.~Knowles, P.~Koehn,
  C.~Monz, M.~Morishita, M.~Nagata, T.~Nakazawa, M.~Nov{\'{a}}k, M.~Popel, and
  M.~Popovic, ``Findings of the 2022 conference on machine translation
  {(WMT22)},'' in \emph{Proceedings of the Seventh Conference on Machine
  Translation, {WMT} 2022, Abu Dhabi, United Arab Emirates (Hybrid), December
  7-8, 2022}, P.~Koehn, L.~Barrault, O.~Bojar, F.~Bougares, R.~Chatterjee,
  M.~R. Costa{-}juss{\`{a}}, C.~Federmann, M.~Fishel, A.~Fraser, M.~Freitag,
  Y.~Graham, R.~Grundkiewicz, P.~Guzman, B.~Haddow, M.~Huck, A.~Jimeno{-}Yepes,
  T.~Kocmi, A.~Martins, M.~Morishita, C.~Monz, M.~Nagata, T.~Nakazawa,
  M.~Negri, A.~N{\'{e}}v{\'{e}}ol, M.~Neves, M.~Popel, M.~Turchi, and
  M.~Zampieri, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2022, pp. 1--45.

\bibitem{Goyal-TACL-2022-The}
N.~Goyal, C.~Gao, V.~Chaudhary, P.~Chen, G.~Wenzek, D.~Ju, S.~Krishnan,
  M.~Ranzato, F.~Guzm{\'{a}}n, and A.~Fan, ``The flores-101 evaluation
  benchmark for low-resource and multilingual machine translation,''
  \emph{Trans. Assoc. Comput. Linguistics}, vol.~10, pp. 522--538, 2022.

\bibitem{Bawden-journal-2021-DiaBLa}
R.~Bawden, E.~Bilinski, T.~Lavergne, and S.~Rosset, ``Diabla: a corpus of
  bilingual spontaneous written dialogues for machine translation,''
  \emph{Lang. Resour. Evaluation}, vol.~55, no.~3, pp. 635--660, 2021.

\bibitem{Nallapati-acl-2016-Abstractive}
R.~Nallapati, B.~Zhou, C.~N. dos Santos, {\c{C}}.~G{\"{u}}l{\c{c}}ehre, and
  B.~Xiang, ``Abstractive text summarization using sequence-to-sequence rnns
  and beyond,'' in \emph{Proceedings of the 20th {SIGNLL} Conference on
  Computational Natural Language Learning, CoNLL 2016, Berlin, Germany, August
  11-12, 2016}, Y.~Goldberg and S.~Riezler, Eds.\hskip 1em plus 0.5em minus
  0.4em\relax {ACL}, 2016, pp. 280--290.

\bibitem{Naryan-EMNLP-2018-XSUM}
S.~Narayan, S.~B. Cohen, and M.~Lapata, ``Don't give me the details, just the
  summary! topic-aware convolutional neural networks for extreme
  summarization,'' in \emph{{EMNLP}}.\hskip 1em plus 0.5em minus 0.4em\relax
  Association for Computational Linguistics, 2018, pp. 1797--1807.

\bibitem{Ladhak-EMNLP-2020-WikiLingua}
F.~Ladhak, E.~Durmus, C.~Cardie, and K.~Mckeown, ``Wikilingua: A new benchmark
  dataset for cross-lingual abstractive summarization,'' in \emph{Findings of
  the Association for Computational Linguistics: EMNLP 2020}, 2020, pp.
  4034--4048.

\bibitem{Moon-ACL-2019-OpenDialKG}
S.~Moon, P.~Shah, A.~Kumar, and R.~Subba, ``Opendialkg: Explainable
  conversational reasoning with attention-based walks over knowledge graphs,''
  in \emph{{ACL} {(1)}}.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2019, pp. 845--854.

\bibitem{Lai-arxiv-2022-DS}
Y.~Lai, C.~Li, Y.~Wang, T.~Zhang, R.~Zhong, L.~Zettlemoyer, S.~W. Yih,
  D.~Fried, S.~I. Wang, and T.~Yu, ``{DS-1000:} {A} natural and reliable
  benchmark for data science code generation,'' \emph{CoRR}, vol.
  abs/2211.11501, 2022.

\bibitem{Wang-arxiv-2022-Execution}
Z.~Wang, S.~Zhou, D.~Fried, and G.~Neubig, ``Execution-based evaluation for
  open-domain code generation,'' \emph{CoRR}, vol. abs/2212.10481, 2022.

\bibitem{Kwiatkowski-ACL-2019-Natural}
T.~Kwiatkowski, J.~Palomaki, O.~Redfield, M.~Collins, A.~P. Parikh, C.~Alberti,
  D.~Epstein, I.~Polosukhin, J.~Devlin, K.~Lee, K.~Toutanova, L.~Jones,
  M.~Kelcey, M.~Chang, A.~M. Dai, J.~Uszkoreit, Q.~Le, and S.~Petrov, ``Natural
  questions: a benchmark for question answering research,'' \emph{Trans. Assoc.
  Comput. Linguistics}, pp. 452--466, 2019.

\bibitem{Clark-arxiv-2018-Think}
P.~Clark, I.~Cowhey, O.~Etzioni, T.~Khot, A.~Sabharwal, C.~Schoenick, and
  O.~Tafjord, ``Think you have solved question answering? try arc, the {AI2}
  reasoning challenge,'' \emph{CoRR}, vol. abs/1803.05457, 2018.

\bibitem{Lin-ACL-2022-TruthfulQA}
S.~Lin, J.~Hilton, and O.~Evans, ``Truthfulqa: Measuring how models mimic human
  falsehoods,'' in \emph{Proceedings of the 60th Annual Meeting of the
  Association for Computational Linguistics (Volume 1: Long Papers), {ACL}
  2022, Dublin, Ireland, May 22-27, 2022}, 2022, pp. 3214--3252.

\bibitem{Berant-EMNLP-2013-Semantic}
J.~Berant, A.~Chou, R.~Frostig, and P.~Liang, ``Semantic parsing on freebase
  from question-answer pairs,'' in \emph{Proceedings of the 2013 Conference on
  Empirical Methods in Natural Language Processing, {EMNLP} 2013, 18-21 October
  2013, Grand Hyatt Seattle, Seattle, Washington, USA, {A} meeting of SIGDAT, a
  Special Interest Group of the {ACL}}, 2013, pp. 1533--1544.

\bibitem{Joshi-ACL-2017-TriviaQA}
M.~Joshi, E.~Choi, D.~S. Weld, and L.~Zettlemoyer, ``Triviaqa: {A} large scale
  distantly supervised challenge dataset for reading comprehension,'' in
  \emph{Proceedings of the 55th Annual Meeting of the Association for
  Computational Linguistics, {ACL} 2017, Vancouver, Canada, July 30 - August 4,
  Volume 1: Long Papers}, 2017, pp. 1601--1611.

\bibitem{Bisk-AAAI-2020-PIQA}
Y.~Bisk, R.~Zellers, R.~L. Bras, J.~Gao, and Y.~Choi, ``{PIQA:} reasoning about
  physical commonsense in natural language,'' in \emph{The Thirty-Fourth {AAAI}
  Conference on Artificial Intelligence, {AAAI} 2020, The Thirty-Second
  Innovative Applications of Artificial Intelligence Conference, {IAAI} 2020,
  The Tenth {AAAI} Symposium on Educational Advances in Artificial
  Intelligence, {EAAI} 2020, New York, NY, USA, February 7-12, 2020}, 2020, pp.
  7432--7439.

\bibitem{Dubey-ISWC-2019-LC}
M.~Dubey, D.~Banerjee, A.~Abdelkawi, and J.~Lehmann, ``Lc-quad 2.0: {A} large
  dataset for complex question answering over wikidata and dbpedia,'' in
  \emph{The Semantic Web - {ISWC} 2019 - 18th International Semantic Web
  Conference, Auckland, New Zealand, October 26-30, 2019, Proceedings, Part
  {II}}, 2019, pp. 69--78.

\bibitem{Gu-WWW-2021-Beyond}
Y.~Gu, S.~Kase, M.~Vanni, B.~M. Sadler, P.~Liang, X.~Yan, and Y.~Su, ``Beyond
  {I.I.D.:} three levels of generalization for question answering on knowledge
  bases,'' in \emph{{WWW} '21: The Web Conference 2021, Virtual Event /
  Ljubljana, Slovenia, April 19-23, 2021}, 2021, pp. 3477--3488.

\bibitem{Cao-ACL-2022-KQA}
S.~Cao, J.~Shi, L.~Pan, L.~Nie, Y.~Xiang, L.~Hou, J.~Li, B.~He, and H.~Zhang,
  ``{KQA} pro: {A} dataset with explicit compositional programs for complex
  question answering over knowledge base,'' in \emph{Proceedings of the 60th
  Annual Meeting of the Association for Computational Linguistics (Volume 1:
  Long Papers), {ACL} 2022, Dublin, Ireland, May 22-27, 2022}, 2022, pp.
  6101--6119.

\bibitem{Hu-COLING-2022-Logical}
X.~Hu, X.~Wu, Y.~Shu, and Y.~Qu, ``Logical form generation via multi-task
  learning for complex question answering over knowledge bases,'' in
  \emph{Proceedings of the 29th International Conference on Computational
  Linguistics, {COLING} 2022, Gyeongju, Republic of Korea, October 12-17,
  2022}, 2022, pp. 1687--1696.

\bibitem{Longpre-TACL-2021-MKQA}
S.~Longpre, Y.~Lu, and J.~Daiber, ``{MKQA:} {A} linguistically diverse
  benchmark for multilingual open domain question answering,'' \emph{Trans.
  Assoc. Comput. Linguistics}, vol.~9, pp. 1389--1406, 2021.

\bibitem{Saikh-IJDL-2022-ScienceQA}
T.~Saikh, T.~Ghosal, A.~Mittal, A.~Ekbal, and P.~Bhattacharyya, ``Scienceqa: a
  novel resource for question answering on scholarly articles,'' \emph{Int. J.
  Digit. Libr.}, vol.~23, no.~3, pp. 289--301, 2022.

\bibitem{Mihaylov-EMNLP-2018-Can}
T.~Mihaylov, P.~Clark, T.~Khot, and A.~Sabharwal, ``Can a suit of armor conduct
  electricity? {A} new dataset for open book question answering,'' in
  \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural
  Language Processing, Brussels, Belgium, October 31 - November 4, 2018}, 2018,
  pp. 2381--2391.

\bibitem{Nguyen-NIPS-2016-MS}
T.~Nguyen, M.~Rosenberg, X.~Song, J.~Gao, S.~Tiwary, R.~Majumder, and L.~Deng,
  ``{MS} {MARCO:} {A} human generated machine reading comprehension dataset,''
  in \emph{Proceedings of the Workshop on Cognitive Computation: Integrating
  neural and symbolic approaches 2016 co-located with the 30th Annual
  Conference on Neural Information Processing Systems {(NIPS} 2016), Barcelona,
  Spain, December 9, 2016}, 2016.

\bibitem{Khot-AAAI-2020-QASC}
T.~Khot, P.~Clark, M.~Guerquin, P.~Jansen, and A.~Sabharwal, ``{QASC:} {A}
  dataset for question answering via sentence composition,'' in \emph{The
  Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI} 2020, The
  Thirty-Second Innovative Applications of Artificial Intelligence Conference,
  {IAAI} 2020, The Tenth {AAAI} Symposium on Educational Advances in Artificial
  Intelligence, {EAAI} 2020, New York, NY, USA, February 7-12, 2020}, 2020, pp.
  8082--8090.

\bibitem{Rajpurkar-EMNLP-2016-SQuAD}
P.~Rajpurkar, J.~Zhang, K.~Lopyrev, and P.~Liang, ``Squad: 100, 000+ questions
  for machine comprehension of text,'' in \emph{Proceedings of the 2016
  Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2016,
  Austin, Texas, USA, November 1-4, 2016}, 2016, pp. 2383--2392.

\bibitem{Miller-EMNLP-2016-Key}
A.~H. Miller, A.~Fisch, J.~Dodge, A.~Karimi, A.~Bordes, and J.~Weston,
  ``Key-value memory networks for directly reading documents,'' in
  \emph{Proceedings of the 2016 Conference on Empirical Methods in Natural
  Language Processing, {EMNLP} 2016, Austin, Texas, USA, November 1-4, 2016},
  2016, pp. 1400--1409.

\bibitem{Goodrich-KDD-2019-Assessing}
B.~Goodrich, V.~Rao, P.~J. Liu, and M.~Saleh, ``Assessing the factual accuracy
  of generated text,'' in \emph{Proceedings of the 25th {ACM} {SIGKDD}
  International Conference on Knowledge Discovery {\&} Data Mining, {KDD} 2019,
  Anchorage, AK, USA, August 4-8, 2019}, 2019, pp. 166--175.

\bibitem{Toutanova-CVSC-2015-Observed}
K.~Toutanova and D.~Chen, ``Observed versus latent features for knowledge base
  and text inference,'' in \emph{Proceedings of the 3rd Workshop on Continuous
  Vector Space Models and their Compositionality, {CVSC} 2015, Beijing, China,
  July 26-31, 2015}, 2015, pp. 57--66.

\bibitem{Bollacker-SIGMOD-2008-Freebase}
K.~D. Bollacker, C.~Evans, P.~K. Paritosh, T.~Sturge, and J.~Taylor,
  ``Freebase: a collaboratively created graph database for structuring human
  knowledge,'' in \emph{Proceedings of the {ACM} {SIGMOD} International
  Conference on Management of Data, {SIGMOD} 2008, Vancouver, BC, Canada, June
  10-12, 2008}, 2008, pp. 1247--1250.

\bibitem{Dettmers-AAAI-2018-Convolutional}
T.~Dettmers, P.~Minervini, P.~Stenetorp, and S.~Riedel, ``Convolutional 2d
  knowledge graph embeddings,'' in \emph{Proceedings of the Thirty-Second
  {AAAI} Conference on Artificial Intelligence, (AAAI-18), the 30th innovative
  Applications of Artificial Intelligence (IAAI-18), and the 8th {AAAI}
  Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New
  Orleans, Louisiana, USA, February 2-7, 2018}, 2018, pp. 1811--1818.

\bibitem{Miller-Commun-1995-WordNet}
G.~A. Miller, ``Wordnet: {A} lexical database for english,'' \emph{Commun.
  {ACM}}, pp. 39--41, 1995.

\bibitem{Petroni-EMNLP-2019-Language}
F.~Petroni, T.~Rockt{\"{a}}schel, S.~Riedel, P.~S.~H. Lewis, A.~Bakhtin, Y.~Wu,
  and A.~H. Miller, ``Language models as knowledge bases?'' in
  \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural
  Language Processing and the 9th International Joint Conference on Natural
  Language Processing, {EMNLP-IJCNLP} 2019, Hong Kong, China, November 3-7,
  2019}, 2019, pp. 2463--2473.

\bibitem{Mahdisoltani-CIDR-2015-YAGO3}
F.~Mahdisoltani, J.~Biega, and F.~M. Suchanek, ``{YAGO3:} {A} knowledge base
  from multilingual wikipedias,'' in \emph{Seventh Biennial Conference on
  Innovative Data Systems Research, {CIDR} 2015, Asilomar, CA, USA, January
  4-7, 2015, Online Proceedings}, 2015.

\bibitem{Suchanek-WWW-2007-Yago}
F.~M. Suchanek, G.~Kasneci, and G.~Weikum, ``Yago: a core of semantic
  knowledge,'' in \emph{Proceedings of the 16th International Conference on
  World Wide Web, {WWW} 2007, Banff, Alberta, Canada, May 8-12, 2007}, 2007,
  pp. 697--706.

\bibitem{yang-2018-acl-HotpotQA}
Z.~Yang, P.~Qi, S.~Zhang, Y.~Bengio, W.~W. Cohen, R.~Salakhutdinov, and C.~D.
  Manning, ``Hotpotqa: {A} dataset for diverse, explainable multi-hop question
  answering,'' in \emph{Proceedings of the 2018 Conference on Empirical Methods
  in Natural Language Processing, Brussels, Belgium, October 31 - November 4,
  2018}.\hskip 1em plus 0.5em minus 0.4em\relax Association for Computational
  Linguistics, 2018, pp. 2369--2380.

\bibitem{Clark-naacl-2019-BoolQ}
C.~Clark, K.~Lee, M.~Chang, T.~Kwiatkowski, M.~Collins, and K.~Toutanova,
  ``Boolq: Exploring the surprising difficulty of natural yes/no questions,''
  in \emph{Proceedings of the 2019 Conference of the North American Chapter of
  the Association for Computational Linguistics: Human Language Technologies,
  {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and
  Short Papers)}, J.~Burstein, C.~Doran, and T.~Solorio, Eds.\hskip 1em plus
  0.5em minus 0.4em\relax Association for Computational Linguistics, 2019, pp.
  2924--2936.

\bibitem{Sap-arxiv-2019-SocialIQA}
M.~Sap, H.~Rashkin, D.~Chen, R.~L. Bras, and Y.~Choi, ``Socialiqa: Commonsense
  reasoning about social interactions,'' \emph{CoRR}, vol. abs/1904.09728,
  2019.

\bibitem{Zellers-acl-2019-HellaSwag}
R.~Zellers, A.~Holtzman, Y.~Bisk, A.~Farhadi, and Y.~Choi, ``Hellaswag: Can a
  machine really finish your sentence?'' in \emph{Proceedings of the 57th
  Conference of the Association for Computational Linguistics, {ACL} 2019,
  Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers},
  A.~Korhonen, D.~R. Traum, and L.~M{\`{a}}rquez, Eds.\hskip 1em plus 0.5em
  minus 0.4em\relax Association for Computational Linguistics, 2019, pp.
  4791--4800.

\bibitem{Sakaguchi-aaai-2020-WinoGrande}
K.~Sakaguchi, R.~L. Bras, C.~Bhagavatula, and Y.~Choi, ``Winogrande: An
  adversarial winograd schema challenge at scale,'' in \emph{{AAAI}}.\hskip 1em
  plus 0.5em minus 0.4em\relax {AAAI} Press, 2020, pp. 8732--8740.

\bibitem{Roemmele-aaai-2011-Choice}
M.~Roemmele, C.~A. Bejan, and A.~S. Gordon, ``Choice of plausible alternatives:
  An evaluation of commonsense causal reasoning,'' in \emph{Logical
  Formalizations of Commonsense Reasoning, Papers from the 2011 {AAAI} Spring
  Symposium, Technical Report SS-11-06, Stanford, California, USA, March 21-23,
  2011}.\hskip 1em plus 0.5em minus 0.4em\relax {AAAI}, 2011.

\bibitem{Sakaguchi-acl-2021-proScript}
K.~Sakaguchi, C.~Bhagavatula, R.~L. Bras, N.~Tandon, P.~Clark, and Y.~Choi,
  ``proscript: Partially ordered scripts generation,'' in \emph{Findings of the
  Association for Computational Linguistics: {EMNLP} 2021, Virtual Event /
  Punta Cana, Dominican Republic, 16-20 November, 2021}, M.~Moens, X.~Huang,
  L.~Specia, and S.~W. Yih, Eds.\hskip 1em plus 0.5em minus 0.4em\relax
  Association for Computational Linguistics, 2021, pp. 2138--2149.

\bibitem{Dalvi-acl-2018-Tracking}
B.~Dalvi, L.~Huang, N.~Tandon, W.~Yih, and P.~Clark, ``Tracking state changes
  in procedural text: a challenge dataset and models for process paragraph
  comprehension,'' in \emph{Proceedings of the 2018 Conference of the North
  American Chapter of the Association for Computational Linguistics: Human
  Language Technologies, {NAACL-HLT} 2018, New Orleans, Louisiana, USA, June
  1-6, 2018, Volume 1 (Long Papers)}, M.~A. Walker, H.~Ji, and A.~Stent,
  Eds.\hskip 1em plus 0.5em minus 0.4em\relax Association for Computational
  Linguistics, 2018, pp. 1595--1604.

\bibitem{Saha-acl-2021-ExplaGraphs}
S.~Saha, P.~Yadav, L.~Bauer, and M.~Bansal, ``Explagraphs: An explanation graph
  generation task for structured commonsense reasoning,'' in \emph{Proceedings
  of the 2021 Conference on Empirical Methods in Natural Language Processing,
  {EMNLP} 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November,
  2021}, M.~Moens, X.~Huang, L.~Specia, and S.~W. Yih, Eds.\hskip 1em plus
  0.5em minus 0.4em\relax Association for Computational Linguistics, 2021, pp.
  7716--7740.

\bibitem{Tafjord-acl-2021-ProofWriter}
O.~Tafjord, B.~Dalvi, and P.~Clark, ``Proofwriter: Generating implications,
  proofs, and abductive statements over natural language,'' in \emph{Findings
  of the Association for Computational Linguistics: {ACL/IJCNLP} 2021, Online
  Event, August 1-6, 2021}, ser. Findings of {ACL}, C.~Zong, F.~Xia, W.~Li, and
  R.~Navigli, Eds., vol. {ACL/IJCNLP} 2021.\hskip 1em plus 0.5em minus
  0.4em\relax Association for Computational Linguistics, 2021, pp. 3621--3634.

\bibitem{Dalvi-acl-2021-Explaining}
B.~Dalvi, P.~Jansen, O.~Tafjord, Z.~Xie, H.~Smith, L.~Pipatanangkura, and
  P.~Clark, ``Explaining answers with entailment trees,'' in \emph{Proceedings
  of the 2021 Conference on Empirical Methods in Natural Language Processing,
  {EMNLP} 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November,
  2021}, M.~Moens, X.~Huang, L.~Specia, and S.~W. Yih, Eds.\hskip 1em plus
  0.5em minus 0.4em\relax Association for Computational Linguistics, 2021, pp.
  7358--7370.

\bibitem{Saparov-arxiv-2022-Language}
A.~Saparov and H.~He, ``Language models are greedy reasoners: {A} systematic
  formal analysis of chain-of-thought,'' \emph{CoRR}, vol. abs/2210.01240,
  2022.

\bibitem{Anil-arxiv-2022-Exploring}
C.~Anil, Y.~Wu, A.~Andreassen, A.~Lewkowycz, V.~Misra, V.~V. Ramasesh,
  A.~Slone, G.~Gur{-}Ari, E.~Dyer, and B.~Neyshabur, ``Exploring length
  generalization in large language models,'' \emph{CoRR}, vol. abs/2207.04901,
  2022.

\bibitem{Patel-NAACL-2021-Are}
A.~Patel, S.~Bhattamishra, and N.~Goyal, ``Are {NLP} models really able to
  solve simple math word problems?'' in \emph{{NAACL-HLT}}.\hskip 1em plus
  0.5em minus 0.4em\relax Association for Computational Linguistics, 2021, pp.
  2080--2094.

\bibitem{Roy-acl-2015-Solving}
S.~Roy and D.~Roth, ``Solving general arithmetic word problems,'' in
  \emph{Proceedings of the 2015 Conference on Empirical Methods in Natural
  Language Processing, {EMNLP} 2015, Lisbon, Portugal, September 17-21, 2015},
  L.~M{\`{a}}rquez, C.~Callison{-}Burch, J.~Su, D.~Pighin, and Y.~Marton,
  Eds.\hskip 1em plus 0.5em minus 0.4em\relax The Association for Computational
  Linguistics, 2015, pp. 1743--1752.

\bibitem{Amini-acl-2019-MathQA}
A.~Amini, S.~Gabriel, S.~Lin, R.~Koncel{-}Kedziorski, Y.~Choi, and
  H.~Hajishirzi, ``Mathqa: Towards interpretable math word problem solving with
  operation-based formalisms,'' in \emph{Proceedings of the 2019 Conference of
  the North American Chapter of the Association for Computational Linguistics:
  Human Language Technologies, {NAACL-HLT} 2019, Minneapolis, MN, USA, June
  2-7, 2019, Volume 1 (Long and Short Papers)}, J.~Burstein, C.~Doran, and
  T.~Solorio, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2019, pp. 2357--2367.

\bibitem{Ling-acl-2017-Program}
W.~Ling, D.~Yogatama, C.~Dyer, and P.~Blunsom, ``Program induction by rationale
  generation: Learning to solve and explain algebraic word problems,'' in
  \emph{Proceedings of the 55th Annual Meeting of the Association for
  Computational Linguistics, {ACL} 2017, Vancouver, Canada, July 30 - August 4,
  Volume 1: Long Papers}, R.~Barzilay and M.~Kan, Eds.\hskip 1em plus 0.5em
  minus 0.4em\relax Association for Computational Linguistics, 2017, pp.
  158--167.

\bibitem{Koncel-NAACL-2016-MAWPS}
R.~Koncel-Kedziorski, S.~Roy, A.~Amini, N.~Kushman, and H.~Hajishirzi, ``Mawps:
  A math word problem repository,'' in \emph{Proceedings of the 2016 conference
  of the north american chapter of the association for computational
  linguistics: human language technologies}, 2016, pp. 1152--1157.

\bibitem{Dua-NAACL-2019-DROP}
D.~Dua, Y.~Wang, P.~Dasigi, G.~Stanovsky, S.~Singh, and M.~Gardner, ``{DROP:}
  {A} reading comprehension benchmark requiring discrete reasoning over
  paragraphs,'' in \emph{Proceedings of the 2019 Conference of the North
  American Chapter of the Association for Computational Linguistics: Human
  Language Technologies, {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7,
  2019, Volume 1 (Long and Short Papers)}, 2019, pp. 2368--2378.

\bibitem{Welleck-NIPS-2021-NaturalProofs}
S.~Welleck, J.~Liu, R.~L. Bras, H.~Hajishirzi, Y.~Choi, and K.~Cho,
  ``Naturalproofs: Mathematical theorem proving in natural language,'' in
  \emph{Proceedings of the Neural Information Processing Systems Track on
  Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December
  2021, virtual}, J.~Vanschoren and S.~Yeung, Eds., 2021.

\bibitem{Jiang-AITP-2021-LISA}
A.~Q. Jiang, W.~Li, J.~M. Han, and Y.~Wu, ``Lisa: Language models of isabelle
  proofs,'' in \emph{6th Conference on Artificial Intelligence and Theorem
  Proving}, 2021, pp. 378--392.

\bibitem{Zheng-ICLR-2022-miniF2F}
K.~Zheng, J.~M. Han, and S.~Polu, ``minif2f: a cross-system benchmark for
  formal olympiad-level mathematics,'' in \emph{The Tenth International
  Conference on Learning Representations, {ICLR} 2022, Virtual Event, April
  25-29, 2022}.\hskip 1em plus 0.5em minus 0.4em\relax OpenReview.net, 2022.

\bibitem{Azerbayev-arxiv-2023-ProofNet}
Z.~Azerbayev, B.~Piotrowski, H.~Schoelkopf, E.~W. Ayers, D.~Radev, and
  J.~Avigad, ``Proofnet: Autoformalizing and formally proving
  undergraduate-level mathematics,'' \emph{CoRR}, vol. abs/2302.12433, 2023.

\bibitem{Li-arxiv-2023-HaluEval}
J.~Li, X.~Cheng, W.~X. Zhao, J.~Nie, and J.~Wen, ``Halueval: {A} large-scale
  hallucination evaluation benchmark for large language models,'' \emph{CoRR},
  vol. abs/2305.11747, 2023.

\bibitem{Nangia-EMNLP-2020-CrowS}
N.~Nangia, C.~Vania, R.~Bhalerao, and S.~R. Bowman, ``Crows-pairs: {A}
  challenge dataset for measuring social biases in masked language models,'' in
  \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural
  Language Processing, {EMNLP} 2020, Online, November 16-20, 2020}, 2020, pp.
  1953--1967.

\bibitem{Rudinger-NAACL-2018-Gender}
R.~Rudinger, J.~Naradowsky, B.~Leonard, and B.~V. Durme, ``Gender bias in
  coreference resolution,'' in \emph{Proceedings of the 2018 Conference of the
  North American Chapter of the Association for Computational Linguistics:
  Human Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June
  1-6, 2018, Volume 2 (Short Papers)}, 2018, pp. 8--14.

\bibitem{Gehman-2023-arxiv-RealToxicityPrompts}
S.~Gehman, S.~Gururangan, M.~Sap, Y.~Choi, and N.~A. Smith,
  ``Realtoxicityprompts: Evaluating neural toxic degeneration in language
  models,'' in \emph{Findings of the Association for Computational Linguistics:
  {EMNLP} 2020, Online Event, 16-20 November 2020}, ser. Findings of {ACL},
  T.~Cohn, Y.~He, and Y.~Liu, Eds., vol. {EMNLP} 2020.\hskip 1em plus 0.5em
  minus 0.4em\relax Association for Computational Linguistics, 2020, pp.
  3356--3369.

\bibitem{Puig-CVPR-2018-VirtualHome}
X.~Puig, K.~Ra, M.~Boben, J.~Li, T.~Wang, S.~Fidler, and A.~Torralba,
  ``Virtualhome: Simulating household activities via programs,'' in
  \emph{{CVPR}}.\hskip 1em plus 0.5em minus 0.4em\relax Computer Vision
  Foundation / {IEEE} Computer Society, 2018, pp. 8494--8502.

\bibitem{Srivastava-CoRL-2021-BEHAVIOR}
S.~Srivastava, C.~Li, M.~Lingelbach, R.~Mart{\'{\i}}n{-}Mart{\'{\i}}n, F.~Xia,
  K.~E. Vainio, Z.~Lian, C.~Gokmen, S.~Buch, C.~K. Liu, S.~Savarese, H.~Gweon,
  J.~Wu, and L.~Fei{-}Fei, ``{BEHAVIOR:} benchmark for everyday household
  activities in virtual, interactive, and ecological environments,'' in
  \emph{CoRL}, ser. Proceedings of Machine Learning Research, vol. 164.\hskip
  1em plus 0.5em minus 0.4em\relax {PMLR}, 2021, pp. 477--490.

\bibitem{Shridhar-CVPR-2020-ALFRED}
M.~Shridhar, J.~Thomason, D.~Gordon, Y.~Bisk, W.~Han, R.~Mottaghi,
  L.~Zettlemoyer, and D.~Fox, ``{ALFRED:} {A} benchmark for interpreting
  grounded instructions for everyday tasks,'' in \emph{{CVPR}}.\hskip 1em plus
  0.5em minus 0.4em\relax Computer Vision Foundation / {IEEE}, 2020, pp.
  10\,737--10\,746.

\bibitem{Shridhar-2021-iclr-ALFWorld}
M.~Shridhar, X.~Yuan, M.~C{\^{o}}t{\'{e}}, Y.~Bisk, A.~Trischler, and M.~J.
  Hausknecht, ``Alfworld: Aligning text and embodied environments for
  interactive learning,'' in \emph{9th International Conference on Learning
  Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}.\hskip
  1em plus 0.5em minus 0.4em\relax OpenReview.net, 2021.

\bibitem{Yao-2022-nips-WebShop}
S.~Yao, H.~Chen, J.~Yang, and K.~Narasimhan, ``Webshop: Towards scalable
  real-world web interaction with grounded language agents,'' in
  \emph{NeurIPS}, 2022.

\bibitem{Deng-2023-arxiv-Mind2Web}
X.~Deng, Y.~Gu, B.~Zheng, S.~Chen, S.~Stevens, B.~Wang, H.~Sun, and Y.~Su,
  ``Mind2web: Towards a generalist agent for the web,'' \emph{CoRR}, vol.
  abs/2306.06070, 2023.

\bibitem{Guss-2019-ijcai-MineRL}
W.~H. Guss, B.~Houghton, N.~Topin, P.~Wang, C.~Codel, M.~Veloso, and
  R.~Salakhutdinov, ``Minerl: {A} large-scale dataset of minecraft
  demonstrations,'' in \emph{Proceedings of the Twenty-Eighth International
  Joint Conference on Artificial Intelligence, {IJCAI} 2019, Macao, China,
  August 10-16, 2019}, S.~Kraus, Ed.\hskip 1em plus 0.5em minus 0.4em\relax
  ijcai.org, 2019, pp. 2442--2448.

\bibitem{Fan-2022-nips-minedojo}
L.~Fan, G.~Wang, Y.~Jiang, A.~Mandlekar, Y.~Yang, H.~Zhu, A.~Tang, D.~Huang,
  Y.~Zhu, and A.~Anandkumar, ``Minedojo: Building open-ended embodied agents
  with internet-scale knowledge,'' in \emph{NeurIPS}, 2022.

\bibitem{Lu-2022-arxiv-Dynamic}
P.~Lu, L.~Qiu, K.~Chang, Y.~N. Wu, S.~Zhu, T.~Rajpurohit, P.~Clark, and
  A.~Kalyan, ``Dynamic prompt learning via policy gradient for semi-structured
  mathematical reasoning,'' \emph{CoRR}, vol. abs/2209.14610, 2022.

\bibitem{Zhang-2023-arxiv-Evaluating}
B.~Zhang, K.~Zhou, X.~Wei, W.~X. Zhao, J.~Sha, S.~Wang, and J.~rong Wen,
  ``Evaluating and improving tool-augmented computation-intensive math
  reasoning,'' \emph{CoRR}, vol. abs/2306.02408, 2023.

\bibitem{yang-2023-arxiv-GPT4Tools}
R.~Yang, L.~Song, Y.~Li, S.~Zhao, Y.~Ge, X.~Li, and Y.~Shan, ``Gpt4tools:
  Teaching large language model to use tools via self-instruction,''
  \emph{CoRR}, vol. abs/2305.18752, 2023.

\bibitem{Patil-2023-arxiv-Gorilla}
S.~G. Patil, T.~Zhang, X.~Wang, and J.~E. Gonzalez, ``Gorilla: Large language
  model connected with massive apis,'' \emph{CoRR}, vol. abs/2305.15334, 2023.

\bibitem{Yih-2016-acl-The}
W.~Yih, M.~Richardson, C.~Meek, M.~Chang, and J.~Suh, ``The value of semantic
  parse labeling for knowledge base question answering,'' in \emph{Proceedings
  of the 54th Annual Meeting of the Association for Computational Linguistics,
  {ACL} 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short
  Papers}.\hskip 1em plus 0.5em minus 0.4em\relax The Association for Computer
  Linguistics, 2016.

\bibitem{Puerto-2023-eacl-MetaQA}
H.~Puerto, G.~G. Sahin, and I.~Gurevych, ``Metaqa: Combining expert agents for
  multi-skill question answering,'' in \emph{Proceedings of the 17th Conference
  of the European Chapter of the Association for Computational Linguistics,
  {EACL} 2023, Dubrovnik, Croatia, May 2-6, 2023}, A.~Vlachos and
  I.~Augenstein, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2023, pp. 3548--3562.

\bibitem{Pasupat-2015-acl-Compositional}
P.~Pasupat and P.~Liang, ``Compositional semantic parsing on semi-structured
  tables,'' in \emph{Proceedings of the 53rd Annual Meeting of the Association
  for Computational Linguistics and the 7th International Joint Conference on
  Natural Language Processing of the Asian Federation of Natural Language
  Processing, {ACL} 2015, July 26-31, 2015, Beijing, China, Volume 1: Long
  Papers}.\hskip 1em plus 0.5em minus 0.4em\relax The Association for Computer
  Linguistics, 2015, pp. 1470--1480.

\bibitem{Zhang-2017-arxiv-Seq2SQL}
V.~Zhong, C.~Xiong, and R.~Socher, ``Seq2sql: Generating structured queries
  from natural language using reinforcement learning,'' \emph{CoRR}, vol.
  abs/1709.00103, 2017.

\bibitem{Chen-2020-iclr-TabFact}
W.~Chen, H.~Wang, J.~Chen, Y.~Zhang, H.~Wang, S.~Li, X.~Zhou, and W.~Y. Wang,
  ``Tabfact: {A} large-scale dataset for table-based fact verification,'' in
  \emph{8th International Conference on Learning Representations, {ICLR} 2020,
  Addis Ababa, Ethiopia, April 26-30, 2020}.\hskip 1em plus 0.5em minus
  0.4em\relax OpenReview.net, 2020.

\bibitem{Yu-2018-emnlp-Spider}
T.~Yu, R.~Zhang, K.~Yang, M.~Yasunaga, D.~Wang, Z.~Li, J.~Ma, I.~Li, Q.~Yao,
  S.~Roman, Z.~Zhang, and D.~R. Radev, ``Spider: {A} large-scale human-labeled
  dataset for complex and cross-domain semantic parsing and text-to-sql task,''
  in \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural
  Language Processing, Brussels, Belgium, October 31 - November 4, 2018},
  E.~Riloff, D.~Chiang, J.~Hockenmaier, and J.~Tsujii, Eds.\hskip 1em plus
  0.5em minus 0.4em\relax Association for Computational Linguistics, 2018, pp.
  3911--3921.

\bibitem{Bahdanau-ICLR-2015-Neural}
D.~Bahdanau, K.~Cho, and Y.~Bengio, ``Neural machine translation by jointly
  learning to align and translate,'' in \emph{{ICLR}}, 2015.

\bibitem{Papineni-acl-2002-bleu}
K.~Papineni, S.~Roukos, T.~Ward, and W.~Zhu, ``Bleu: a method for automatic
  evaluation of machine translation,'' in \emph{Proceedings of the 40th Annual
  Meeting of the Association for Computational Linguistics, July 6-12, 2002,
  Philadelphia, PA, {USA}}.\hskip 1em plus 0.5em minus 0.4em\relax {ACL}, 2002,
  pp. 311--318.

\bibitem{lin-acl-2004-rouge}
C.-Y. Lin, ``{ROUGE}: A package for automatic evaluation of summaries,'' in
  \emph{Text Summarization Branches Out}.\hskip 1em plus 0.5em minus
  0.4em\relax Association for Computational Linguistics, Jul. 2004, pp. 74--81.

\bibitem{Jiao-arxiv-2023-mt}
W.~Jiao, W.~Wang, J.-t. Huang, X.~Wang, and Z.~Tu, ``Is chatgpt a good
  translator? a preliminary study,'' \emph{arXiv preprint arXiv:2301.08745},
  2023.

\bibitem{Zhang-2023-arxiv-Benchmarking}
T.~Zhang, F.~Ladhak, E.~Durmus, P.~Liang, K.~R. McKeown, and T.~B. Hashimoto,
  ``Benchmarking large language models for news summarization,'' \emph{CoRR},
  vol. abs/2301.13848, 2023.

\bibitem{Goyal-2023-arxiv-News}
T.~Goyal, J.~J. Li, and G.~Durrett, ``News summarization and evaluation in the
  era of {GPT-3},'' \emph{CoRR}, vol. abs/2209.12356, 2022.

\bibitem{Gehrmann-2022-arxiv-Repairing}
S.~Gehrmann, E.~Clark, and T.~Sellam, ``Repairing the cracked foundation: {A}
  survey of obstacles in evaluation practices for generated text,''
  \emph{CoRR}, vol. abs/2202.06935, 2022.

\bibitem{Wang-2023-arxiv-Is}
J.~Wang, Y.~Liang, F.~Meng, H.~Shi, Z.~Li, J.~Xu, J.~Qu, and J.~Zhou, ``Is
  chatgpt a good {NLG} evaluator? {A} preliminary study,'' \emph{CoRR}, vol.
  abs/2303.04048, 2023.

\bibitem{Liu-2023-arxiv-G-Eval}
Y.~Liu, D.~Iter, Y.~Xu, S.~Wang, R.~Xu, and C.~Zhu, ``G-eval: {NLG} evaluation
  using {GPT-4} with better human alignment,'' \emph{CoRR}, vol.
  abs/2303.16634, 2023.

\bibitem{Yang-EMNLP-2022-Re3}
K.~Yang, Y.~Tian, N.~Peng, and D.~Klein, ``Re3: Generating longer stories with
  recursive reprompting and revision,'' in \emph{Proceedings of the 2022
  Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2022,
  Abu Dhabi, United Arab Emirates, December 7-11, 2022}, Y.~Goldberg,
  Z.~Kozareva, and Y.~Zhang, Eds.\hskip 1em plus 0.5em minus 0.4em\relax
  Association for Computational Linguistics, 2022, pp. 4393--4479.

\bibitem{Zhou-2023-arxiv-RecurrentGPT}
W.~Zhou, Y.~E. Jiang, P.~Cui, T.~Wang, Z.~Xiao, Y.~Hou, R.~Cotterell, and
  M.~Sachan, ``Recurrentgpt: Interactive generation of (arbitrarily) long
  text,'' \emph{CoRR}, vol. abs/2305.13304, 2023.

\bibitem{Gulwani-Found-2017-Program}
S.~Gulwani, O.~Polozov, and R.~Singh, ``Program synthesis,'' \emph{Found.
  Trends Program. Lang.}, vol.~4, no. 1-2, pp. 1--119, 2017.

\bibitem{Zhang-ICLR-2023-Planning}
S.~Zhang, Z.~Chen, Y.~Shen, M.~Ding, J.~B. Tenenbaum, and C.~Gan, ``Planning
  with large language models for code generation,'' 2023.

\bibitem{Welsh-ACM-2023-The}
M.~Welsh, ``The end of programming,'' \emph{Commun. {ACM}}, vol.~66, no.~1, pp.
  34--35, 2023.

\bibitem{Bang-arxiv-2023-A}
Y.~Bang, S.~Cahyawijaya, N.~Lee, W.~Dai, D.~Su, B.~Wilie, H.~Lovenia, Z.~Ji,
  T.~Yu, W.~Chung, Q.~V. Do, Y.~Xu, and P.~Fung, ``A multitask, multilingual,
  multimodal evaluation of chatgpt on reasoning, hallucination, and
  interactivity,'' \emph{CoRR}, vol. abs/2302.04023, 2023.

\bibitem{Liu-arxiv-2022-Revisiting}
Y.~Liu, A.~R. Fabbri, P.~Liu, Y.~Zhao, L.~Nan, R.~Han, S.~Han, S.~R. Joty,
  C.~Wu, C.~Xiong, and D.~Radev, ``Revisiting the gold standard: Grounding
  summarization evaluation with robust human evaluation,'' \emph{CoRR}, vol.
  abs/2212.07981, 2022.

\bibitem{Fabri-2021-tacl-SummEval}
A.~R. Fabbri, W.~Kryscinski, B.~McCann, C.~Xiong, R.~Socher, and D.~R. Radev,
  ``Summeval: Re-evaluating summarization evaluation,'' \emph{Trans. Assoc.
  Comput. Linguistics}, vol.~9, pp. 391--409, 2021.

\bibitem{Tang-2023-arxiv-Not}
T.~Tang, H.~Lu, Y.~E. Jiang, H.~Huang, D.~Zhang, W.~X. Zhao, and F.~Wei, ``Not
  all metrics are guilty: Improving {NLG} evaluation with {LLM} paraphrasing,''
  \emph{CoRR}, vol. abs/2305.15067, 2023.

\bibitem{Wang-2023-arxiv-rethinking}
X.~Wang, X.~Tang, W.~X. Zhao, J.~Wang, and J.~Wen, ``Rethinking the evaluation
  for conversational recommendation in the era of large language models,''
  \emph{CoRR}, vol. abs/2305.13112, 2023.

\bibitem{Gao-arxiv-2023-Human}
M.~Gao, J.~Ruan, R.~Sun, X.~Yin, S.~Yang, and X.~Wan, ``Human-like
  summarization evaluation with chatgpt,'' \emph{CoRR}, vol. abs/2304.02554,
  2023.

\bibitem{Ji-2023-arxiv-Exploring}
Y.~Ji, Y.~Gong, Y.~Peng, C.~Ni, P.~Sun, D.~Pan, B.~Ma, and X.~Li, ``Exploring
  chatgpt's ability to rank content: {A} preliminary study on consistency with
  human preferences,'' \emph{CoRR}, vol. abs/2303.07610, 2023.

\bibitem{Bai-2023-arxiv-Benchmarking}
Y.~Bai, J.~Ying, Y.~Cao, X.~Lv, Y.~He, X.~Wang, J.~Yu, K.~Zeng, Y.~Xiao,
  H.~Lyu, J.~Zhang, J.~Li, and L.~Hou, ``Benchmarking foundation models with
  language-model-as-an-examiner,'' \emph{CoRR}, vol. abs/2306.04181, 2023.

\bibitem{Liu-2023-arxiv-Evaluate}
Y.~Liu, S.~Feng, D.~Wang, Y.~Zhang, and H.~Sch{\"{u}}tze, ``Evaluate what you
  can't evaluate: Unassessable generated responses quality,'' \emph{CoRR}, vol.
  abs/2305.14658, 2023.

\bibitem{Wang-2023-arxiv-Large}
P.~Wang, L.~Li, L.~Chen, D.~Zhu, B.~Lin, Y.~Cao, Q.~Liu, T.~Liu, and Z.~Sui,
  ``Large language models are not fair evaluators,'' \emph{CoRR}, vol.
  abs/2305.17926, 2023.

\bibitem{Ye-arxiv-2023-A}
J.~Ye, X.~Chen, N.~Xu, C.~Zu, Z.~Shao, S.~Liu, Y.~Cui, Z.~Zhou, C.~Gong,
  Y.~Shen, J.~Zhou, S.~Chen, T.~Gui, Q.~Zhang, and X.~Huang, ``A comprehensive
  capability analysis of gpt-3 and gpt-3.5 series models,'' \emph{arXiv
  preprint arXiv:2303.10420}, 2023.

\bibitem{Michael-Psychology-1989-Catastrophic}
M.~McCloskey and N.~J. Cohen, ``Catastrophic interference in connectionist
  networks: The sequential learning problem,'' in \emph{Psychology of learning
  and motivation}, 1989, pp. 109--165.

\bibitem{Kemker-AAAI-2018-Measuring}
R.~Kemker, M.~McClure, A.~Abitino, T.~L. Hayes, and C.~Kanan, ``Measuring
  catastrophic forgetting in neural networks,'' in \emph{Proceedings of the
  Thirty-Second {AAAI} Conference on Artificial Intelligence, (AAAI-18), the
  30th innovative Applications of Artificial Intelligence (IAAI-18), and the
  8th {AAAI} Symposium on Educational Advances in Artificial Intelligence
  (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018}, 2018, pp.
  3390--3398.

\bibitem{Xie-EMNLP-2022-UnifiedSKG}
T.~Xie, C.~H. Wu, P.~Shi, R.~Zhong, T.~Scholak, M.~Yasunaga, C.~Wu, M.~Zhong,
  P.~Yin, S.~I. Wang, V.~Zhong, B.~Wang, C.~Li, C.~Boyle, A.~Ni, Z.~Yao,
  D.~Radev, C.~Xiong, L.~Kong, R.~Zhang, N.~A. Smith, L.~Zettlemoyer, and
  T.~Yu, ``Unifiedskg: Unifying and multi-tasking structured knowledge
  grounding with text-to-text language models,'' in \emph{{EMNLP}}.\hskip 1em
  plus 0.5em minus 0.4em\relax Association for Computational Linguistics, 2022,
  pp. 602--631.

\bibitem{Roberts-EMNLP-2020-How}
A.~Roberts, C.~Raffel, and N.~Shazeer, ``How much knowledge can you pack into
  the parameters of a language model?'' in \emph{Proceedings of the 2020
  Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2020,
  Online, November 16-20, 2020}, 2020, pp. 5418--5426.

\bibitem{Izacard-arxiv-2022-Few}
G.~Izacard, P.~S.~H. Lewis, M.~Lomeli, L.~Hosseini, F.~Petroni, T.~Schick,
  J.~Dwivedi{-}Yu, A.~Joulin, S.~Riedel, and E.~Grave, ``Few-shot learning with
  retrieval augmented language models,'' \emph{CoRR}, vol. abs/2208.03299,
  2022.

\bibitem{Guu-ICML-2020-Retrieval}
K.~Guu, K.~Lee, Z.~Tung, P.~Pasupat, and M.~Chang, ``Retrieval augmented
  language model pre-training,'' in \emph{Proceedings of the 37th International
  Conference on Machine Learning, {ICML} 2020, 13-18 July 2020, Virtual Event},
  2020, pp. 3929--3938.

\bibitem{Lewis-NeurIPS-2020-Retrieval}
P.~S.~H. Lewis, E.~Perez, A.~Piktus, F.~Petroni, V.~Karpukhin, N.~Goyal,
  H.~K{\"{u}}ttler, M.~Lewis, W.~Yih, T.~Rockt{\"{a}}schel, S.~Riedel, and
  D.~Kiela, ``Retrieval-augmented generation for knowledge-intensive {NLP}
  tasks,'' in \emph{Advances in Neural Information Processing Systems 33:
  Annual Conference on Neural Information Processing Systems 2020, NeurIPS
  2020, December 6-12, 2020, virtual}, 2020.

\bibitem{Lan-2021-arxiv-Complex}
Y.~Lan, G.~He, J.~Jiang, J.~Jiang, W.~X. Zhao, and J.~Wen, ``Complex knowledge
  base question answering: {A} survey,'' \emph{CoRR}, vol. abs/2108.06688,
  2021.

\bibitem{Borgeaud-icml-2022-Improving}
S.~Borgeaud, A.~Mensch, J.~Hoffmann, T.~Cai, E.~Rutherford, K.~Millican,
  G.~van~den Driessche, J.~Lespiau, B.~Damoc, A.~Clark, D.~de~Las~Casas,
  A.~Guy, J.~Menick, R.~Ring, T.~Hennigan, S.~Huang, L.~Maggiore, C.~Jones,
  A.~Cassirer, A.~Brock, M.~Paganini, G.~Irving, O.~Vinyals, S.~Osindero,
  K.~Simonyan, J.~W. Rae, E.~Elsen, and L.~Sifre, ``Improving language models
  by retrieving from trillions of tokens,'' in \emph{International Conference
  on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland,
  {USA}}, ser. Proceedings of Machine Learning Research, K.~Chaudhuri,
  S.~Jegelka, L.~Song, C.~Szepesv{\'{a}}ri, G.~Niu, and S.~Sabato, Eds., vol.
  162.\hskip 1em plus 0.5em minus 0.4em\relax {PMLR}, 2022, pp. 2206--2240.

\bibitem{Xu-arxiv-2023-Search}
S.~Xu, L.~Pang, H.~Shen, X.~Cheng, and T.-S. Chua, ``Search-in-the-chain:
  Towards accurate, credible and traceable large language models for
  knowledge-intensive tasks,'' \emph{CoRR}, vol. abs/2304.14732, 2023.

\bibitem{Peng-arxiv-2023-Check}
B.~Peng, M.~Galley, P.~He, H.~Cheng, Y.~Xie, Y.~Hu, Q.~Huang, L.~Liden, Z.~Yu,
  W.~Chen, and J.~Gao, ``Check your facts and try again: Improving large
  language models with external knowledge and automated feedback,''
  \emph{CoRR}, vol. abs/2302.12813, 2023.

\bibitem{Jiang-2023-arxiv-Active}
Z.~Jiang, F.~F. Xu, L.~Gao, Z.~Sun, Q.~Liu, J.~Dwivedi{-}Yu, Y.~Yang,
  J.~Callan, and G.~Neubig, ``Active retrieval augmented generation,''
  \emph{CoRR}, vol. abs/2305.06983, 2023.

\bibitem{Huang-arxiv-2023-A}
L.~Huang, W.~Yu, W.~Ma, W.~Zhong, Z.~Feng, H.~Wang, Q.~Chen, W.~Peng, X.~Feng,
  B.~Qin, and T.~Liu, ``A survey on hallucination in large language models:
  Principles, taxonomy, challenges, and open questions,'' \emph{CoRR}, vol.
  abs/2311.05232, 2023.

\bibitem{Li-arxiv-2023-Evaluating}
Y.~Li, Y.~Du, K.~Zhou, J.~Wang, W.~X. Zhao, and J.~Wen, ``Evaluating object
  hallucination in large vision-language models,'' \emph{CoRR}, vol.
  abs/2305.10355, 2023.

\bibitem{Kadavath-arxiv-2023-Language}
S.~Kadavath, T.~Conerly, A.~Askell, T.~J. Henighan, D.~Drain, E.~Perez,
  N.~Schiefer, Z.~Dodds, N.~DasSarma, E.~Tran-Johnson, S.~Johnston,
  S.~El-Showk, A.~Jones, N.~Elhage, T.~Hume, A.~Chen, Y.~Bai, S.~Bowman,
  S.~Fort, D.~Ganguli, D.~Hernandez, J.~Jacobson, J.~Kernion, S.~Kravec,
  L.~Lovitt, K.~Ndousse, C.~Olsson, S.~Ringer, D.~Amodei, T.~B. Brown,
  J.~Clark, N.~Joseph, B.~Mann, S.~McCandlish, C.~Olah, and J.~Kaplan,
  ``Language models (mostly) know what they know,'' \emph{CoRR}, vol.
  abs/2207.05221, 2022.

\bibitem{Manakul-arxiv-2023-SelfCheckGPT}
P.~Manakul, A.~Liusie, and M.~J.~F. Gales, ``Selfcheckgpt: Zero-resource
  black-box hallucination detection for generative large language models,''
  \emph{ArXiv}, vol. abs/2305.06983, 2023.

\bibitem{OpenAI-blog-2023-plugins}
S.~Agarwal, I.~Akkaya, V.~Balcom, M.~Bavarian, G.~Bernadett-Shapiro,
  G.~Brockman, M.~Brundage, J.~Chan, F.~Chantzis, N.~Deutsch, B.~Eastman,
  A.~Eleti, N.~Felix, S.~P. Fishman, I.~Fulford, C.~Gibson, J.~Gross,
  M.~Heaton, J.~Hilton, X.~Hu, S.~Jain, H.~Jin, L.~Kilpatrick, C.~Kim,
  M.~Kolhede, A.~Mayne, P.~McMillan, D.~Medina, J.~Menick, A.~Mishchenko,
  A.~Nair, R.~Nayak, A.~Neelakantan, R.~Nuttall, J.~Parish, A.~T. Passos,
  A.~Perelman, F.~de~Avila Belbute~Peres, V.~Pong, J.~Schulman, E.~Sigler,
  N.~Staudacher, N.~Turley, J.~Tworek, R.~Greene, A.~Vijayvergiya, C.~Voss,
  J.~Weng, M.~Wiethoff, S.~Yoo, K.~Yu, W.~Zaremba, S.~Zhao, W.~Zhuk, and
  B.~Zoph, ``Chatgpt plugins,'' \emph{OpenAI Blog}, March 2023.

\bibitem{Lazaridou-arxiv-2022-Internet}
A.~Lazaridou, E.~Gribovskaya, W.~Stokowiec, and N.~Grigorev,
  ``Internet-augmented language models through few-shot prompting for
  open-domain question answering,'' \emph{CoRR}, vol. abs/2203.05115, 2022.

\bibitem{Qian-2023-arxiv-WebBrain}
H.~Qian, Y.~Zhu, Z.~Dou, H.~Gu, X.~Zhang, Z.~Liu, R.~Lai, Z.~Cao, J.~Nie, and
  J.~Wen, ``Webbrain: Learning to generate factually correct articles for
  queries by grounding on large web corpus,'' \emph{CoRR}, vol. abs/2304.04358,
  2023.

\bibitem{Liu-2023-arxiv-RETA-LLM}
J.~Liu, J.~Jin, Z.~Wang, J.~Cheng, Z.~Dou, and J.~Wen, ``{RETA-LLM:} {A}
  retrieval-augmented large language model toolkit,'' \emph{CoRR}, vol.
  abs/2306.05212, 2023.

\bibitem{Dai-ACL-2022-Knowledge}
D.~Dai, L.~Dong, Y.~Hao, Z.~Sui, B.~Chang, and F.~Wei, ``Knowledge neurons in
  pretrained transformers,'' in \emph{Proceedings of the 60th Annual Meeting of
  the Association for Computational Linguistics (Volume 1: Long Papers), {ACL}
  2022, Dublin, Ireland, May 22-27, 2022}, S.~Muresan, P.~Nakov, and
  A.~Villavicencio, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2022, pp. 8493--8502.

\bibitem{Meng-NIPS-2022-Locating}
K.~Meng, D.~Bau, A.~J. Andonian, and Y.~Belinkov, ``Locating and editing
  factual associations in gpt,'' in \emph{Advances in Neural Information
  Processing Systems}, 2022.

\bibitem{Geva-2021-emnlp-Transformer}
M.~Geva, R.~Schuster, J.~Berant, and O.~Levy, ``Transformer feed-forward layers
  are key-value memories,'' in \emph{Proceedings of the 2021 Conference on
  Empirical Methods in Natural Language Processing, {EMNLP} 2021, Virtual Event
  / Punta Cana, Dominican Republic, 7-11 November, 2021}, M.~Moens, X.~Huang,
  L.~Specia, and S.~W. Yih, Eds.\hskip 1em plus 0.5em minus 0.4em\relax
  Association for Computational Linguistics, 2021, pp. 5484--5495.

\bibitem{Yao-arxiv-2023-Editing}
Y.~Yao, P.~Wang, B.~Tian, S.~Cheng, Z.~Li, S.~Deng, H.~Chen, and N.~Zhang,
  ``Editing large language models: Problems, methods, and opportunities,''
  \emph{CoRR}, vol. abs/2305.13172, 2023.

\bibitem{wang-CoRR-2023-EasyEdit}
P.~Wang, N.~Zhang, X.~Xie, Y.~Yao, B.~Tian, M.~Wang, Z.~Xi, S.~Cheng, K.~Liu,
  G.~Zheng, and H.~Chen, ``Easyedit: An easy-to-use knowledge editing framework
  for large language models,'' \emph{CoRR}, vol. abs/2308.07269, 2023.

\bibitem{Shao-arxiv-2023-Synthetic}
Z.~Shao, Y.~Gong, Y.~Shen, M.~Huang, N.~Duan, and W.~Chen, ``Synthetic
  prompting: Generating chain-of-thought demonstrations for large language
  models,'' \emph{CoRR}, vol. abs/2302.00618, 2023.

\bibitem{Sifatkaur-arxiv-2023-Mind}
Sifatkaur, M.~Singh, V.~S. B, and N.~Malviya, ``Mind meets machine: Unravelling
  gpt-4's cognitive psychology,'' \emph{CoRR}, vol. abs/2303.11436, 2023.

\bibitem{Nye-arxiv-2021-Show}
M.~I. Nye, A.~J. Andreassen, G.~Gur{-}Ari, H.~Michalewski, J.~Austin,
  D.~Bieber, D.~Dohan, A.~Lewkowycz, M.~Bosma, D.~Luan, C.~Sutton, and
  A.~Odena, ``Show your work: Scratchpads for intermediate computation with
  language models,'' \emph{CoRR}, vol. abs/2112.00114, 2021.

\bibitem{Qian-arxiv-2022-Limitations}
J.~Qian, H.~Wang, Z.~Li, S.~Li, and X.~Yan, ``Limitations of language models in
  arithmetic and symbolic induction,'' \emph{CoRR}, vol. abs/2208.05051, 2022.

\bibitem{Zhao-KDD-2022-JiuZhang}
W.~X. Zhao, K.~Zhou, Z.~Gong, B.~Zhang, Y.~Zhou, J.~Sha, Z.~Chen, S.~Wang,
  C.~Liu, and J.~Wen, ``Jiuzhang: {A} chinese pre-trained language model for
  mathematical problem understanding,'' in \emph{{KDD} '22: The 28th {ACM}
  {SIGKDD} Conference on Knowledge Discovery and Data Mining, Washington, DC,
  USA, August 14 - 18, 2022}, A.~Zhang and H.~Rangwala, Eds.\hskip 1em plus
  0.5em minus 0.4em\relax {ACM}, 2022, pp. 4571--4581.

\bibitem{Wang-CICM-2018-First}
Q.~Wang, C.~Kaliszyk, and J.~Urban, ``First experiments with neural translation
  of informal to formal mathematics,'' in \emph{Intelligent Computer
  Mathematics - 11th International Conference, {CICM} 2018, Hagenberg, Austria,
  August 13-17, 2018, Proceedings}, ser. Lecture Notes in Computer Science,
  F.~Rabe, W.~M. Farmer, G.~O. Passmore, and A.~Youssef, Eds., vol.
  11006.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2018, pp. 255--270.

\bibitem{Polu-arxiv-2020-Generative}
S.~Polu and I.~Sutskever, ``Generative language modeling for automated theorem
  proving,'' \emph{CoRR}, vol. abs/2009.03393, 2020.

\bibitem{Jiang-arxiv-2022-Thor}
A.~Q. Jiang, W.~Li, S.~Tworkowski, K.~Czechowski, T.~Odrzyg{\'{o}}zdz,
  P.~Milos, Y.~Wu, and M.~Jamnik, ``Thor: Wielding hammers to integrate
  language models and automated theorem provers,'' \emph{CoRR}, vol.
  abs/2205.10893, 2022.

\bibitem{Polu-arxiv-2022-Formal}
S.~Polu, J.~M. Han, K.~Zheng, M.~Baksys, I.~Babuschkin, and I.~Sutskever,
  ``Formal mathematics statement curriculum learning,'' \emph{CoRR}, vol.
  abs/2202.01344, 2022.

\bibitem{Wu-arxiv-2022-Autoformalization}
Y.~Wu, A.~Q. Jiang, W.~Li, M.~N. Rabe, C.~Staats, M.~Jamnik, and C.~Szegedy,
  ``Autoformalization with large language models,'' \emph{CoRR}, vol.
  abs/2205.12615, 2022.

\bibitem{Jiang-arxiv-2022-Draft}
A.~Q. Jiang, S.~Welleck, J.~P. Zhou, W.~Li, J.~Liu, M.~Jamnik, T.~Lacroix,
  Y.~Wu, and G.~Lample, ``Draft, sketch, and prove: Guiding formal theorem
  provers with informal proofs,'' \emph{CoRR}, vol. abs/2210.12283, 2022.

\bibitem{Madaan-arxiv-2023-Refine}
A.~Madaan, N.~Tandon, P.~Gupta, S.~Hallinan, L.~Gao, S.~Wiegreffe, U.~Alon,
  N.~Dziri, S.~Prabhumoye, Y.~Yang, S.~Welleck, B.~P. Majumder, S.~Gupta,
  A.~Yazdanbakhsh, and P.~Clark, ``Self-refine: Iterative refinement with
  self-feedback,'' \emph{CoRR}, vol. abs/2303.17651, 2023.

\bibitem{Shinn-arxiv-2023-Reflexion}
N.~Shinn, B.~Labash, and A.~Gopinath, ``Reflexion: an autonomous agent with
  dynamic memory and self-reflection,'' \emph{CoRR}, vol. abs/2303.11366, 2023.

\bibitem{Gou-arxiv-2023-Critic}
Z.~Gou, Z.~Shao, Y.~Gong, Y.~Shen, Y.~Yang, N.~Duan, and W.~Chen, ``{CRITIC:}
  large language models can self-correct with tool-interactive critiquing,''
  \emph{CoRR}, vol. abs/2305.11738, 2023.

\bibitem{Uesate-2023-arxiv-Solving}
J.~Uesato, N.~Kushman, R.~Kumar, H.~F. Song, N.~Y. Siegel, L.~Wang,
  A.~Creswell, G.~Irving, and I.~Higgins, ``Solving math word problems with
  process- and outcome-based feedback,'' \emph{CoRR}, vol. abs/2211.14275,
  2022.

\bibitem{Lightman-2023-arxiv-Let}
H.~Lightman, V.~Kosaraju, Y.~Burda, H.~Edwards, B.~Baker, T.~Lee, J.~Leike,
  J.~Schulman, I.~Sutskever, and K.~Cobbe, ``Let's verify step by step,''
  \emph{CoRR}, vol. abs/2305.20050, 2023.

\bibitem{Yuan-arxiv-2023-Arithmetic}
Z.~Yuan, H.~Yuan, C.~Tan, W.~Wang, and S.~Huang, ``How well do large language
  models perform in arithmetic tasks?'' \emph{CoRR}, vol. abs/2304.02015, 2023.

\bibitem{Pi-EMNLP-2022-Reasoning}
X.~Pi, Q.~Liu, B.~Chen, M.~Ziyadi, Z.~Lin, Q.~Fu, Y.~Gao, J.~Lou, and W.~Chen,
  ``Reasoning like program executors,'' in \emph{Proceedings of the 2022
  Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2022,
  Abu Dhabi, United Arab Emirates, December 7-11, 2022}, 2022, pp. 761--779.

\bibitem{Zhou-2023-arxiv-Teaching}
H.~Zhou, A.~Nova, H.~Larochelle, A.~C. Courville, B.~Neyshabur, and H.~Sedghi,
  ``Teaching algorithmic reasoning via in-context learning,'' \emph{CoRR}, vol.
  abs/2211.09066, 2022.

\bibitem{Parisi-arxiv-2022-TALM}
A.~Parisi, Y.~Zhao, and N.~Fiedel, ``{TALM:} tool augmented language models,''
  \emph{CoRR}, vol. abs/2205.12255, 2022.

\bibitem{Huang-ICML-2022-Language}
W.~Huang, P.~Abbeel, D.~Pathak, and I.~Mordatch, ``Language models as zero-shot
  planners: Extracting actionable knowledge for embodied agents,'' in
  \emph{{ICML}}, ser. Proceedings of Machine Learning Research, vol. 162.\hskip
  1em plus 0.5em minus 0.4em\relax {PMLR}, 2022, pp. 9118--9147.

\bibitem{Carta-arxiv-2023-Grounding}
T.~Carta, C.~Romac, T.~Wolf, S.~Lamprier, O.~Sigaud, and P.~Oudeyer,
  ``Grounding large language models in interactive environments with online
  reinforcement learning,'' \emph{CoRR}, vol. abs/2302.02662, 2023.

\bibitem{Zhu-arxiv-2023-Ghost}
X.~Zhu, Y.~Chen, H.~Tian, C.~Tao, W.~Su, C.~Yang, G.~Huang, B.~Li, L.~Lu,
  X.~Wang, Y.~Qiao, Z.~Zhang, and J.~Dai, ``Ghost in the minecraft: Generally
  capable agents for open-world environments via large language models with
  text-based knowledge and memory,'' \emph{CoRR}, vol. abs/2305.17144, 2023.

\bibitem{Wang-arxiv-2023-Voyager}
G.~Wang, Y.~Xie, Y.~Jiang, A.~Mandlekar, C.~Xiao, Y.~Zhu, L.~Fan, and
  A.~Anandkumar, ``Voyager: An open-ended embodied agent with large language
  models,'' \emph{CoRR}, vol. abs/2305.16291, 2023.

\bibitem{Ahn-arxiv-2022-Do}
M.~Ahn, A.~Brohan, N.~Brown, Y.~Chebotar, O.~Cortes, B.~David, C.~Finn,
  K.~Gopalakrishnan, K.~Hausman, A.~Herzog, D.~Ho, J.~Hsu, J.~Ibarz, B.~Ichter,
  A.~Irpan, E.~Jang, R.~J. Ruano, K.~Jeffrey, S.~Jesmonth, N.~J. Joshi,
  R.~Julian, D.~Kalashnikov, Y.~Kuang, K.~Lee, S.~Levine, Y.~Lu, L.~Luu,
  C.~Parada, P.~Pastor, J.~Quiambao, K.~Rao, J.~Rettinghouse, D.~Reyes,
  P.~Sermanet, N.~Sievers, C.~Tan, A.~Toshev, V.~Vanhoucke, F.~Xia, T.~Xiao,
  P.~Xu, S.~Xu, and M.~Yan, ``Do as {I} can, not as {I} say: Grounding language
  in robotic affordances,'' \emph{CoRR}, vol. abs/2204.01691, 2022.

\bibitem{Liang-arxiv-2022-Code}
J.~Liang, W.~Huang, F.~Xia, P.~Xu, K.~Hausman, B.~Ichter, P.~Florence, and
  A.~Zeng, ``Code as policies: Language model programs for embodied control,''
  \emph{CoRR}, vol. abs/2209.07753, 2022.

\bibitem{Fu-arxiv-2023-Improving}
Y.~Fu, H.~Peng, T.~Khot, and M.~Lapata, ``Improving language model negotiation
  with self-play and in-context learning from {AI} feedback,'' \emph{CoRR},
  vol. abs/2305.10142, 2023.

\bibitem{Metha-arxiv-2023-Improving}
N.~Mehta, M.~Teruel, P.~F. Sanz, X.~Deng, A.~H. Awadallah, and J.~Kiseleva,
  ``Improving grounded language understanding in a collaborative environment by
  interacting with agents through help feedback,'' \emph{CoRR}, vol.
  abs/2304.10750, 2023.

\bibitem{Shishir-2023-arxiv-Gorilla}
S.~G. Patil, T.~Zhang, X.~Wang, and J.~E. Gonzalez, ``Gorilla: Large language
  model connected with massive apis,'' \emph{CoRR}, vol. abs/2305.15334, 2023.

\bibitem{Hao-2023-arxiv-ToolkenGPT}
S.~Hao, T.~Liu, Z.~Wang, and Z.~Hu, ``Toolkengpt: Augmenting frozen language
  models with massive tools via tool embeddings,'' \emph{CoRR}, vol.
  abs/2305.11554, 2023.

\bibitem{Liang-2023-arxiv-TaskMatrix}
Y.~Liang, C.~Wu, T.~Song, W.~Wu, Y.~Xia, Y.~Liu, Y.~Ou, S.~Lu, L.~Ji, S.~Mao,
  Y.~Wang, L.~Shou, M.~Gong, and N.~Duan, ``Taskmatrix.ai: Completing tasks by
  connecting foundation models with millions of apis,'' \emph{CoRR}, vol.
  abs/2303.16434, 2023.

\bibitem{Cai-arxiv-2023-Tool}
T.~Cai, X.~Wang, T.~Ma, X.~Chen, and D.~Zhou, ``Large language models as tool
  makers,'' \emph{CoRR}, vol. abs/2305.17126, 2023.

\bibitem{Huang-arxiv-2022-Large}
J.~Huang, S.~S. Gu, L.~Hou, Y.~Wu, X.~Wang, H.~Yu, and J.~Han, ``Large language
  models can self-improve,'' \emph{CoRR}, vol. abs/2210.11610, 2022.

\bibitem{Edward-2023-hf-open}
E.~Beeching, C.~Fourrier, N.~Habib, S.~Han, N.~Lambert, N.~Rajani,
  O.~Sanseviero, L.~Tunstall, and T.~Wolf, ``Open llm leaderboard,''
  \url{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}, 2023.

\bibitem{Zhong-2023-arxiv-AGIEval}
W.~Zhong, R.~Cui, Y.~Guo, Y.~Liang, S.~Lu, Y.~Wang, A.~Saied, W.~Chen, and
  N.~Duan, ``Agieval: {A} human-centric benchmark for evaluating foundation
  models,'' \emph{CoRR}, vol. abs/2304.06364, 2023.

\bibitem{Zeng-arxiv-2023-MMCU}
H.~Zeng, ``Measuring massive multitask chinese understanding,'' \emph{CoRR},
  vol. abs/2304.12986, 2023.

\bibitem{Liu-2023-arxiv-M3KE}
C.~Liu, R.~Jin, Y.~Ren, L.~Yu, T.~Dong, X.~Peng, S.~Zhang, J.~Peng, P.~Zhang,
  Q.~Lyu, X.~Su, Q.~Liu, and D.~Xiong, ``{M3KE:} {A} massive multi-level
  multi-subject knowledge evaluation benchmark for chinese large language
  models,'' \emph{CoRR}, vol. abs/2305.10263, 2023.

\bibitem{Huang-arxiv-2023-CEval}
Y.~Huang, Y.~Bai, Z.~Zhu, J.~Zhang, J.~Zhang, T.~Su, J.~Liu, C.~Lv, Y.~Zhang,
  J.~Lei, Y.~Fu, M.~Sun, and J.~He, ``C-eval: {A} multi-level multi-discipline
  chinese evaluation suite for foundation models,'' \emph{CoRR}, vol.
  abs/2305.08322, 2023.

\bibitem{Gu-2023-arxiv-Xiezhi}
Z.~Gu, X.~Zhu, H.~Ye, L.~Zhang, J.~Wang, S.~Jiang, Z.~Xiong, Z.~Li, Q.~He,
  R.~Xu, W.~Huang, W.~Zheng, H.~Feng, and Y.~Xiao, ``Xiezhi: An ever-updating
  benchmark for holistic domain knowledge evaluation,'' \emph{CoRR}, vol.
  abs/2306.05783, 2023.

\bibitem{2023opencompass}
O.~Contributors, ``Opencompass: A universal evaluation platform for foundation
  models,'' \url{https://github.com/InternLM/OpenCompass}, 2023.

\bibitem{Fu-arxiv-2023-Chain}
Y.~Fu, L.~Ou, M.~Chen, Y.~Wan, H.~Peng, and T.~Khot, ``Chain-of-thought hub:
  {A} continuous effort to measure large language models' reasoning
  performance,'' \emph{CoRR}, vol. abs/2305.17306, 2023.

\bibitem{Yu-arxiv-2023-KoLA}
J.~Yu, X.~Wang, S.~Tu, S.~Cao, D.~Zhang{-}li, X.~Lv, H.~Peng, Z.~Yao, X.~Zhang,
  H.~Li, C.~Li, Z.~Zhang, Y.~Bai, Y.~Liu, A.~Xin, N.~Lin, K.~Yun, L.~Gong,
  J.~Chen, Z.~Wu, Y.~Qi, W.~Li, Y.~Guan, K.~Zeng, J.~Qi, H.~Jin, J.~Liu, Y.~Gu,
  Y.~Yao, N.~Ding, L.~Hou, Z.~Liu, B.~Xu, J.~Tang, and J.~Li, ``Kola: Carefully
  benchmarking world knowledge of large language models,'' \emph{CoRR}, vol.
  abs/2306.09296, 2023.

\bibitem{Sawada-arxiv-2023-ARB}
T.~Sawada, D.~Paleka, A.~Havrilla, P.~Tadepalli, P.~Vidas, A.~Kranias, J.~J.
  Nay, K.~Gupta, and A.~Komatsuzaki, ``{ARB:} advanced reasoning benchmark for
  large language models,'' \emph{CoRR}, vol. abs/2307.13692, 2023.

\bibitem{Peng-arxiv-2023-Revisiting}
Y.~Peng, S.~Li, W.~Gu, Y.~Li, W.~Wang, C.~Gao, and M.~R. Lyu, ``Revisiting,
  benchmarking and exploring {API} recommendation: How far are we?''
  \emph{{IEEE} Trans. Software Eng.}, vol.~49, no.~4, pp. 1876--1897, 2023.

\bibitem{Li-arxiv-2023-API-Bank}
M.~Li, F.~Song, B.~Yu, H.~Yu, Z.~Li, F.~Huang, and Y.~Li, ``Api-bank: {A}
  benchmark for tool-augmented llms,'' \emph{CoRR}, vol. abs/2304.08244, 2023.

\bibitem{Tang-arxiv-2023-ToolAlpaca}
Q.~Tang, Z.~Deng, H.~Lin, X.~Han, Q.~Liang, and L.~Sun, ``Toolalpaca:
  Generalized tool learning for language models with 3000 simulated cases,''
  \emph{CoRR}, vol. abs/2306.05301, 2023.

\bibitem{Xu-arxiv-2023-On}
Q.~Xu, F.~Hong, B.~Li, C.~Hu, Z.~Chen, and J.~Zhang, ``On the tool manipulation
  capability of open-source large language models,'' \emph{CoRR}, vol.
  abs/2305.16504, 2023.

\bibitem{Qin-arxiv-2023-ToolLLM}
Y.~Qin, S.~Liang, Y.~Ye, K.~Zhu, L.~Yan, Y.~Lu, Y.~Lin, X.~Cong, X.~Tang,
  B.~Qian, S.~Zhao, R.~Tian, R.~Xie, J.~Zhou, M.~Gerstein, D.~Li, Z.~Liu, and
  M.~Sun, ``Toolllm: Facilitating large language models to master 16000+
  real-world apis,'' \emph{CoRR}, vol. abs/2307.16789, 2023.

\bibitem{Liu-arxiv-2023-BOLAA}
Z.~Liu, W.~Yao, J.~Zhang, L.~Xue, S.~Heinecke, R.~Murthy, Y.~Feng, Z.~Chen,
  J.~C. Niebles, D.~Arpit, R.~Xu, P.~Mui, H.~Wang, C.~Xiong, and S.~Savarese,
  ``{BOLAA:} benchmarking and orchestrating llm-augmented autonomous agents,''
  \emph{CoRR}, vol. abs/2308.05960, 2023.

\bibitem{Liu-arxiv-2023-AgentBench}
X.~Liu, H.~Yu, H.~Zhang, Y.~Xu, X.~Lei, H.~Lai, Y.~Gu, H.~Ding, K.~Men,
  K.~Yang, S.~Zhang, X.~Deng, A.~Zeng, Z.~Du, C.~Zhang, S.~Shen, T.~Zhang,
  Y.~Su, H.~Sun, M.~Huang, Y.~Dong, and J.~Tang, ``Agentbench: Evaluating llms
  as agents,'' \emph{CoRR}, vol. abs/2308.03688, 2023.

\bibitem{Zhu-arxiv-2023-PromptBench}
K.~Zhu, J.~Wang, J.~Zhou, Z.~Wang, H.~Chen, Y.~Wang, L.~Yang, W.~Ye, N.~Z.
  Gong, Y.~Zhang, and X.~Xie, ``Promptbench: Towards evaluating the robustness
  of large language models on adversarial prompts,'' \emph{CoRR}, vol.
  abs/2306.04528, 2023.

\bibitem{Shah-arxiv-2023-FLUE}
R.~S. Shah, K.~Chawla, D.~Eidnani, A.~Shah, W.~Du, S.~Chava, N.~Raman,
  C.~Smiley, J.~Chen, and D.~Yang, ``{WHEN} {FLUE} {MEETS} {FLANG:} benchmarks
  and large pre-trained language model for financial domain,'' \emph{CoRR},
  vol. abs/2211.00083, 2022.

\bibitem{Guha-arxiv-2022-LegalBench}
N.~Guha, D.~E. Ho, J.~Nyarko, and C.~R{\'e}, ``Legalbench: Prototyping a
  collaborative benchmark for legal reasoning,'' \emph{CoRR}, vol.
  abs/2209.06120, 2022.

\bibitem{Zheng-2023-arxiv-Judging}
L.~Zheng, W.~Chiang, Y.~Sheng, S.~Zhuang, Z.~Wu, Y.~Zhuang, Z.~Lin, Z.~Li,
  D.~Li, E.~P. Xing, H.~Zhang, J.~E. Gonzalez, and I.~Stoica, ``Judging
  llm-as-a-judge with mt-bench and chatbot arena,'' \emph{CoRR}, vol.
  abs/2306.05685, 2023.

\bibitem{Wang-arxiv-2023-SciBench}
X.~Wang, Z.~Hu, P.~Lu, Y.~Zhu, J.~Zhang, S.~Subramaniam, A.~R. Loomba,
  S.~Zhang, Y.~Sun, and W.~Wang, ``Scibench: Evaluating college-level
  scientific problem-solving abilities of large language models,'' \emph{CoRR},
  vol. abs/2307.10635, 2023.

\bibitem{Li-2023-github-alpaca_eval}
X.~Li, T.~Zhang, Y.~Dubois, R.~Taori, I.~Gulrajani, C.~Guestrin, P.~Liang, and
  T.~B. Hashimoto, ``Alpacaeval: An automatic evaluator of
  instruction-following models,''
  \url{https://github.com/tatsu-lab/alpaca_eval}, 2023.

\bibitem{Huang-arxiv-2023-TrustGPT}
Y.~Huang, Q.~Zhang, P.~S. Yu, and L.~Sun, ``Trustgpt: {A} benchmark for
  trustworthy and responsible large language models,'' \emph{CoRR}, vol.
  abs/2306.11507, 2023.

\bibitem{Bai-arxiv-2023-Benchmarking}
Y.~Bai, J.~Ying, Y.~Cao, X.~Lv, Y.~He, X.~Wang, J.~Yu, K.~Zeng, Y.~Xiao,
  H.~Lyu, J.~Zhang, J.~Li, and L.~Hou, ``Benchmarking foundation models with
  language-model-as-an-examiner,'' \emph{CoRR}, vol. abs/2306.04181, 2023.

\bibitem{Chan-arixiv-2023-ChatEval}
C.~Chan, W.~Chen, Y.~Su, J.~Yu, W.~Xue, S.~Zhang, J.~Fu, and Z.~Liu,
  ``Chateval: Towards better llm-based evaluators through multi-agent debate,''
  \emph{CoRR}, vol. abs/2308.07201, 2023.

\bibitem{Chang-2023-arxiv-A}
Y.~Chang, X.~Wang, J.~Wang, Y.~Wu, K.~Zhu, H.~Chen, L.~Yang, X.~Yi, C.~Wang,
  Y.~Wang, W.~Ye, Y.~Zhang, Y.~Chang, P.~S. Yu, Q.~Yang, and X.~Xie, ``A survey
  on evaluation of large language models,'' \emph{CoRR}, vol. abs/2307.03109,
  2023.

\bibitem{Zhuang-2023-arxiv-Through}
Z.~Zhuang, Q.~Chen, L.~Ma, M.~Li, Y.~Han, Y.~Qian, H.~Bai, Z.~Feng, W.~Zhang,
  and T.~Liu, ``Through the lens of core competency: Survey on evaluation of
  large language models,'' \emph{CoRR}, vol. abs/2308.07902, 2023.

\bibitem{Clark-trans-2020-TyDi}
J.~H. Clark, J.~Palomaki, V.~Nikolaev, E.~Choi, D.~Garrette, M.~Collins, and
  T.~Kwiatkowski, ``Tydi {QA:} {A} benchmark for information-seeking question
  answering in typologically diverse languages,'' \emph{Trans. Assoc. Comput.
  Linguistics}, vol.~8, pp. 454--470, 2020.

\bibitem{Leo-zenodo-2021-A}
L.~Gao, J.~Tow, S.~Biderman, S.~Black, A.~DiPofi, C.~Foster, L.~Golding,
  J.~Hsu, K.~McDonell, N.~Muennighoff, J.~Phang, L.~Reynolds, E.~Tang,
  A.~Thite, B.~Wang, K.~Wang, and A.~Zou, ``A framework for few-shot language
  model evaluation,'' Sep. 2021.

\bibitem{Shah-2022-EMNLP-When}
R.~Shah, K.~Chawla, D.~Eidnani, A.~Shah, W.~Du, S.~Chava, N.~Raman, C.~Smiley,
  J.~Chen, and D.~Yang, ``When flue meets flang: Benchmarks and large
  pretrained language model for financial domain,'' in \emph{Proceedings of the
  2022 Conference on Empirical Methods in Natural Language Processing}, 2022,
  pp. 2322--2335.

\bibitem{zhou-arxiv-2023-dont}
K.~Zhou, Y.~Zhu, Z.~Chen, W.~Chen, W.~X. Zhao, X.~Chen, Y.~Lin, J.-R. Wen, and
  J.~Han, ``Don't make your llm an evaluation benchmark cheater,'' \emph{arXiv
  preprint arXiv:2311.01964}, 2023.

\bibitem{Zan-WMT-2022-Vega-MT}
C.~Zan, K.~Peng, L.~Ding, B.~Qiu, B.~Liu, S.~He, Q.~Lu, Z.~Zhang, C.~Liu,
  W.~Liu, Y.~Zhan, and D.~Tao, ``Vega-mt: The {JD} explore academy machine
  translation system for {WMT22},'' in \emph{Proceedings of the Seventh
  Conference on Machine Translation, {WMT} 2022, Abu Dhabi, United Arab
  Emirates (Hybrid), December 7-8, 2022}, P.~Koehn, L.~Barrault, O.~Bojar,
  F.~Bougares, R.~Chatterjee, M.~R. Costa{-}juss{\`{a}}, C.~Federmann,
  M.~Fishel, A.~Fraser, M.~Freitag, Y.~Graham, R.~Grundkiewicz, P.~Guzman,
  B.~Haddow, M.~Huck, A.~Jimeno{-}Yepes, T.~Kocmi, A.~Martins, M.~Morishita,
  C.~Monz, M.~Nagata, T.~Nakazawa, M.~Negri, A.~N{\'{e}}v{\'{e}}ol, M.~Neves,
  M.~Popel, M.~Turchi, and M.~Zampieri, Eds.\hskip 1em plus 0.5em minus
  0.4em\relax Association for Computational Linguistics, 2022, pp. 411--422.

\bibitem{Zhao-arxiv-2022-Calibrating}
\BIBentryALTinterwordspacing
Y.~Zhao, M.~Khalman, R.~Joshi, S.~Narayan, M.~Saleh, and P.~J. Liu,
  ``Calibrating sequence likelihood improves conditional language generation,''
  \emph{CoRR}, vol. abs/2210.00045, 2022. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2210.00045}
\BIBentrySTDinterwordspacing

\bibitem{Khashabi-EMNLP-2020-UnifiedQA}
D.~Khashabi, S.~Min, T.~Khot, A.~Sabharwal, O.~Tafjord, P.~Clark, and
  H.~Hajishirzi, ``Unifiedqa: Crossing format boundaries with a single {QA}
  system,'' in \emph{{EMNLP} (Findings)}, ser. Findings of {ACL}, vol. {EMNLP}
  2020.\hskip 1em plus 0.5em minus 0.4em\relax Association for Computational
  Linguistics, 2020, pp. 1896--1907.

\bibitem{Zhu-arxiv-2022-Solving}
X.~Zhu, J.~Wang, L.~Zhang, Y.~Zhang, R.~Gan, J.~Zhang, and Y.~Yang, ``Solving
  math word problem via cooperative reasoning induced language models,''
  \emph{arXiv preprint arXiv:2210.16257}, 2022.

\bibitem{Nguyen-arxiv-2023-Meet}
\BIBentryALTinterwordspacing
A.~Nguyen, N.~Karampatziakis, and W.~Chen, ``Meet in the middle: {A} new
  pre-training paradigm,'' \emph{CoRR}, vol. abs/2303.07295, 2023. [Online].
  Available: \url{https://doi.org/10.48550/arXiv.2303.07295}
\BIBentrySTDinterwordspacing

\bibitem{Li-arxiv-2023-RESDSQL}
\BIBentryALTinterwordspacing
H.~Li, J.~Zhang, C.~Li, and H.~Chen, ``{RESDSQL:} decoupling schema linking and
  skeleton parsing for text-to-sql,'' \emph{CoRR}, vol. abs/2302.05965, 2023.
  [Online]. Available: \url{https://doi.org/10.48550/arXiv.2302.05965}
\BIBentrySTDinterwordspacing

\bibitem{Kang-ICDM-2018-Self}
W.~Kang and J.~J. McAuley, ``Self-attentive sequential recommendation,'' in
  \emph{{IEEE} International Conference on Data Mining, {ICDM} 2018, Singapore,
  November 17-20, 2018}.\hskip 1em plus 0.5em minus 0.4em\relax {IEEE} Computer
  Society, 2018, pp. 197--206.

\bibitem{Yang-NAACL-2022-Improving}
B.~Yang, C.~Han, Y.~Li, L.~Zuo, and Z.~Yu, ``Improving conversational
  recommendation systems' quality with context-aware item meta-information,''
  in \emph{Findings of the Association for Computational Linguistics: {NAACL}
  2022, Seattle, WA, United States, July 10-15, 2022}, M.~Carpuat,
  M.~de~Marneffe, and I.~V.~M. Ru{\'{\i}}z, Eds.\hskip 1em plus 0.5em minus
  0.4em\relax Association for Computational Linguistics, 2022, pp. 38--48.

\bibitem{Ebtesam-arxiv-2023-Falcon}
E.~Almazrouei, H.~Alobeidli, A.~Alshamsi, A.~Cappelli, R.~Cojocaru, M.~Debbah,
  E.~Goffinet, D.~Heslow, J.~Launay, Q.~Malartic, B.~Noune, B.~Pannier, and
  G.~Penedo, ``{Falcon-40B}: an open large language model with state-of-the-art
  performance,'' 2023.

\bibitem{Martin-Speech-1998-clustering}
S.~Martin, J.~Liermann, and H.~Ney, ``Algorithms for bigram and trigram word
  clustering,'' \emph{Speech communication}, vol.~24, no.~1, pp. 19--37, 1998.

\bibitem{Navigli-ACM-2009-disambiguation}
R.~Navigli, ``Word sense disambiguation: A survey,'' \emph{ACM computing
  surveys (CSUR)}, vol.~41, no.~2, pp. 1--69, 2009.

\bibitem{Gomaa-International-2013-similarity}
W.~H. Gomaa, A.~A. Fahmy \emph{et~al.}, ``A survey of text similarity
  approaches,'' \emph{international journal of Computer Applications}, vol.~68,
  no.~13, pp. 13--18, 2013.

\bibitem{Minaee-ACM-2021-classification}
S.~Minaee, N.~Kalchbrenner, E.~Cambria, N.~Nikzad, M.~Chenaghlu, and J.~Gao,
  ``Deep learning--based text classification: a comprehensive review,''
  \emph{ACM computing surveys (CSUR)}, vol.~54, no.~3, pp. 1--40, 2021.

\bibitem{Alex-NIPS-2021-RAFT}
N.~Alex, E.~Lifland, L.~Tunstall, A.~Thakur, P.~Maham, C.~J. Riedel, E.~Hine,
  C.~Ashurst, P.~Sedille, A.~Carlier, M.~Noetel, and A.~Stuhlm{\"{u}}ller,
  ``{RAFT:} {A} real-world few-shot text classification benchmark,'' in
  \emph{NeurIPS Datasets and Benchmarks}, 2021.

\bibitem{Qin-arxiv-2023-Is}
C.~Qin, A.~Zhang, Z.~Zhang, J.~Chen, M.~Yasunaga, and D.~Yang, ``Is chatgpt a
  general-purpose natural language processing task solver?'' \emph{CoRR}, vol.
  abs/2302.06476, 2023.

\bibitem{Chen-arxiv-2023-Robust}
X.~Chen, J.~Ye, C.~Zu, N.~Xu, R.~Zheng, M.~Peng, J.~Zhou, T.~Gui, Q.~Zhang, and
  X.~Huang, ``How robust is gpt-3.5 to predecessors? a comprehensive study on
  language understanding tasks,'' 2023.

\bibitem{Nadeau-Lingvisticae-2007-NER}
D.~Nadeau and S.~Sekine, ``A survey of named entity recognition and
  classification,'' \emph{Lingvisticae Investigationes}, vol.~30, no.~1, pp.
  3--26, 2007.

\bibitem{Ratnaparkhi-EMNLP-1996-maximum}
A.~Ratnaparkhi, ``A maximum entropy model for part-of-speech tagging,'' in
  \emph{Conference on empirical methods in natural language processing}, 1996.

\bibitem{Yadav-COLING-2018-survey}
V.~Yadav and S.~Bethard, ``A survey on recent advances in named entity
  recognition from deep learning models,'' in \emph{Proceedings of the 27th
  International Conference on Computational Linguistics}, 2018, pp. 2145--2158.

\bibitem{Souza-arxiv-2019-portuguese}
F.~Souza, R.~Nogueira, and R.~Lotufo, ``Portuguese named entity recognition
  using bert-crf,'' \emph{arXiv preprint arXiv:1909.10649}, 2019.

\bibitem{Pawar-arxiv-2017-relation}
S.~Pawar, G.~K. Palshikar, and P.~Bhattacharyya, ``Relation extraction: A
  survey,'' \emph{arXiv preprint arXiv:1712.05191}, 2017.

\bibitem{walker-ldc-2006-ace}
C.~Walker and et~al., ``Ace 2005 multilingual training corpus ldc2006t06,''
  Philadelphia, 2006.

\bibitem{gao-arxiv-2023-exploring}
J.~Gao, H.~Zhao, C.~Yu, and R.~Xu, ``Exploring the feasibility of chatgpt for
  event extraction,'' \emph{CoRR}, vol. abs/2303.03836, 2023.

\bibitem{ma-arxiv-2023-Large}
Y.~Ma, Y.~Cao, Y.~Hong, and A.~Sun, ``Large language model is not a good
  few-shot information extractor, but a good reranker for hard samples!''
  \emph{CoRR}, vol. abs/2303.08559, 2023.

\bibitem{tang-arxiv-2023-does}
R.~Tang, X.~Han, X.~Jiang, and X.~Hu, ``Does synthetic data generation of llms
  help clinical text mining?'' \emph{arXiv preprint arXiv:2303.04360}, 2023.

\bibitem{vaswani-CAMT-2018-tensor2tensor}
A.~Vaswani, S.~Bengio, E.~Brevdo, F.~Chollet, A.~Gomez, S.~Gouws, L.~Jones,
  {\L}.~Kaiser, N.~Kalchbrenner, N.~Parmar \emph{et~al.}, ``Tensor2tensor for
  neural machine translation,'' in \emph{Proceedings of the 13th Conference of
  the Association for Machine Translation in the Americas (Volume 1: Research
  Track)}, 2018, pp. 193--199.

\bibitem{zhang-arxiv-2023-prompting}
B.~Zhang, B.~Haddow, and A.~Birch, ``Prompting large language model for machine
  translation: A case study,'' \emph{arXiv preprint arXiv:2301.07069}, 2023.

\bibitem{ghazvininejad-arxiv-2023-dictionary}
M.~Ghazvininejad, H.~Gonen, and L.~Zettlemoyer, ``Dictionary-based phrase-level
  prompting of large language models for machine translation,'' \emph{arXiv
  preprint arXiv:2302.07856}, 2023.

\bibitem{wang-arxiv-2023-document}
L.~Wang, C.~Lyu, T.~Ji, Z.~Zhang, D.~Yu, S.~Shi, and Z.~Tu, ``Document-level
  machine translation with large language models,'' \emph{arXiv preprint
  arXiv:2304.02210}, 2023.

\bibitem{jiao-arxiv-2023-parrot}
W.~Jiao, J.-t. Huang, W.~Wang, X.~Wang, S.~Shi, and Z.~Tu, ``Parrot:
  Translating during chat using large language models,'' \emph{arXiv preprint
  arXiv:2304.02426}, 2023.

\bibitem{yang-arxiv-2023-bigtrans}
W.~Yang, C.~Li, J.~Zhang, and C.~Zong, ``Bigtrans: Augmenting large language
  models with multilingual translation capability over 100 languages,''
  \emph{arXiv preprint arXiv:2305.18098}, 2023.

\bibitem{Kocon-arxiv-2023-ChatGPT}
J.~Kocon, I.~Cichecki, O.~Kaszyca, M.~Kochanek, D.~Szydlo, J.~Baran,
  J.~Bielaniewicz, M.~Gruza, A.~Janz, K.~Kanclerz, A.~Kocon, B.~Koptyra,
  W.~Mieleszczenko{-}Kowszewicz, P.~Milkowski, M.~Oleksy, M.~Piasecki,
  L.~Radlinski, K.~Wojtasik, S.~Wozniak, and P.~Kazienko, ``Chatgpt: Jack of
  all trades, master of none,'' \emph{CoRR}, vol. abs/2302.10724, 2023.

\bibitem{Zhong-arxiv-2023-Can}
Q.~Zhong, L.~Ding, J.~Liu, B.~Du, and D.~Tao, ``Can chatgpt understand too? {A}
  comparative study on chatgpt and fine-tuned {BERT},'' \emph{CoRR}, vol.
  abs/2302.10198, 2023.

\bibitem{Cheng-arxiv-2023-UPRISE}
D.~Cheng, S.~Huang, J.~Bi, Y.~Zhan, J.~Liu, Y.~Wang, H.~Sun, F.~Wei, D.~Deng,
  and Q.~Zhang, ``Uprise: Universal prompt retrieval for improving zero-shot
  evaluation,'' \emph{arXiv preprint arXiv:2303.08518}, 2023.

\bibitem{Ren-EMNLP-2021-rocketqav2}
R.~Ren, Y.~Qu, J.~Liu, W.~X. Zhao, Q.~She, H.~Wu, H.~Wang, and J.-R. Wen,
  ``Rocketqav2: A joint training method for dense passage retrieval and passage
  re-ranking,'' in \emph{Proceedings of the 2021 Conference on Empirical
  Methods in Natural Language Processing}, 2021, pp. 2825--2835.

\bibitem{sun-arxiv-2023-chatgpt}
W.~Sun, L.~Yan, X.~Ma, P.~Ren, D.~Yin, and Z.~Ren, ``Is chatgpt good at search?
  investigating large language models as re-ranking agent,'' \emph{arXiv
  preprint arXiv:2304.09542}, 2023.

\bibitem{qin-arxiv-2023-large}
Z.~Qin, R.~Jagerman, K.~Hui, H.~Zhuang, J.~Wu, J.~Shen, T.~Liu, J.~Liu,
  D.~Metzler, X.~Wang \emph{et~al.}, ``Large language models are effective text
  rankers with pairwise ranking prompting,'' \emph{arXiv preprint
  arXiv:2306.17563}, 2023.

\bibitem{Cho-ACL-2023-Discrete}
S.~Cho, S.~Jeong, J.~Seo, and J.~C. Park, ``Discrete prompt optimization via
  constrained generation for zero-shot re-ranker,'' \emph{arXiv preprint
  arXiv:2305.13729}, 2023.

\bibitem{tang-arxiv-2023-found}
R.~Tang, X.~Zhang, X.~Ma, J.~Lin, and F.~Ture, ``Found in the middle:
  Permutation self-consistency improves listwise ranking in large language
  models,'' \emph{arXiv preprint arXiv:2310.07712}, 2023.

\bibitem{ma-arxiv-2023-zero}
X.~Ma, X.~Zhang, R.~Pradeep, and J.~Lin, ``Zero-shot listwise document
  reranking with a large language model,'' \emph{arXiv preprint
  arXiv:2305.02156}, 2023.

\bibitem{zhuang-arxiv-2023-setwise}
S.~Zhuang, H.~Zhuang, B.~Koopman, and G.~Zuccon, ``A setwise approach for
  effective and highly efficient zero-shot ranking with large language
  models,'' \emph{arXiv preprint arXiv:2310.09497}, 2023.

\bibitem{zhuang-arxiv-2023-beyond}
H.~Zhuang, Z.~Qin, K.~Hui, J.~Wu, L.~Yan, X.~Wang, and M.~Berdersky, ``Beyond
  yes and no: Improving zero-shot llm rankers via scoring fine-grained
  relevance labels,'' \emph{arXiv preprint arXiv:2310.14122}, 2023.

\bibitem{ziems-arxiv-2023-large}
N.~Ziems, W.~Yu, Z.~Zhang, and M.~Jiang, ``Large language models are built-in
  autoregressive search engines,'' \emph{arXiv preprint arXiv:2305.09612},
  2023.

\bibitem{Ma-arxiv-2023-fine}
X.~Ma, L.~Wang, N.~Yang, F.~Wei, and J.~Lin, ``Fine-tuning llama for
  multi-stage text retrieval,'' \emph{arXiv preprint arXiv:2310.08319}, 2023.

\bibitem{pradeep-arxiv-2023-rankvicuna}
R.~Pradeep, S.~Sharifymoghaddam, and J.~Lin, ``Rankvicuna: Zero-shot listwise
  document reranking with open-source large language models,'' \emph{arXiv
  preprint arXiv:2309.15088}, 2023.

\bibitem{Tay-NIPS-2022-transformer}
Y.~Tay, V.~Q. Tran, M.~Dehghani, J.~Ni, D.~Bahri, H.~Mehta, Z.~Qin, K.~Hui,
  Z.~Zhao, J.~Gupta \emph{et~al.}, ``Transformer memory as a differentiable
  search index,'' in \emph{Advances in Neural Information Processing Systems},
  2022.

\bibitem{Ren-ACL-2023-tome}
\BIBentryALTinterwordspacing
R.~Ren, W.~X. Zhao, J.~Liu, H.~Wu, J.-R. Wen, and H.~Wang, ``{TOME}: A
  two-stage approach for model-based retrieval,'' in \emph{Proceedings of the
  61st Annual Meeting of the Association for Computational Linguistics (Volume
  1: Long Papers)}.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2023, pp. 6102--6114. [Online]. Available:
  \url{https://aclanthology.org/2023.acl-long.336}
\BIBentrySTDinterwordspacing

\bibitem{Qu-NAACL-2021-rocketqa}
Y.~Qu, Y.~Ding, J.~Liu, K.~Liu, R.~Ren, W.~X. Zhao, D.~Dong, H.~Wu, and
  H.~Wang, ``Rocketqa: An optimized training approach to dense passage
  retrieval for open-domain question answering,'' in \emph{Proceedings of the
  2021 Conference of the North American Chapter of the Association for
  Computational Linguistics: Human Language Technologies}, 2021, pp.
  5835--5847.

\bibitem{Ren-ACL-2021-PAIR}
R.~Ren, S.~Lv, Y.~Qu, J.~Liu, W.~X. Zhao, Q.~She, H.~Wu, H.~Wang, and J.-R.
  Wen, ``Pair: Leveraging passage-centric similarity relation for improving
  dense passage retrieval,'' in \emph{Findings of the Association for
  Computational Linguistics: ACL-IJCNLP 2021}, 2021, pp. 2173--2183.

\bibitem{peng-arxiv-2023-soft}
Z.~Peng, X.~Wu, and Y.~Fang, ``Soft prompt tuning for augmenting dense
  retrieval with large language models,'' \emph{arXiv preprint
  arXiv:2307.08303}, 2023.

\bibitem{Dai-ICLR-2023-promptagator}
Z.~Dai, V.~Y. Zhao, J.~Ma, Y.~Luan, J.~Ni, J.~Lu, A.~Bakalov, K.~Guu, K.~Hall,
  and M.-W. Chang, ``Promptagator: Few-shot dense retrieval from 8 examples,''
  in \emph{The Eleventh International Conference on Learning Representations},
  2023.

\bibitem{askari-arxiv-2023-generating}
A.~Askari, M.~Aliannejadi, E.~Kanoulas, and S.~Verberne, ``Generating synthetic
  documents for cross-encoder re-rankers: A comparative study of chatgpt and
  human experts,'' \emph{arXiv preprint arXiv:2305.02320}, 2023.

\bibitem{mao-arxiv-2023-large}
K.~Mao, Z.~Dou, H.~Chen, F.~Mo, and H.~Qian, ``Large language models know your
  contextual search intent: A prompting framework for conversational search,''
  \emph{arXiv preprint arXiv:2303.06573}, 2023.

\bibitem{Gao-ACL-2023-precise}
L.~Gao, X.~Ma, J.~Lin, and J.~Callan, ``Precise zero-shot dense retrieval
  without relevance labels,'' in \emph{Proceedings of the 61st Annual Meeting
  of the Association for Computational Linguistics (Volume 1: Long
  Papers)}.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2023, pp. 1762--1777.

\bibitem{Wang-arxiv-2023-query2doc}
L.~Wang, N.~Yang, and F.~Wei, ``Query2doc: Query expansion with large language
  models,'' \emph{arXiv preprint arXiv:2303.07678}, 2023.

\bibitem{ma-arxiv-2023-pre}
G.~Ma, X.~Wu, P.~Wang, Z.~Lin, and S.~Hu, ``Pre-training with large language
  model-based document expansion for dense passage retrieval,'' \emph{arXiv
  preprint arXiv:2308.08285}, 2023.

\bibitem{sun-arxiv-2023-instruction}
W.~Sun, Z.~Chen, X.~Ma, L.~Yan, S.~Wang, P.~Ren, Z.~Chen, D.~Yin, and Z.~Ren,
  ``Instruction distillation makes large language models efficient zero-shot
  rankers,'' \emph{arXiv preprint arXiv:2311.01555}, 2023.

\bibitem{wang-arxiv-2023-large}
X.~Wang, W.~Zhu, and W.~Y. Wang, ``Large language models are implicitly topic
  models: Explaining and finding good demonstrations for in-context learning,''
  \emph{CoRR}, vol. abs/2301.11916, 2023.

\bibitem{Li-arXiv-2023-Multimodal}
C.~Li, Z.~Gan, Z.~Yang, J.~Yang, L.~Li, L.~Wang, and J.~Gao, ``Multimodal
  foundation models: From specialists to general-purpose assistants,''
  \emph{CoRR}, vol. abs/2309.10020, 2023.

\bibitem{zhao-cikm-2021-recbole}
W.~X. Zhao, S.~Mu, Y.~Hou, Z.~Lin, Y.~Chen, X.~Pan, K.~Li, Y.~Lu, H.~Wang,
  C.~Tian, Y.~Min, Z.~Feng, X.~Fan, X.~Chen, P.~Wang, W.~Ji, Y.~Li, X.~Wang,
  and J.~Wen, ``Recbole: Towards a unified, comprehensive and efficient
  framework for recommendation algorithms,'' in \emph{{CIKM}}, G.~Demartini,
  G.~Zuccon, J.~S. Culpepper, Z.~Huang, and H.~Tong, Eds.\hskip 1em plus 0.5em
  minus 0.4em\relax {ACM}, 2021, pp. 4653--4664.

\bibitem{zhou-cikm-2021-s3rec}
K.~Zhou, H.~Wang, W.~X. Zhao, Y.~Zhu, S.~Wang, F.~Zhang, Z.~Wang, and J.~Wen,
  ``S3-rec: Self-supervised learning for sequential recommendation with mutual
  information maximization,'' in \emph{{CIKM}}, M.~d'Aquin, S.~Dietze,
  C.~Hauff, E.~Curry, and P.~Cudr{\'{e}}{-}Mauroux, Eds.\hskip 1em plus 0.5em
  minus 0.4em\relax {ACM}, 2020, pp. 1893--1902.

\bibitem{Zhao-cikm-2022-recbole-2}
W.~X. Zhao, Y.~Hou, X.~Pan, C.~Yang, Z.~Zhang, Z.~Lin, J.~Zhang, S.~Bian,
  J.~Tang, W.~Sun, Y.~Chen, L.~Xu, G.~Zhang, Z.~Tian, C.~Tian, S.~Mu, X.~Fan,
  X.~Chen, and J.~Wen, ``Recbole 2.0: Towards a more up-to-date recommendation
  library,'' in \emph{{CIKM}}, M.~A. Hasan and L.~Xiong, Eds.\hskip 1em plus
  0.5em minus 0.4em\relax {ACM}, 2022, pp. 4722--4726.

\bibitem{Xu-sigir-2023-towards}
L.~Xu, Z.~Tian, G.~Zhang, J.~Zhang, L.~Wang, B.~Zheng, Y.~Li, J.~Tang,
  Z.~Zhang, Y.~Hou, X.~Pan, W.~X. Zhao, X.~Chen, and J.~Wen, ``Towards a more
  user-friendly and easy-to-use benchmark library for recommender systems,'' in
  \emph{{SIGIR}}, H.~Chen, W.~E. Duh, H.~Huang, M.~P. Kato, J.~Mothe, and
  B.~Poblete, Eds.\hskip 1em plus 0.5em minus 0.4em\relax {ACM}, 2023, pp.
  2837--2847.

\bibitem{Rendle-arxiv-2012-bpr}
S.~Rendle, C.~Freudenthaler, Z.~Gantner, and L.~Schmidt{-}Thieme, ``{BPR:}
  bayesian personalized ranking from implicit feedback,'' \emph{CoRR}, vol.
  abs/1205.2618, 2012.

\bibitem{fan-arxiv-2023-recommender}
W.~Fan, Z.~Zhao, J.~Li, Y.~Liu, X.~Mei, Y.~Wang, J.~Tang, and Q.~Li,
  ``Recommender systems in the era of large language models (llms),''
  \emph{{CoRR}}, 2023.

\bibitem{wu-arixv-2023-a}
L.~Wu, Z.~Zheng, Z.~Qiu, H.~Wang, H.~Gu, T.~Shen, C.~Qin, C.~Zhu, H.~Zhu,
  Q.~Liu, H.~Xiong, and E.~Chen, ``A survey on large language models for
  recommendation,'' \emph{{CoRR}}, 2023.

\bibitem{Gao-arxiv-2023-chat-rec}
Y.~Gao, T.~Sheng, Y.~Xiang, Y.~Xiong, H.~Wang, and J.~Zhang, ``Chat-rec:
  Towards interactive and explainable llms-augmented recommender system,''
  \emph{CoRR}, vol. abs/2303.14524, 2023.

\bibitem{dai-recsys-2023-uncovering}
S.~Dai, N.~Shao, H.~Zhao, W.~Yu, Z.~Si, C.~Xu, Z.~Sun, X.~Zhang, and J.~Xu,
  ``Uncovering chatgpt's capabilities in recommender systems,'' in
  \emph{{RecSys}}, J.~Zhang, L.~Chen, S.~Berkovsky, M.~Zhang, T.~D. Noia,
  J.~Basilico, L.~Pizzato, and Y.~Song, Eds.\hskip 1em plus 0.5em minus
  0.4em\relax {ACM}, 2023, pp. 1126--1132.

\bibitem{hou-arxiv-2023-large}
Y.~Hou, J.~Zhang, Z.~Lin, H.~Lu, R.~Xie, J.~J. McAuley, and W.~X. Zhao, ``Large
  language models are zero-shot rankers for recommender systems,''
  \emph{{CoRR}}, 2023.

\bibitem{Liu-arxiv-2023-is}
J.~Liu, C.~Liu, R.~Lv, K.~Zhou, and Y.~Zhang, ``Is chatgpt a good recommender?
  {A} preliminary study,'' \emph{CoRR}, vol. abs/2304.10149, 2023.

\bibitem{bao-recsys-2023-tallrec}
K.~Bao, J.~Zhang, Y.~Zhang, W.~Wang, F.~Feng, and X.~He, ``Tallrec: An
  effective and efficient tuning framework to align large language model with
  recommendation,'' in \emph{{RecSys}}, J.~Zhang, L.~Chen, S.~Berkovsky,
  M.~Zhang, T.~D. Noia, J.~Basilico, L.~Pizzato, and Y.~Song, Eds.\hskip 1em
  plus 0.5em minus 0.4em\relax {ACM}, 2023, pp. 1007--1014.

\bibitem{Zhu-arxiv-2023-Collaborative}
Y.~Zhu, L.~Wu, Q.~Guo, L.~Hong, and J.~Li, ``Collaborative large language model
  for recommender systems,'' \emph{arXiv preprint arXiv:2311.01343}, 2023.

\bibitem{Zheng-2023-arxiv-Adapting}
\BIBentryALTinterwordspacing
B.~Zheng, Y.~Hou, H.~Lu, Y.~Chen, W.~X. Zhao, and J.-R. Wen, ``Adapting large
  language models by integrating collaborative semantics for recommendation,''
  2023. [Online]. Available:
  \url{https://api.semanticscholar.org/CorpusID:265213194}
\BIBentrySTDinterwordspacing

\bibitem{xi-arxiv-2023-towards}
Y.~Xi, W.~Liu, J.~Lin, J.~Zhu, B.~Chen, R.~Tang, W.~Zhang, R.~Zhang, and Y.~Yu,
  ``Towards open-world recommendation with knowledge augmentation from large
  language models,'' \emph{{CoRR}}, vol. abs/2306.10933, 2023.

\bibitem{liu-arxiv-2023-a}
Q.~Liu, N.~Chen, T.~Sakai, and X.~Wu, ``A first look at llm-powered generative
  news recommendation,'' \emph{CoRR}, vol. abs/2305.06566, 2023.

\bibitem{li-arxiv-2023-exploring}
R.~Li, W.~Deng, Y.~Cheng, Z.~Yuan, J.~Zhang, and F.~Yuan, ``Exploring the upper
  limits of text-based collaborative filtering using large language models:
  Discoveries and insights,'' \emph{CoRR}, vol. abs/2305.11700, 2023.

\bibitem{Wei-arixiv-2023-llmrec}
W.~Wei, X.~Ren, J.~Tang, Q.~Wang, L.~Su, S.~Cheng, J.~Wang, D.~Yin, and
  C.~Huang, ``Llmrec: Large language models with graph augmentation for
  recommendation,'' \emph{CoRR}, vol. abs/2311.00423, 2023.

\bibitem{Li-arxiv-2023-ctrl}
X.~Li, B.~Chen, L.~Hou, and R.~Tang, ``Ctrl: Connect tabular and language model
  for ctr prediction,'' \emph{arXiv preprint arXiv:2306.02841}, 2023.

\bibitem{Muhamed-nips-2021-ctr-bert}
A.~Muhamed, I.~Keivanloo, S.~Perera, J.~Mracek, Y.~Xu, Q.~Cui, S.~Rajagopalan,
  B.~Zeng, and T.~Chilimbi, ``Ctr-bert: Cost-effective knowledge distillation
  for billion-parameter teacher models,'' in \emph{NeurIPS Efficient Natural
  Language and Speech Processing Workshop}, 2021.

\bibitem{wang-arxiv-2023-a}
L.~Wang, C.~Ma, X.~Feng, Z.~Zhang, H.~Yang, J.~Zhang, Z.~Chen, J.~Tang,
  X.~Chen, Y.~Lin, W.~X. Zhao, Z.~Wei, and J.~Wen, ``A survey on large language
  model based autonomous agents,'' \emph{CoRR}, vol. abs/2308.11432, 2023.

\bibitem{Wang-arxiv-2023-RecAgent}
L.~Wang, J.~Zhang, X.~Chen, Y.~Lin, R.~Song, W.~X. Zhao, and J.~Wen,
  ``Recagent: {A} novel simulation paradigm for recommender systems,''
  \emph{CoRR}, vol. abs/2306.02552, 2023.

\bibitem{Ie-arxiv-2019-recsim}
E.~Ie, C.~Hsu, M.~Mladenov, V.~Jain, S.~Narvekar, J.~Wang, R.~Wu, and
  C.~Boutilier, ``Recsim: {A} configurable simulation platform for recommender
  systems,'' \emph{CoRR}, vol. abs/1909.04847, 2019.

\bibitem{Zhang-arxiv-2023-AgentCF}
J.~Zhang, Y.~Hou, R.~Xie, W.~Sun, J.~J. McAuley, W.~X. Zhao, L.~Lin, and
  J.~Wen, ``Agentcf: Collaborative learning with autonomous language agents for
  recommender systems,'' \emph{CoRR}, vol. abs/2310.09233, 2023.

\bibitem{zhang-arxiv-2023-on}
A.~Zhang, L.~Sheng, Y.~Chen, H.~Li, Y.~Deng, X.~Wang, and T.~Chua, ``On
  generative agents in recommendation,'' \emph{CoRR}, vol. abs/2310.10108,
  2023.

\bibitem{Du-arxiv-2023-survey}
Y.~Du, Z.~Liu, J.~Li, and W.~X. Zhao, ``A survey of vision-language pre-trained
  models,'' in \emph{Proceedings of the Thirty-First International Joint
  Conference on Artificial Intelligence, {IJCAI} 2022, Vienna, Austria, 23-29
  July 2022}, L.~D. Raedt, Ed.\hskip 1em plus 0.5em minus 0.4em\relax
  ijcai.org, 2022, pp. 5436--5443.

\bibitem{Gan-Foundation-2022-vision}
Z.~Gan, L.~Li, C.~Li, L.~Wang, Z.~Liu, and J.~Gao, ``Vision-language
  pre-training: Basics, recent advances, and future trends,'' \emph{Found.
  Trends Comput. Graph. Vis.}, vol.~14, no. 3-4, pp. 163--352, 2022.

\bibitem{Rubenstein-2023-arxiv-audiopalm}
P.~K. Rubenstein, C.~Asawaroengchai, D.~D. Nguyen, A.~Bapna, Z.~Borsos,
  F.~de~Chaumont~Quitry, P.~Chen, D.~E. Badawy, W.~Han, E.~Kharitonov
  \emph{et~al.}, ``Audiopalm: {A} large language model that can speak and
  listen,'' \emph{CoRR}, 2023.

\bibitem{Alayrac-nips-2022-flamingo}
J.~Alayrac, J.~Donahue, P.~Luc, A.~Miech, I.~Barr, Y.~Hasson, K.~Lenc,
  A.~Mensch, K.~Millican, M.~Reynolds, R.~Ring, E.~Rutherford, S.~Cabi, T.~Han,
  Z.~Gong, S.~Samangooei, M.~Monteiro, J.~L. Menick, S.~Borgeaud, A.~Brock,
  A.~Nematzadeh, S.~Sharifzadeh, M.~Binkowski, R.~Barreira, O.~Vinyals,
  A.~Zisserman, and K.~Simonyan, ``Flamingo: a visual language model for
  few-shot learning,'' in \emph{NeurIPS}, 2022.

\bibitem{Schuhmann-nips-2022-laion5b}
C.~Schuhmann, R.~Beaumont, R.~Vencu, C.~Gordon, R.~Wightman, M.~Cherti,
  T.~Coombes, A.~Katta, C.~Mullis, M.~Wortsman, P.~Schramowski, S.~Kundurthy,
  K.~Crowson, L.~Schmidt, R.~Kaczmarczyk, and J.~Jitsev, ``{LAION-5B:} an open
  large-scale dataset for training next generation image-text models,'' in
  \emph{NeurIPS}, 2022.

\bibitem{Changpinyo-cvpr-2023-conceptual}
S.~Changpinyo, P.~Sharma, N.~Ding, and R.~Soricut, ``Conceptual 12m: Pushing
  web-scale image-text pre-training to recognize long-tail visual concepts,''
  in \emph{{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
  2021, virtual, June 19-25, 2021}.\hskip 1em plus 0.5em minus 0.4em\relax
  Computer Vision Foundation / {IEEE}, 2021, pp. 3558--3568.

\bibitem{Ye-arxiv-2023-mplug}
Q.~Ye, H.~Xu, G.~Xu, J.~Ye, M.~Yan, Y.~Zhou, J.~Wang, A.~Hu, P.~Shi, Y.~Shi,
  C.~Li, Y.~Xu, H.~Chen, J.~Tian, Q.~Qi, J.~Zhang, and F.~Huang, ``mplug-owl:
  Modularization empowers large language models with multimodality,''
  \emph{CoRR}, vol. abs/2304.14178, 2023.

\bibitem{Bai-arxiv-2023-qwen}
J.~Bai, S.~Bai, S.~Yang, S.~Wang, S.~Tan, P.~Wang, J.~Lin, C.~Zhou, and
  J.~Zhou, ``Qwen-vl: {A} frontier large vision-language model with versatile
  abilities,'' \emph{CoRR}, vol. abs/2308.12966, 2023.

\bibitem{Liu-arxiv-2023-improved}
H.~Liu, C.~Li, Y.~Li, and Y.~J. Lee, ``Improved baselines with visual
  instruction tuning,'' \emph{CoRR}, vol. abs/2310.03744, 2023.

\bibitem{Zhang-arxiv-2023-internlm}
P.~Zhang, X.~Dong, B.~Wang, Y.~Cao, C.~Xu, L.~Ouyang, Z.~Zhao, S.~Ding,
  S.~Zhang, H.~Duan, W.~Zhang, H.~Yan, X.~Zhang, W.~Li, J.~Li, K.~Chen, C.~He,
  X.~Zhang, Y.~Qiao, D.~Lin, and J.~Wang, ``Internlm-xcomposer: {A}
  vision-language large model for advanced text-image comprehension and
  composition,'' \emph{CoRR}, vol. abs/2309.15112, 2023.

\bibitem{Chen-arxiv-2023-shikra}
K.~Chen, Z.~Zhang, W.~Zeng, R.~Zhang, F.~Zhu, and R.~Zhao, ``Shikra: Unleashing
  multimodal llm's referential dialogue magic,'' \emph{CoRR}, vol.
  abs/2306.15195, 2023.

\bibitem{liu-arxiv-2023-aligning}
F.~Liu, K.~Lin, L.~Li, J.~Wang, Y.~Yacoob, and L.~Wang, ``Aligning large
  multi-modal model with robust instruction tuning,'' \emph{CoRR}, vol.
  abs/2306.14565, 2023.

\bibitem{Du-arxiv-2023-What}
Y.~Du, H.~Guo, K.~Zhou, W.~X. Zhao, J.~Wang, C.~Wang, M.~Cai, R.~Song, and
  J.-R. Wen, ``What makes for good visual instructions? synthesizing complex
  visual reasoning instructions for visual instruction tuning,'' 2023.

\bibitem{Gurari-cvpr-2018-vizwiz}
D.~Gurari, Q.~Li, A.~J. Stangl, A.~Guo, C.~Lin, K.~Grauman, J.~Luo, and J.~P.
  Bigham, ``Vizwiz grand challenge: Answering visual questions from blind
  people,'' in \emph{{CVPR}}.\hskip 1em plus 0.5em minus 0.4em\relax Computer
  Vision Foundation / {IEEE} Computer Society, 2018, pp. 3608--3617.

\bibitem{Mishra-cvpr-2012-top}
A.~Mishra, K.~Alahari, and C.~V. Jawahar, ``Top-down and bottom-up cues for
  scene text recognition,'' in \emph{{CVPR}}.\hskip 1em plus 0.5em minus
  0.4em\relax {IEEE} Computer Society, 2012, pp. 2687--2694.

\bibitem{Liu-arxiv-2023-mmbench}
Y.~Liu, H.~Duan, Y.~Zhang, B.~Li, S.~Zhang, W.~Zhao, Y.~Yuan, J.~Wang, C.~He,
  Z.~Liu, K.~Chen, and D.~Lin, ``Mmbench: Is your multi-modal model an
  all-around player?'' \emph{CoRR}, vol. abs/2307.06281, 2023.

\bibitem{Fu-arxiv-2023-mme}
C.~Fu, P.~Chen, Y.~Shen, Y.~Qin, M.~Zhang, X.~Lin, Z.~Qiu, W.~Lin, J.~Yang,
  X.~Zheng, K.~Li, X.~Sun, and R.~Ji, ``{MME:} {A} comprehensive evaluation
  benchmark for multimodal large language models,'' \emph{CoRR}, vol.
  abs/2306.13394, 2023.

\bibitem{Zhang-arxiv-2023-siren}
Y.~Zhang, Y.~Li, L.~Cui, D.~Cai, L.~Liu, T.~Fu, X.~Huang, E.~Zhao, Y.~Zhang,
  Y.~Chen, L.~Wang, A.~T. Luu, W.~Bi, F.~Shi, and S.~Shi, ``Siren's song in the
  {AI} ocean: {A} survey on hallucination in large language models,''
  \emph{CoRR}, vol. abs/2309.01219, 2023.

\bibitem{Gunjal-arxiv-2023-detecting}
A.~Gunjal, J.~Yin, and E.~Bas, ``Detecting and preventing hallucinations in
  large vision language models,'' \emph{CoRR}, vol. abs/2308.06394, 2023.

\bibitem{Lu-arxiv-2023-evaluation}
J.~Lu, J.~Rao, K.~Chen, X.~Guo, Y.~Zhang, B.~Sun, C.~Yang, and J.~Yang,
  ``Evaluation and mitigation of agnosia in multimodal large language models,''
  \emph{CoRR}, vol. abs/2309.04041, 2023.

\bibitem{Rohrbach-emnlp-2018-object}
A.~Rohrbach, L.~A. Hendricks, K.~Burns, T.~Darrell, and K.~Saenko, ``Object
  hallucination in image captioning,'' in \emph{{EMNLP}}.\hskip 1em plus 0.5em
  minus 0.4em\relax Association for Computational Linguistics, 2018, pp.
  4035--4045.

\bibitem{Li-emnlp-2023-evaluating}
\BIBentryALTinterwordspacing
Y.~Li, Y.~Du, K.~Zhou, J.~Wang, W.~X. Zhao, and J.-R. Wen, ``Evaluating object
  hallucination in large vision-language models,'' in \emph{The 2023 Conference
  on Empirical Methods in Natural Language Processing}, 2023. [Online].
  Available: \url{https://openreview.net/forum?id=xozJw0kZXF}
\BIBentrySTDinterwordspacing

\bibitem{Hudson-cvpr-2019-gqa}
D.~A. Hudson and C.~D. Manning, ``{GQA:} {A} new dataset for real-world visual
  reasoning and compositional question answering,'' in \emph{{CVPR}}.\hskip 1em
  plus 0.5em minus 0.4em\relax Computer Vision Foundation / {IEEE}, 2019, pp.
  6700--6709.

\bibitem{Lu-nips-2022-learn}
P.~Lu, S.~Mishra, T.~Xia, L.~Qiu, K.~Chang, S.~Zhu, O.~Tafjord, P.~Clark, and
  A.~Kalyan, ``Learn to explain: Multimodal reasoning via thought chains for
  science question answering,'' in \emph{NeurIPS}, 2022.

\bibitem{Amanpreet-cvpr-2019-textvqa}
A.~Singh, V.~Natarjan, M.~Shah, Y.~Jiang, X.~Chen, D.~Parikh, and M.~Rohrbach,
  ``Towards vqa models that can read,'' in \emph{Proceedings of the IEEE
  Conference on Computer Vision and Pattern Recognition}, 2019, pp. 8317--8326.

\bibitem{Liu-arxiv-2023-hallusionbench}
F.~Liu, T.~Guan, Z.~Li, L.~Chen, Y.~Yacoob, D.~Manocha, and T.~Zhou,
  ``Hallusionbench: You see what you think? or you think what you see? an
  image-context reasoning benchmark challenging for gpt-4v(ision), llava-1.5,
  and other multi-modality models,'' \emph{CoRR}, vol. abs/2310.14566, 2023.

\bibitem{Antol-iccv-2015-vqa}
S.~Antol, A.~Agrawal, J.~Lu, M.~Mitchell, D.~Batra, C.~L. Zitnick, and
  D.~Parikh, ``{VQA:} visual question answering,'' in \emph{{ICCV}}.\hskip 1em
  plus 0.5em minus 0.4em\relax {IEEE} Computer Society, 2015, pp. 2425--2433.

\bibitem{Vedantam-cvpr-2015-cider}
R.~Vedantam, C.~L. Zitnick, and D.~Parikh, ``Cider: Consensus-based image
  description evaluation,'' in \emph{{CVPR}}.\hskip 1em plus 0.5em minus
  0.4em\relax {IEEE} Computer Society, 2015, pp. 4566--4575.

\bibitem{Liu-2023-arxiv-Visual}
H.~Liu, C.~Li, Q.~Wu, and Y.~J. Lee, ``Visual instruction tuning,''
  \emph{CoRR}, vol. abs/2304.08485, 2023.

\bibitem{Xu-arxiv-2023-lvlm}
P.~Xu, W.~Shao, K.~Zhang, P.~Gao, S.~Liu, M.~Lei, F.~Meng, S.~Huang, Y.~Qiao,
  and P.~Luo, ``Lvlm-ehub: {A} comprehensive evaluation benchmark for large
  vision-language models,'' \emph{CoRR}, vol. abs/2306.09265, 2023.

\bibitem{Li-arxiv-2023-reform}
Z.~Li, Y.~Wang, M.~Du, Q.~Liu, B.~Wu, J.~Zhang, C.~Zhou, Z.~Fan, J.~Fu,
  J.~Chen, X.~Huang, and Z.~Wei, ``Reform-eval: Evaluating large vision
  language models via unified re-formulation of task-oriented benchmarks,''
  \emph{CoRR}, vol. abs/2310.02569, 2023.

\bibitem{Li-arxiv-2023-seed}
B.~Li, R.~Wang, G.~Wang, Y.~Ge, Y.~Ge, and Y.~Shan, ``Seed-bench: Benchmarking
  multimodal llms with generative comprehension,'' \emph{CoRR}, vol.
  abs/2307.16125, 2023.

\bibitem{Yu-arxiv-2023-mmvet}
W.~Yu, Z.~Yang, L.~Li, J.~Wang, K.~Lin, Z.~Liu, X.~Wang, and L.~Wang, ``Mm-vet:
  Evaluating large multimodal models for integrated capabilities,''
  \emph{CoRR}, vol. abs/2308.02490, 2023.

\bibitem{Wang-arxiv-2023-To}
J.~Wang, L.~Meng, Z.~Weng, B.~He, Z.~Wu, and Y.~Jiang, ``To see is to believe:
  Prompting {GPT-4V} for better visual instruction tuning,'' \emph{CoRR}, vol.
  abs/2311.07574, 2023.

\bibitem{Zhang-arxiv-2023-LLaVAR}
Y.~Zhang, R.~Zhang, J.~Gu, Y.~Zhou, N.~Lipka, D.~Yang, and T.~Sun, ``Llavar:
  Enhanced visual instruction tuning for text-rich image understanding,''
  \emph{arXiv preprint arXiv:2306.17107}, 2023.

\bibitem{Qi-2023-NAML-Visual}
X.~Qi, K.~Huang, A.~Panda, M.~Wang, and P.~Mittal, ``Visual adversarial
  examples jailbreak aligned large language models,'' in \emph{The Second
  Workshop on New Frontiers in Adversarial Machine Learning}, 2023.

\bibitem{Zhou-arxiv-2023-analyzing}
Y.~Zhou, C.~Cui, J.~Yoon, L.~Zhang, Z.~Deng, C.~Finn, M.~Bansal, and H.~Yao,
  ``Analyzing and mitigating object hallucination in large vision-language
  models,'' \emph{arXiv preprint arXiv:2310.00754}, 2023.

\bibitem{Sun-arxiv-2023-Aligning}
Z.~Sun, S.~Shen, S.~Cao, H.~Liu, C.~Li, Y.~Shen, C.~Gan, L.-Y. Gui, Y.-X. Wang,
  Y.~Yang \emph{et~al.}, ``Aligning large multimodal models with factually
  augmented rlhf,'' \emph{arXiv preprint arXiv:2309.14525}, 2023.

\bibitem{Pan-arxiv-2023-Unifying}
S.~Pan, L.~Luo, Y.~Wang, C.~Chen, J.~Wang, and X.~Wu, ``Unifying large language
  models and knowledge graphs: {A} roadmap,'' \emph{CoRR}, vol. abs/2306.08302,
  2023.

\bibitem{Ruiz-arxiv-2023-SemTab}
E.~Jim{\'{e}}nez{-}Ruiz, O.~Hassanzadeh, V.~Efthymiou, J.~Chen, and
  K.~Srinivas, ``Semtab 2019: Resources to benchmark tabular data to knowledge
  graph matching systems,'' in \emph{The Semantic Web - 17th International
  Conference, {ESWC} 2020, Heraklion, Crete, Greece, May 31-June 4, 2020,
  Proceedings}, ser. Lecture Notes in Computer Science, vol. 12123.\hskip 1em
  plus 0.5em minus 0.4em\relax Springer, 2020, pp. 514--530.

\bibitem{ERNIE3}
\BIBentryALTinterwordspacing
Y.~Sun, S.~Wang, S.~Feng, S.~Ding, C.~Pang, J.~Shang, J.~Liu, X.~Chen, Y.~Zhao,
  Y.~Lu, W.~Liu, Z.~Wu, W.~Gong, J.~Liang, Z.~Shang, P.~Sun, W.~Liu, X.~Ouyang,
  D.~Yu, H.~Tian, H.~Wu, and H.~Wang, ``{ERNIE} 3.0: Large-scale knowledge
  enhanced pre-training for language understanding and generation,''
  \emph{CoRR}, vol. abs/2107.02137, 2021. [Online]. Available:
  \url{https://arxiv.org/abs/2107.02137}
\BIBentrySTDinterwordspacing

\bibitem{Zhang-ACL-19-ERNIE}
Z.~Zhang, X.~Han, Z.~Liu, X.~Jiang, M.~Sun, and Q.~Liu, ``{ERNIE:} enhanced
  language representation with informative entities,'' in \emph{Proceedings of
  the 57th Conference of the Association for Computational Linguistics, {ACL}
  2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers}.\hskip
  1em plus 0.5em minus 0.4em\relax Association for Computational Linguistics,
  2019, pp. 1441--1451.

\bibitem{Wang-TACL-21-KEPLER}
X.~Wang, T.~Gao, Z.~Zhu, Z.~Zhang, Z.~Liu, J.~Li, and J.~Tang, ``{KEPLER:} {A}
  unified model for knowledge embedding and pre-trained language
  representation,'' \emph{Trans. Assoc. Comput. Linguistics}, vol.~9, pp.
  176--194, 2021.

\bibitem{Zhang-ACL-2022-Subgraph}
J.~Zhang, X.~Zhang, J.~Yu, J.~Tang, J.~Tang, C.~Li, and H.~Chen, ``Subgraph
  retrieval enhanced model for multi-hop knowledge base question answering,''
  in \emph{Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin,
  Ireland, May 22-27, 2022}.\hskip 1em plus 0.5em minus 0.4em\relax Association
  for Computational Linguistics, 2022, pp. 5773--5784.

\bibitem{Ke-ACL-21-JointGT}
P.~Ke, H.~Ji, Y.~Ran, X.~Cui, L.~Wang, L.~Song, X.~Zhu, and M.~Huang,
  ``Jointgt: Graph-text joint representation learning for text generation from
  knowledge graphs,'' in \emph{Findings of the Association for Computational
  Linguistics: {ACL/IJCNLP} 2021, Online Event, August 1-6, 2021}, ser.
  Findings of {ACL}, vol. {ACL/IJCNLP} 2021.\hskip 1em plus 0.5em minus
  0.4em\relax Association for Computational Linguistics, 2021, pp. 2526--2538.

\bibitem{Agarwal-arxiv-2020-Large}
O.~Agarwal, H.~Ge, S.~Shakeri, and R.~Al{-}Rfou, ``Large scale knowledge graph
  based synthetic corpus generation for knowledge-enhanced language model
  pre-training,'' \emph{CoRR}, vol. abs/2010.12688, 2020.

\bibitem{Chen-EMNLP-2020-KGPT}
W.~Chen, Y.~Su, X.~Yan, and W.~Y. Wang, ``{KGPT:} knowledge-grounded
  pre-training for data-to-text generation,'' in \emph{Proceedings of the 2020
  Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2020,
  Online, November 16-20, 2020}.\hskip 1em plus 0.5em minus 0.4em\relax
  Association for Computational Linguistics, 2020, pp. 8635--8648.

\bibitem{Gu-ACL-23-Pangu}
Y.~Gu, X.~Deng, and Y.~Su, ``Don't generate, discriminate: {A} proposal for
  grounding language models to real-world environments,'' in \emph{Proceedings
  of the 61st Annual Meeting of the Association for Computational Linguistics
  (Volume 1: Long Papers), {ACL} 2023, Toronto, Canada, July 9-14, 2023}.\hskip
  1em plus 0.5em minus 0.4em\relax Association for Computational Linguistics,
  2023, pp. 4928--4949.

\bibitem{Luo-arxiv-23-Reasoning}
L.~Luo, Y.~Li, G.~Haffari, and S.~Pan, ``Reasoning on graphs: Faithful and
  interpretable large language model reasoning,'' \emph{CoRR}, vol.
  abs/2310.01061, 2023.

\bibitem{Lan-2020-ACL-Query}
Y.~Lan and J.~Jiang, ``Query graph generation for answering multi-hop complex
  questions from knowledge bases,'' in \emph{Proceedings of the 58th Annual
  Meeting of the Association for Computational Linguistics, {ACL} 2020, Online,
  July 5-10, 2020}, D.~J. and, Ed.\hskip 1em plus 0.5em minus 0.4em\relax
  Association for Computational Linguistics, 2020, pp. 969--974.

\bibitem{Wang-arxiv-23-easyedit}
P.~Wang, N.~Zhang, X.~Xie, Y.~Yao, B.~Tian, M.~Wang, Z.~Xi, S.~Cheng, K.~Liu,
  G.~Zheng, and H.~Chen, ``Easyedit: An easy-to-use knowledge editing framework
  for large language models,'' \emph{CoRR}, vol. abs/2308.07269, 2023.

\bibitem{Yao-arxiv-23-editing}
Y.~Yao, P.~Wang, B.~Tian, S.~Cheng, Z.~Li, S.~Deng, H.~Chen, and N.~Zhang,
  ``Editing large language models: Problems, methods, and opportunities,''
  \emph{CoRR}, vol. abs/2305.13172, 2023.

\bibitem{Choi-arxiv-23-KCTs}
S.~Choi, T.~Fang, Z.~Wang, and Y.~Song, ``{KCTS:} knowledge-constrained tree
  search decoding with token-level hallucination detection,'' \emph{CoRR}, vol.
  abs/2310.09044, 2023.

\bibitem{Zhang-arxiv-23-Mitigating}
S.~Zhang, L.~Pan, J.~Zhao, and W.~Y. Wang, ``Mitigating language model
  hallucination with interactive question-knowledge alignment,'' \emph{CoRR},
  vol. abs/2305.13669, 2023.

\bibitem{Zhu-arxiv-23-LLMs}
\BIBentryALTinterwordspacing
Y.~Zhu, X.~Wang, J.~Chen, S.~Qiao, Y.~Ou, Y.~Yao, S.~Deng, H.~Chen, and
  N.~Zhang, ``Llms for knowledge graph construction and reasoning: Recent
  capabilities and future opportunities,'' \emph{CoRR}, vol. abs/2305.13168,
  2023. [Online]. Available: \url{https://doi.org/10.48550/arXiv.2305.13168}
\BIBentrySTDinterwordspacing

\bibitem{Russell-Pearson-2020-Artificial}
\BIBentryALTinterwordspacing
S.~Russell and P.~Norvig, \emph{Artificial Intelligence: {A} Modern Approach
  (4th Edition)}.\hskip 1em plus 0.5em minus 0.4em\relax Pearson, 2020.
  [Online]. Available: \url{http://aima.cs.berkeley.edu/}
\BIBentrySTDinterwordspacing

\bibitem{Lake-arxiv-2016-Building}
B.~M. Lake, T.~D. Ullman, J.~B. Tenenbaum, and S.~J. Gershman, ``Building
  machines that learn and think like people,'' \emph{CoRR}, vol.
  abs/1604.00289, 2016.

\bibitem{Yao-arxiv-2022-ReAct}
S.~Yao, J.~Zhao, D.~Yu, N.~Du, I.~Shafran, K.~Narasimhan, and Y.~Cao, ``React:
  Synergizing reasoning and acting in language models,'' \emph{CoRR}, vol.
  abs/2210.03629, 2022.

\bibitem{GPT-Engineer}
\BIBentryALTinterwordspacing
2023. [Online]. Available: \url{https://github.com/AntonOsika/gpt-engineer}
\BIBentrySTDinterwordspacing

\bibitem{xagent2023}
X.~Team, ``Xagent: An autonomous agent for complex task solving,'' 2023.

\bibitem{Li-arxiv-2023-CAMEL}
G.~Li, H.~A. A.~K. Hammoud, H.~Itani, D.~Khizbullin, and B.~Ghanem, ``{CAMEL:}
  communicative agents for "mind" exploration of large scale language model
  society,'' \emph{CoRR}, vol. abs/2303.17760, 2023.

\bibitem{Hone-arxiv-2023-MetaGPT}
S.~Hong, X.~Zheng, J.~Chen, Y.~Cheng, J.~Wang, C.~Zhang, Z.~Wang, S.~K.~S. Yau,
  Z.~Lin, L.~Zhou, C.~Ran, L.~Xiao, and C.~Wu, ``Metagpt: Meta programming for
  multi-agent collaborative framework,'' \emph{CoRR}, vol. abs/2308.00352,
  2023.

\bibitem{Pham-arxiv-2023-Let}
C.~Pham, B.~Liu, Y.~Yang, Z.~Chen, T.~Liu, J.~Yuan, B.~A. Plummer, Z.~Wang, and
  H.~Yang, ``Let models speak ciphers: Multiagent debate through embeddings,''
  \emph{CoRR}, vol. abs/2310.06272, 2023.

\bibitem{Du-arxiv-2023-Improving}
Y.~Du, S.~Li, A.~Torralba, J.~B. Tenenbaum, and I.~Mordatch, ``Improving
  factuality and reasoning in language models through multiagent debate,''
  \emph{CoRR}, vol. abs/2305.14325, 2023.

\bibitem{Karpinska-arxiv-23-The}
M.~Karpinska, N.~Akoury, and M.~Iyyer, ``The perils of using mechanical turk to
  evaluate open-ended text generation,'' in \emph{Proceedings of the 2021
  Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2021,
  Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021},
  M.~Moens, X.~Huang, L.~Specia, and S.~W. Yih, Eds.\hskip 1em plus 0.5em minus
  0.4em\relax Association for Computational Linguistics, 2021, pp. 1265--1285.

\bibitem{Lee-23-arxiv-RLAIF}
H.~Lee, S.~Phatale, H.~Mansoor, K.~Lu, T.~Mesnard, C.~Bishop, V.~Carbune, and
  A.~Rastogi, ``{RLAIF:} scaling reinforcement learning from human feedback
  with {AI} feedback,'' \emph{CoRR}, vol. abs/2309.00267, 2023.

\bibitem{Wang-23-arxiv-Shepherd}
T.~Wang, P.~Yu, X.~E. Tan, S.~O'Brien, R.~Pasunuru, J.~Dwivedi{-}Yu,
  O.~Golovneva, L.~Zettlemoyer, M.~Fazel{-}Zarandi, and A.~Celikyilmaz,
  ``Shepherd: {A} critic for language model generation,'' \emph{CoRR}, vol.
  abs/2308.04592, 2023.

\bibitem{Cui-23-arxiv-UltraFeedback}
G.~Cui, L.~Yuan, N.~Ding, G.~Yao, W.~Zhu, Y.~Ni, G.~Xie, Z.~Liu, and M.~Sun,
  ``Ultrafeedback: Boosting language models with high-quality feedback,''
  \emph{CoRR}, vol. abs/2310.01377, 2023.

\bibitem{Wang-23-arxiv-MINT}
X.~Wang, Z.~Wang, J.~Liu, Y.~Chen, L.~Yuan, H.~Peng, and H.~Ji, ``{MINT:}
  evaluating llms in multi-turn interaction with tools and language feedback,''
  \emph{CoRR}, vol. abs/2309.10691, 2023.

\bibitem{Saha-23-arxiv-Branch}
S.~Saha, O.~Levy, A.~Celikyilmaz, M.~Bansal, J.~Weston, and X.~Li,
  ``Branch-solve-merge improves large language model evaluation and
  generation,'' \emph{CoRR}, vol. abs/2310.15123, 2023.

\bibitem{Zhang-23-arxiv-Wider}
X.~Zhang, B.~Yu, H.~Yu, Y.~Lv, T.~Liu, F.~Huang, H.~Xu, and Y.~Li, ``Wider and
  deeper {LLM} networks are fairer {LLM} evaluators,'' \emph{CoRR}, vol.
  abs/2308.01862, 2023.

\bibitem{Chan-23-arxiv-ChatEval}
C.~Chan, W.~Chen, Y.~Su, J.~Yu, W.~Xue, S.~Zhang, J.~Fu, and Z.~Liu,
  ``Chateval: Towards better llm-based evaluators through multi-agent debate,''
  \emph{CoRR}, vol. abs/2308.07201, 2023.

\bibitem{Li-23-arxiv-PRD}
R.~Li, T.~Patel, and X.~Du, ``{PRD:} peer rank and discussion improve large
  language model based evaluations,'' \emph{CoRR}, vol. abs/2307.02762, 2023.

\bibitem{Zhu-23-arxiv-Judge}
L.~Zhu, X.~Wang, and X.~Wang, ``Judgelm: Fine-tuned large language models are
  scalable judges,'' \emph{CoRR}, vol. abs/2310.17631, 2023.

\bibitem{Zeng-23-arxiv-Evaluating}
Z.~Zeng, J.~Yu, T.~Gao, Y.~Meng, T.~Goyal, and D.~Chen, ``Evaluating large
  language models at evaluating instruction following,'' \emph{CoRR}, vol.
  abs/2310.07641, 2023.

\bibitem{Koo-23-arxiv-Benchmarking}
R.~Koo, M.~Lee, V.~Raheja, J.~I. Park, Z.~M. Kim, and D.~Kang, ``Benchmarking
  cognitive biases in large language models as evaluators,'' \emph{CoRR}, vol.
  abs/2309.17012, 2023.

\bibitem{West-23-arxiv-The}
P.~West, X.~Lu, N.~Dziri, F.~Brahman, L.~Li, J.~D. Hwang, L.~Jiang, J.~Fisher,
  A.~Ravichander, K.~Chandu, B.~Newman, P.~W. Koh, A.~Ettinger, and Y.~Choi,
  ``The generative {AI} paradox: "what it can create, it may not understand",''
  \emph{CoRR}, vol. abs/2311.00059, 2023.

\bibitem{Huang-23-arixv-Large}
J.~Huang, X.~Chen, S.~Mishra, H.~S. Zheng, A.~W. Yu, X.~Song, and D.~Zhou,
  ``Large language models cannot self-correct reasoning yet,'' \emph{CoRR},
  vol. abs/2310.01798, 2023.

\bibitem{Stechly-23-arxiv-GPT4}
K.~Stechly, M.~Marquez, and S.~Kambhampati, ``{GPT-4} doesn't know it's wrong:
  An analysis of iterative prompting for reasoning problems,'' \emph{CoRR},
  vol. abs/2310.12397, 2023.

\bibitem{Nov-arxiv-2023-Medical}
O.~Nov, N.~Singh, and D.~M. Mann, ``Putting chatgpt's medical advice to the
  (turing) test,'' \emph{CoRR}, vol. abs/2301.10035, 2023.

\bibitem{Yang-arxiv-2023-mental}
K.~Yang, S.~Ji, T.~Zhang, Q.~Xie, and S.~Ananiadou, ``On the evaluations of
  chatgpt and emotion-enhanced prompting for mental health analysis,''
  \emph{CoRR}, vol. abs/2304.03347, 2023.

\bibitem{Jeblick-arxiv-2023-Medicine}
K.~Jeblick, B.~Schachtner, J.~Dexl, A.~Mittermeier, A.~T. St{\"{u}}ber,
  J.~Topalis, T.~Weber, P.~Wesp, B.~O. Sabel, J.~Ricke, and M.~Ingrisch,
  ``Chatgpt makes medicine easy to swallow: An exploratory case study on
  simplified radiology reports,'' \emph{CoRR}, vol. abs/2212.14882, 2022.

\bibitem{Singhal-2023-arxiv-Towards}
K.~Singhal, T.~Tu, J.~Gottweis, R.~Sayres, E.~Wulczyn, L.~Hou, K.~Clark,
  S.~Pfohl, H.~Cole{-}Lewis, D.~Neal, M.~Schaekermann, A.~Wang, M.~Amin,
  S.~Lachgar, P.~A. Mansfield, S.~Prakash, B.~Green, E.~Dominowska, B.~A.
  y~Arcas, N.~Tomasev, Y.~Liu, R.~Wong, C.~Semturs, S.~S. Mahdavi, J.~K.
  Barral, D.~R. Webster, G.~S. Corrado, Y.~Matias, S.~Azizi,
  A.~Karthikesalingam, and V.~Natarajan, ``Towards expert-level medical
  question answering with large language models,'' \emph{CoRR}, vol.
  abs/2305.09617, 2023.

\bibitem{Yang-arxiv-2023-zhongjing}
S.~Yang, H.~Zhao, S.~Zhu, G.~Zhou, H.~Xu, Y.~Jia, and H.~Zan, ``Zhongjing:
  Enhancing the chinese medical capabilities of large language model through
  expert feedback and real-world multi-turn dialogue,'' \emph{CoRR}, vol.
  abs/2308.03549, 2023.

\bibitem{Chen-medrxiv-2023-cancer}
S.~Chen, B.~H. Kann, M.~B. Foote, H.~J. Aerts, G.~K. Savova, R.~H. Mak, and
  D.~S. Bitterman, ``The utility of chatgpt for cancer treatment information,''
  \emph{medRxiv}, 2023.

\bibitem{Malinka-arxiv-2023-Education}
K.~Malinka, M.~Peres{\'{\i}}ni, A.~Firc, O.~Hujnak, and F.~Janus, ``On the
  educational impact of chatgpt: Is artificial intelligence ready to obtain a
  university degree?'' \emph{CoRR}, vol. abs/2303.11146, 2023.

\bibitem{Susnjak-arxiv-2022-Education}
T.~Susnjak, ``Chatgpt: The end of online exam integrity?'' \emph{CoRR}, vol.
  abs/2212.09292, 2022.

\bibitem{Tan-arxiv-2023-towards}
K.~Tan, T.~Pang, and C.~Fan, ``Towards applying powerful large ai models in
  classroom teaching: Opportunities, challenges and prospects,'' 2023.

\bibitem{Kamalov-2023-arxiv-A}
F.~Kamalov and I.~Gurrib, ``A new era of artificial intelligence in education:
  {A} multifaceted revolution,'' \emph{CoRR}, vol. abs/2305.18303, 2023.

\bibitem{Kasneci-learning-2023-chatgpt}
E.~Kasneci, K.~Se{\ss}ler, S.~K{\"u}chemann, M.~Bannert, D.~Dementieva,
  F.~Fischer, U.~Gasser, G.~Groh, S.~G{\"u}nnemann, E.~H{\"u}llermeier
  \emph{et~al.}, ``Chatgpt for good? on opportunities and challenges of large
  language models for education,'' \emph{Learning and Individual Differences},
  vol. 103, p. 102274, 2023.

\bibitem{Stanek-arxiv-2023-Can}
A.~Blair{-}Stanek, N.~Holzenberger, and B.~V. Durme, ``Can {GPT-3} perform
  statutory reasoning?'' \emph{CoRR}, vol. abs/2302.06100, 2023.

\bibitem{Trautmann-arxiv-2022-Legal}
D.~Trautmann, A.~Petrova, and F.~Schilder, ``Legal prompt engineering for
  multilingual legal judgement prediction,'' \emph{CoRR}, vol. abs/2212.02199,
  2022.

\bibitem{Choi-SSRN-2023-Chatgpt}
J.~H. Choi, K.~E. Hickman, A.~Monahan, and D.~Schwarcz, ``Chatgpt goes to law
  school,'' \emph{Available at SSRN}, 2023.

\bibitem{Nay-arxiv-2022-Law}
J.~J. Nay, ``Law informs code: {A} legal informatics approach to aligning
  artificial intelligence with humans,'' \emph{CoRR}, vol. abs/2209.13020,
  2022.

\bibitem{Yu-2022-arxiv-Legal}
F.~Yu, L.~Quartey, and F.~Schilder, ``Legal prompting: Teaching a language
  model to think like a lawyer,'' \emph{CoRR}, vol. abs/2212.01326, 2022.

\bibitem{Trautmann-2022-arxiv-Legal}
D.~Trautmann, A.~Petrova, and F.~Schilder, ``Legal prompt engineering for
  multilingual legal judgement prediction,'' \emph{CoRR}, vol. abs/2212.02199,
  2022.

\bibitem{Tamkin-arxiv-2021-Understanding}
A.~Tamkin, M.~Brundage, J.~Clark, and D.~Ganguli, ``Understanding the
  capabilities, limitations, and societal impact of large language models,''
  \emph{CoRR}, vol. abs/2102.02503, 2021.

\bibitem{Sun-arxiv-2023-A}
Z.~Sun, ``A short survey of viewing large language models in legal aspect,''
  \emph{CoRR}, vol. abs/2303.09136, 2023.

\bibitem{Abid-AIES-2021-Persistent}
A.~Abid, M.~Farooqi, and J.~Zou, ``Persistent anti-muslim bias in large
  language models,'' in \emph{{AIES} '21: {AAAI/ACM} Conference on AI, Ethics,
  and Society, Virtual Event, USA, May 19-21, 2021}, M.~Fourcade, B.~Kuipers,
  S.~Lazar, and D.~K. Mulligan, Eds.\hskip 1em plus 0.5em minus 0.4em\relax
  {ACM}, 2021, pp. 298--306.

\bibitem{Shah-arxiv-2023-Zero}
A.~Shah and S.~Chava, ``Zero is not hero yet: Benchmarking zero-shot
  performance of llms for financial tasks,'' \emph{CoRR}, vol. abs/2305.16633,
  2023.

\bibitem{Araci-arxiv-2023-FinBERT}
D.~Araci, ``Finbert: Financial sentiment analysis with pre-trained language
  models,'' \emph{CoRR}, vol. abs/1908.10063, 2019.

\bibitem{Alvarado-ALTA-2015-Domain}
J.~C.~S. Alvarado, K.~Verspoor, and T.~Baldwin, ``Domain adaption of named
  entity recognition to support credit risk assessment,'' in \emph{Proceedings
  of the Australasian Language Technology Association Workshop, {ALTA} 2015,
  Parramatta, Australia, December 8 - 9, 2015}, B.~Hachey and K.~Webster,
  Eds.\hskip 1em plus 0.5em minus 0.4em\relax {ACL}, 2015, pp. 84--90.

\bibitem{Son-arxiv-2023-Beyond}
G.~Son, H.~Jung, M.~Hahm, K.~Na, and S.~Jin, ``Beyond classification: Financial
  reasoning in state-of-the-art language models,'' \emph{CoRR}, vol.
  abs/2305.01505, 2023.

\bibitem{zhang-arxiv-2023-xuanyuan}
X.~Zhang, Q.~Yang, and D.~Xu, ``Xuanyuan 2.0: A large chinese financial chat
  model with hundreds of billions parameters,'' \emph{arXiv preprint
  arXiv:2305.12002}, 2023.

\bibitem{Yang-2023-arxiv-FinGPT}
H.~Yang, X.-Y. Liu, and C.~D. Wang, ``Fingpt: Open-source financial large
  language models,'' \emph{CoRR}, vol. abs/2306.06031, 2023.

\bibitem{Jin-emnlp-2019-PubMedQA}
Q.~Jin, B.~Dhingra, Z.~Liu, W.~W. Cohen, and X.~Lu, ``Pubmedqa: {A} dataset for
  biomedical research question answering,'' in \emph{Proceedings of the 2019
  Conference on Empirical Methods in Natural Language Processing and the 9th
  International Joint Conference on Natural Language Processing, {EMNLP-IJCNLP}
  2019, Hong Kong, China, November 3-7, 2019}, 2019, pp. 2567--2577.

\bibitem{Anastasia-blog-2022-BioASQ}
A.~Krithara, A.~Nentidis, K.~Bougiatiotis, and G.~Paliouras, ``Bioasq-qa: A
  manually curated corpus for biomedical question answering,'' 2022.

\bibitem{Bi-arxiv-2023-OceanGPT}
Z.~Bi, N.~Zhang, Y.~Xue, Y.~Ou, D.~Ji, G.~Zheng, and H.~Chen, ``Oceangpt: {A}
  large language model for ocean science tasks,'' \emph{CoRR}, vol.
  abs/2310.02031, 2023.

\bibitem{Zhang-arxiv-2023-One}
C.~Zhang, C.~Zhang, C.~Li, Y.~Qiao, S.~Zheng, S.~K. Dam, M.~Zhang, J.~U. Kim,
  S.~T. Kim, J.~Choi, G.~Park, S.~Bae, L.~Lee, P.~Hui, I.~S. Kweon, and C.~S.
  Hong, ``One small step for generative ai, one giant leap for {AGI:} {A}
  complete survey on chatgpt in {AIGC} era,'' \emph{CoRR}, vol. abs/2304.06488,
  2023.

\bibitem{Haman-2023-air-Using}
M.~Haman and M.~Skolnik, ``Using chatgpt to conduct a literature review.''
  \emph{Accountability in research}, 2023.

\bibitem{Aydn-2022-ssrn-OpenAI}
{\"O}.~Aydın and E.~Karaarslan, ``Openai chatgpt generated literature review:
  Digital twin in healthcare,'' \emph{SSRN Electronic Journal}, 2022.

\bibitem{Part-2023-arxiv-Can}
Y.~J. Park, D.~Kaplan, Z.~Ren, C.~Hsu, C.~Li, H.~Xu, S.~Li, and J.~Li, ``Can
  chatgpt be used to generate scientific hypotheses?'' \emph{CoRR}, vol.
  abs/2304.12208, 2023.

\bibitem{Hasaan-2023-arxiv-ChatGPT}
M.~M. Hassan, R.~A. Knipper, and S.~K.~K. Santu, ``Chatgpt as your personal
  data scientist,'' \emph{CoRR}, vol. abs/2305.13657, 2023.

\bibitem{Cheng-2023-arxiv-Is}
L.~Cheng, X.~Li, and L.~Bing, ``Is {GPT-4} a good data analyst?'' \emph{CoRR},
  vol. abs/2305.15038, 2023.

\bibitem{Alkaissi-pubmed-2023-Artificial}
S.~I.~M. Hussam~Alkaissi, ``Artificial hallucinations in chatgpt: Implications
  in scientific writing,'' \emph{PubMed}, 2023.

\bibitem{Azaria-2023-arxiv-ChatGPT}
A.~Azaria, R.~Azoulay, and S.~Reches, ``Chatgpt is a remarkable tool -- for
  experts,'' \emph{CoRR}, vol. abs/2306.03102, 2023.

\bibitem{Buruk-2023-arxiv-Academic}
O.~O. Buruk, ``Academic writing with {GPT-3.5:} reflections on practices,
  efficacy and transparency,'' \emph{CoRR}, vol. abs/2304.11079, 2023.

\bibitem{Liu-2023-arxiv-ReviewerGPT}
R.~Liu and N.~B. Shah, ``Reviewergpt? an exploratory study on using large
  language models for paper reviewing,'' \emph{CoRR}, vol. abs/2306.00622,
  2023.

\bibitem{Kosinski-arxiv-2023-tom}
M.~Kosinski, ``Theory of mind may have spontaneously emerged in large language
  models,'' \emph{CoRR}, vol. abs/2302.02083, 2023.

\bibitem{Amin-arxiv-2023-affective}
M.~M. Amin, E.~Cambria, and B.~W. Schuller, ``Will affective computing emerge
  from foundation models and general ai? {A} first evaluation on chatgpt,''
  \emph{CoRR}, vol. abs/2303.03186, 2023.

\bibitem{Sridhara-2023-arxiv-ChatGPT}
G.~Sridhara, R.~H. G., and S.~Mazumdar, ``Chatgpt: {A} study on its utility for
  ubiquitous software engineering tasks,'' \emph{CoRR}, vol. abs/2305.16837,
  2023.

\bibitem{Sun-2023-arxiv-Automatic}
W.~Sun, C.~Fang, Y.~You, Y.~Miao, Y.~Liu, Y.~Li, G.~Deng, S.~Huang, Y.~Chen,
  Q.~Zhang, H.~Qian, Y.~Liu, and Z.~Chen, ``Automatic code summarization via
  chatgpt: How far are we?'' \emph{CoRR}, vol. abs/2305.12865, 2023.

\bibitem{Xia-2023-arxiv-Conversational}
C.~S. Xia and L.~Zhang, ``Conversational automated program repair,''
  \emph{CoRR}, vol. abs/2301.13246, 2023.

\bibitem{Kuang-2023-arxiv-FederatedScope}
W.~Kuang, B.~Qian, Z.~Li, D.~Chen, D.~Gao, X.~Pan, Y.~Xie, Y.~Li, B.~Ding, and
  J.~Zhou, ``Federatedscope-llm: A comprehensive package for fine-tuning large
  language models in federated learning,'' 2023.

\end{thebibliography}
