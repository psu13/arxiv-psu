\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Hinton, Mnih, Leibo, and Ionescu]{ba2016using}
Jimmy Ba, Geoffrey~E Hinton, Volodymyr Mnih, Joel~Z Leibo, and Catalin Ionescu.
\newblock Using fast weights to attend to the recent past.
\newblock In \emph{NIPS}, pp.\  4331--4339, 2016.

\bibitem[Bahdanau et~al.(2015)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock In \emph{ICLR}, 2015.

\bibitem[Chelba et~al.(2013)Chelba, Mikolov, Schuster, Ge, Brants, Koehn, and
  Robinson]{chelba2013one}
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi~Ge, Thorsten Brants, Phillipp
  Koehn, and Tony Robinson.
\newblock One billion word benchmark for measuring progress in statistical
  language modeling.
\newblock \emph{arXiv preprint arXiv:1312.3005}, 2013.

\bibitem[Cheng et~al.(2016)Cheng, Dong, and Lapata]{cheng2016long}
Jianpeng Cheng, Li~Dong, and Mirella Lapata.
\newblock Long short-term memory-networks for machine reading.
\newblock In \emph{EMNLP}, pp.\  551--561, 2016.

\bibitem[Chorowski et~al.(2015)Chorowski, Bahdanau, Serdyuk, Cho, and
  Bengio]{chorowski2015attention}
Jan~K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua
  Bengio.
\newblock Attention-based models for speech recognition.
\newblock In \emph{NIPS}, pp.\  577--585, 2015.

\bibitem[Cui et~al.(2016{\natexlab{a}})Cui, Chen, Wei, Wang, Liu, and
  Hu]{cui2016attention}
Yiming Cui, Zhipeng Chen, Si~Wei, Shijin Wang, Ting Liu, and Guoping Hu.
\newblock Attention-over-attention neural networks for reading comprehension.
\newblock \emph{arXiv preprint arXiv:1607.04423}, 2016{\natexlab{a}}.

\bibitem[Cui et~al.(2016{\natexlab{b}})Cui, Liu, Chen, Wang, and
  Hu]{cui2016consensus}
Yiming Cui, Ting Liu, Zhipeng Chen, Shijin Wang, and Guoping Hu.
\newblock Consensus attention-based neural networks for chinese reading
  comprehension.
\newblock \emph{arXiv preprint arXiv:1607.02250}, 2016{\natexlab{b}}.

\bibitem[Dhingra et~al.(2016)Dhingra, Liu, Cohen, and
  Salakhutdinov]{dhingra2016gated}
Bhuwan Dhingra, Hanxiao Liu, William~W Cohen, and Ruslan Salakhutdinov.
\newblock Gated-attention readers for text comprehension.
\newblock \emph{arXiv preprint arXiv:1606.01549}, 2016.

\bibitem[Graves et~al.(2014)Graves, Wayne, and Danihelka]{graves2014neural}
Alex Graves, Greg Wayne, and Ivo Danihelka.
\newblock Neural turing machines.
\newblock \emph{arXiv preprint arXiv:1410.5401}, 2014.

\bibitem[Gulcehre et~al.(2016)Gulcehre, Chandar, Cho, and
  Bengio]{gulcehre2016dynamic}
Caglar Gulcehre, Sarath Chandar, Kyunghyun Cho, and Yoshua Bengio.
\newblock Dynamic neural turing machine with soft and hard addressing schemes.
\newblock \emph{arXiv preprint arXiv:1607.00036}, 2016.

\bibitem[Hill et~al.(2016)Hill, Bordes, Chopra, and Weston]{hill2015goldilocks}
Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston.
\newblock The goldilocks principle: Reading children's books with explicit
  memory representations.
\newblock In \emph{ICLR}, 2016.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and Schmidhuber]{HOC97}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Ji et~al.(2016)Ji, Vishwanathan, Satish, Anderson, and
  Dubey]{ji2015blackout}
Shihao Ji, SVN Vishwanathan, Nadathur Satish, Michael~J Anderson, and Pradeep
  Dubey.
\newblock Blackout: Speeding up recurrent neural network language models with
  very large vocabularies.
\newblock In \emph{ICLR}, 2016.

\bibitem[Jozefowicz et~al.(2016)Jozefowicz, Vinyals, Schuster, Shazeer, and
  Wu]{JOZ16}
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.
\newblock Exploring the limits of language modeling.
\newblock \emph{arXiv preprint arXiv:1602.02410}, 2016.

\bibitem[Kadlec et~al.(2016)Kadlec, Schmid, Bajgar, and
  Kleindienst]{kadlec2016text}
Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan Kleindienst.
\newblock Text understanding with the attention sum reader network.
\newblock In \emph{ACL}, 2016.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.

\bibitem[Mikolov et~al.(2010)Mikolov, Karafi{\'a}t, Burget, Cernock{\`y}, and
  Khudanpur]{MIK10}
Tomas Mikolov, Martin Karafi{\'a}t, Lukas Burget, Jan Cernock{\`y}, and Sanjeev
  Khudanpur.
\newblock Recurrent neural network based language model.
\newblock In \emph{Interspeech}, volume~2, pp.\ ~3, 2010.

\bibitem[Mikolov et~al.(2011)Mikolov, Deoras, Kombrink, Burget, and
  Cernock{\`y}]{mikolov2011empirical}
Tomas Mikolov, Anoop Deoras, Stefan Kombrink, Lukas Burget, and Jan
  Cernock{\`y}.
\newblock Empirical evaluation and combination of advanced language modeling
  techniques.
\newblock In \emph{Interspeech}, 2011.

\bibitem[Miller et~al.(2016)Miller, Fisch, Dodge, Karimi, Bordes, and
  Weston]{MIL16}
Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes,
  and Jason Weston.
\newblock Key-value memory networks for directly reading documents.
\newblock \emph{arXiv preprint arXiv:1606.03126}, 2016.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{pascanu2013difficulty}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{ICML}, pp.\  1310--1318, 2013.

\bibitem[Reed \& de~Freitas(2015)Reed and de~Freitas]{reed2015neural}
Scott Reed and Nando de~Freitas.
\newblock Neural programmer-interpreters.
\newblock \emph{arXiv preprint arXiv:1511.06279}, 2015.

\bibitem[Rockt{\"a}schel et~al.(2016)Rockt{\"a}schel, Grefenstette, Hermann,
  Kocisky, and Blunsom]{rocktaschel2015reasoning}
Tim Rockt{\"a}schel, Edward Grefenstette, Karl~Moritz Hermann, Tomas Kocisky,
  and Phil Blunsom.
\newblock Reasoning about entailment with neural attention.
\newblock In \emph{ICLR}, 2016.

\bibitem[Rumelhart et~al.(1985)Rumelhart, Hinton, and
  Williams]{rumelhart1985learning}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning internal representations by error propagation.
\newblock Technical report, DTIC Document, 1985.

\bibitem[Rush et~al.(2015)Rush, Chopra, and Weston]{rush2015attention}
Alexander~M. Rush, Sumit Chopra, and Jason Weston.
\newblock A neural attention model for abstractive sentence summarization.
\newblock In \emph{EMNLP}, pp.\  379--389, 2015.

\bibitem[Shazeer et~al.(2015)Shazeer, Pelemans, and Chelba]{shazeer2015sparse}
Noam Shazeer, Joris Pelemans, and Ciprian Chelba.
\newblock Sparse non-negative matrix language modeling for skip-grams.
\newblock In \emph{Interspeech}, pp.\  1428--1432, 2015.

\bibitem[Soltani \& Jiang(2016)Soltani and Jiang]{soltani2016higher}
Rohollah Soltani and Hui Jiang.
\newblock Higher order recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1605.00064}, 2016.

\bibitem[Steinbuch \& Piske(1963)Steinbuch and Piske]{steinbuch1963learning}
Karl Steinbuch and UAW Piske.
\newblock Learning matrices and their applications.
\newblock \emph{IEEE Transactions on Electronic Computers}, pp.\  846--862,
  1963.

\bibitem[Sukhbaatar et~al.(2015)Sukhbaatar, Weston, and Fergus]{SUK15}
Sainbayar Sukhbaatar, Jason Weston, and Rob Fergus.
\newblock End-to-end memory networks.
\newblock In \emph{NIPS}, pp.\  2440--2448, 2015.

\bibitem[Taylor(1959)]{taylor1959pattern}
WK~Taylor.
\newblock Pattern recognition by means of automatic analogue apparatus.
\newblock \emph{Proceedings of the IEE-Part B: Radio and Electronic
  Engineering}, 106\penalty0 (26):\penalty0 198--209, 1959.

\bibitem[Tran et~al.(2016)Tran, Bisazza, and Monz]{tran2016recurrent}
Ke~Tran, Arianna Bisazza, and Christof Monz.
\newblock Recurrent memory networks for language modeling.
\newblock In \emph{NAACL-HLT}, pp.\  321--331, 2016.

\bibitem[Trischler et~al.(2016)Trischler, Ye, Yuan, and
  Suleman]{trischler2016natural}
Adam Trischler, Zheng Ye, Xingdi Yuan, and Kaheer Suleman.
\newblock Natural language comprehension with the epireader.
\newblock \emph{arXiv preprint arXiv:1606.02270}, 2016.

\bibitem[Weissenborn(2016)]{weissenborn2016separating}
Dirk Weissenborn.
\newblock Separating answers from queries for neural reading comprehension.
\newblock \emph{arXiv preprint arXiv:1607.03316}, 2016.

\bibitem[Werbos(1990)]{werbos1990backpropagation}
Paul~J Werbos.
\newblock Backpropagation through time: what it does and how to do it.
\newblock \emph{Proceedings of the IEEE}, 78\penalty0 (10):\penalty0
  1550--1560, 1990.

\bibitem[Weston et~al.(2015)Weston, Chopra, and Bordes]{weston2014memory}
Jason Weston, Sumit Chopra, and Antoine Bordes.
\newblock Memory networks.
\newblock In \emph{ICLR}, 2015.

\bibitem[Williams et~al.(2015)Williams, Prasad, Mrva, Ash, and
  Robinson]{williams2015scaling}
Will Williams, Niranjani Prasad, David Mrva, Tom Ash, and Tony Robinson.
\newblock Scaling recurrent neural network language models.
\newblock In \emph{2015 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pp.\  5391--5395. IEEE, 2015.

\bibitem[Xu et~al.(2015)Xu, Ba, Kiros, Cho, Courville, Salakhutdinov, Zemel,
  and Bengio]{xu2015show}
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
  Salakhutdinov, Richard~S Zemel, and Yoshua Bengio.
\newblock Show, attend and tell: Neural image caption generation with visual
  attention.
\newblock In \emph{ICML}, 2015.

\bibitem[Yang et~al.(2016)Yang, Blunsom, Dyer, and Ling]{yang2016reference}
Zichao Yang, Phil Blunsom, Chris Dyer, and Wang Ling.
\newblock Reference-aware language models.
\newblock \emph{arXiv preprint arXiv:1611.01628}, 2016.

\end{thebibliography}
