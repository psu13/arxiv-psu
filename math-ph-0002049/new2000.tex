\documentclass[12pt]{article}

\usepackage{amsmath,amsthm,amscd,amssymb}
\usepackage[noBBpl,sc]{mathpazo}
\usepackage[papersize={6.9in, 10.0in}, left=.5in, right=.5in, top=.6in, bottom=.9in]{geometry}
\linespread{1.05}
\sloppy
\raggedbottom
\pagestyle{plain}

% these include amsmath and that can cause trouble in older docs.
\input{../helpers/cmrsum}
\input{../helpers/fix-underbrace.tex}

\usepackage[small]{titlesec}
\usepackage{cite}
\usepackage{microtype}

% hyperref last because otherwise some things go wrong.
\usepackage[colorlinks=true
,breaklinks=true
,urlcolor=blue
,anchorcolor=blue
,citecolor=blue
,filecolor=blue
,linkcolor=blue
,menucolor=blue
,linktocpage=true]{hyperref}
\hypersetup{
bookmarksopen=true,
bookmarksnumbered=true,
bookmarksopenlevel=10
}

% make sure there is enough TOC for reasonable pdf bookmarks.
\setcounter{tocdepth}{3}

%\usepackage[dotinlabels]{titletoc}
%\titlelabel{{\thetitle}.\quad}
%\input{../helpers/psu-plain-titles.tex}
%\input{../helpers/psu-sc-headers.tex}
%\input{../helpers/fix-revtex-12.tex}
%\DeclareSymbolFont{CMlargesymbols}{OMX}{cmex}{m}{n}
%\DeclareMathSymbol{\sum}{\mathop}{CMlargesymbols}{"50}
%\pdfbookmark[1]{Introduction}{Introduction}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
%\renewcommand{\baselinestretch}{2}
\title{Classical and Quantum Probability}
\author{R. F. Streater,\\Dept. of Mathematics,
King's College London,\\ Strand, WC2R 2LS}
\date{25 August 1999.}
\begin{document}
\maketitle
\begin{abstract}
We follow the development of probability theory from the beginning
of the last century, emphasising that quantum theory is really a
generalisation of
this theory. The great achievements of probability theory, such as
the theory of processes, generalised random fields, estimation
theory and information geometry, are reviewed. Their quantum versions
are then described.\\
\\
{\bf Keywords}: Probability, sampling, processes, Markov chains, random fields,
Fisher information, quantum probability, quantum information manifolds.
\end{abstract}
\thispagestyle{empty}
\newpage

\section{Introduction}
The are few mathematical topics that are as badly taught to physicists
as probability theory. Maxwell, Boltzmann and Gibbs were using probabilistic
methods long before the subject was properly established as mathematics.
Their language, of ensembles,
complexions, fluctuations and most probable state, are still used. When
quantum theory came along, the same notions were fitted into the new theory,
sometimes leading to confusion. We review the mathematical development
of probability, emphasising that quantum theory is a generalisation. The
approach to history is in the same spirit as used by Milligan in
\cite{Milligan}

There are three ``philosophies'' concerning probability. 
In the easy case, when there are finitely many possible outcomes to the
experiment being considered, 
Laplace's principle of equal ignorance tells us
that the probability of each of the outcomes is the same. In the case of a
die with six sides, experiments suggest that the probabilities are not all
exactly equal. Nevertheless, there is
not much error if we assume that the probability of each number is $1/6$.
An objection to Laplace's principle in general is that it is
not always clear that the outcome of a particular experiment is a matter
of chance, even when we do not know which outcome
will turn up; it could even be that a particular outcome is inevitable. Thus
a more robust version of Laplace's principle might be that in events
governed by chance, the probability of each possible outcome is the same.
This still leaves open the meaning of the phrase, ``governed by chance''.
The difficulty of defining the ``uniform'' distribution when variates take
continuous values is illustrated by Bertrand's paradox (\cite{Decker},
p. 246). This demolished Laplace's principle for continuous variables.


The philosophy of Laplace applied to probability theory might be described as
{\em Platonic}. A real die is the shadow of the ideal die, which has
perfect sides and exact probabilities of $1/6$ for each outcome. This
has a modern form of expression: we model the real die by the sample space
$\Omega=\{1,2,\ldots,6\}$ whose elements $\omega$ are called {\em outcomes},
and assign the probability $1/6$ to each. The value of a {\em random
variable} $f$ is known if we know the outcome $\omega$; $f$ is
therefore a real-valued function on $\Omega$.
More generally, if the sample space is a finite set $\Omega$, an event $E$
is a subset of $\Omega$; we say that the event has occurred
if the $\omega$ that occurs lies in $E$. The probability that $E$ occurs
is the sum of the probabilities of the points in E:
\begin{equation}
p(E)=\sum_{\omega\in E}p(\omega).
\end{equation}
We say that two events, $E$, $F$, are independent if $p(E\cap F)=p(E)p(F)$.
In this way the binomial distribution can be derived for the total shown
by $n$ dice thrown independently, and all of Laplace's probability theory
can be derived. It can tell us what bets to lay on an event $E$, even
when only one trial is going to occur.

Laplace's method has been
successfully applied to statistical mechanics; the space of states is
discretised, thus avoiding Bertrand's paradox (the choice of
bins being suggested by quantum mechanics). Each bin is said to be
equally probable, and some hypotheses about independence is postulated.
Then it is shown that the complexion (macroscopic state) given by the
Gibbs distribution is not just the most probable, but is {\em
overwhelmingly} the most probable. The chance of any complexion minutely
different is put at $10^{-170}$. The Gibbs distribution is, of course,
the equilibrium state; if it is so probable, how come systems manage to
be out of equilibrium, and remain so for years at a time? This remark
is not aimed at Tolman \cite{Tolman}, who made it clear that the assumption
of equal probabilities applies only to equilibrium, and is to be tested
against experiment; it passes the test well, but he then spoils it by adding
as a further justification, ``{\em without} this
postulate there would be nothing to correspond to the circumstance that
nature does not have any tendency to present us with systems in conditions 
which we regard as mechanically entirely possible but statistically
improbable''. The word ``improbable'' is itself based on Laplace's
assumption!

The second ``philosophy'' of probability can be described as Aristotelian;
it had taken hold by 1920, and is known as the
``frequentist'' approach. It is essential that we can reproduce a long run
of independent trials each conducted under exactly the same experimental
conditions. In this respect, the theory makes sense only within a
scientific culture. Suppose that we have one
``variate'', which may take continuous or discrete values. The result
of a measurement of the variate is assigned to one of a
preassigned set of ``bins'', which are intervals on the real axis.
We repeat a number of times, to find the histogram, that is, the number
$n_i$ of
events (out of N trials) in the $i^{\rm th}$ bin. If the histogram settles
down to a stable shape as we increase $N$, we declare that the value of the
variate is random (or, random enough). We then define the probability of
the event $i$ to be
\begin{equation}
p_i=\lim_{N\rightarrow\infty}\frac{n_i}{N}.
\end{equation}
This approach avoids the above problems that beset the Laplace philosophy.
However, it is completely useless as mathematics; a ``definition''
should not depend on an infinite number of future experimental results.
There is not one theorem that can be
proved from this definition. Feller points out that we must avoid
confusion between a definition, and a method of measurement.
There is great heuristic value to the frequentist approach.
It is easy to teach \cite{Decker};
we do not prejudge the possible values that the variate
can have, or the probability of a given value; we can
introduce another variate $Y$, and observe its distribution, and its
joint probability distribution with $X$;
we can by extending this idea get access to the
joint probability distribution of any finite number of variates;
we can get some idea as to whether the variates
are random by examining a sequence of independent trials. We can even cover
situations in which two variates are not simultaneously observable, as in
quantum mechanics, by listing only the joint distributions of compatible
observables, and omitting those we cannot measure.
If we measure a variate $X$ with $n$ different values $x_i$ with relative
frequency $p_i$, we can construct a sample space $x_1,\ldots,x_n$, and
assign the probability $p_i$ to the occurrence of the outcome $x_i$.
Similarly, we can construct a sample space and probability for any
finite set of compatible variates if each measurement records their
values. The observed probabilities are more reliable than assuming all
points are equally probable.

However, there is one
grave disadvantage of the approach, apart from not being mathematics: it
is simply a description of data, and has much less predictive power than
Laplace's method. In particular, the method takes no position on the
question as to what are the possible variates. If
$\Omega$ has $|\Omega|=n$ points, then the random variables form a vector
space, denoted ${\cal A}(\Omega)$, of dimension $n$, so that at most $n$
random variables can
be linearly independent. No similar constraint holds in the frequentist
point of view. Thus a variate is not the same as a random variable.
In fact, it has no definition, other than the statement that its values are
random.

The frequentist approach is the safest one to use in studies
involving humans; social or financial matters are so complicated that
it is not likely that a sample space, $\Omega_1$ say, chosen to accommodate
the data observed so far, can describe all the possible new variates and the
values available to them. In the frequentist approach, faced with a new
variate, $Y$, one simply
takes the set of possible values of $Y$, say $\Omega_2=\{y_1,\ldots,y_m\}$,
and uses $\Omega_1\times\Omega_2$
as the sample space of the enhanced problem.

In a classical system in physics or chemistry, treated by classical
statistical mechanics, we want to follow the
scientific method: we model the system, do experiments, and
reject the model if forced to. In that case, we make another model,
estimate its parameters, and suggest more testing experiments.
We want and expect to be able to make predictions about variates not
measured yet. So we must reject the frequentist approach.

The third philosophy of probability \cite{Kolmogorov}was made clear by Kolmogorov, and
combines something of the first two;
it is to regard a probability theory as a {\em model}, to be tested
against experiment. It is like Plato's ideal, in that it
is based on a specified sample space $\Omega$;
but now the probability $p$ is not determined by
pure thought; any $p$ satisfying the axioms below provides us with a model.
\begin{definition}
Let $\Omega$ be a countable space.
A map $p:\Omega\rightarrow[0,1]$ is a {\em probability} if
$p(\omega)\geq0$ and $\sum_\omega p(\omega)=1$.
\label{probability}
\end{definition}
The probability of an event $E\subseteq\Omega$, and the concept of
independence of two events, are then as in Laplace's theory and clearly
depend on the choice of $p$.

A random variable $f:\Omega
\rightarrow{\bf R}$ is chosen to
represent the variate being observed, the particular choice being
part of the interpretation of the model. A theoretical idea, or else
the first few experiments on the variate $f$, allow us to get some
guide-lines for $\Omega$ and
the values of $p$. This is the subject of {\em estimation
theory}. We can judge the validity of the model (the choices we have made
for $(\Omega,p,f)$) by comparing the predictions of the model with the
observed frequencies $n_i$
using the theory of {\em significance tests}. Both estimation theory and
significance were developed before Kolmogorov's book. The
founders of these techniques were often frequentists; they realised
that one could not use an extreme frequentist
point of view: in estimation, they often postulated that the data had
Gaussian distributions, but with unknown parameters. In significance
testing, to make a start, they assumed a probability distribution
for the variate being measured; this is called the ``hypothesis H'',
which is part of the model; it can be rejected
if the data are significantly unlikely. This
has a version within Kolmogorov's formulation, in which we are given a
probability space, the pair $(p,\Omega)$, and model the variate with
a random variable, $f$.
To make contact with the well-established theory of estimation
and significance, we must relate the probability distribution of $f$
to the probability $p$. We now remind the reader how this is done.

Given a finite probability space $(p,\Omega)$ and a
random variable $f:\Omega\rightarrow {\bf R}$, the {\em probability
distribution} of $f$ is denoted $p_f(i)$, and is determined as follows:
let $x_i, i=1,2,\ldots,n$ be the values that
$f$ takes, and let $p_f(i)$ be the probability that the event $\{\omega:f
(\omega)=x_i\}$. That is
\begin{equation}
p_f(i):=\sum_{\omega:f(\omega)=x_i}p(\omega).
\end{equation}
This is what is accessible to experiments when we measure $f$.
The mean of $f$ is determined by $x_i$ and $p_f$:
\begin{equation}
E_p[f]:=\sum_\omega p(\omega)f(\omega)=\sum_i x_ip_f(i),
\mbox{ also written }p.f.
\label{mean}
\end{equation}
Given two random variables on $(\Omega,p)$, $f,g$ we define the joint
distribution, denoted $p_{f,g}(i,j)$ to be
\begin{equation}
p_{f,g}(i,j):=p\{\omega:f(\omega)=x_i\mbox{ and }g(\omega)=y_j\}.
\end{equation}
We say two r. v. are independent if the events $\{f(\omega)=x_i\}$ and
$\{g(\omega)=y_j\}$ are independent for all $i,j$. This is equivalent to
the frequentists' version: $p_{f,g}(i,j)=p_f(i)p_g(j)$.
The joint distribution determines $p_f$ and $p_g$ as its marginals,
and also all moments, e. g. the cross-moment $E_p[fg]$
can be shown to be $\sum_{ij}x_iy_jp_{f,g}(i,j).$

A probability $p$ defines a linear functional on the set ${\cal A}(\Omega)$
by the expectation, (\ref{mean}): $f\mapsto E_p[f]$.
We shall call any such functional a {\em state}: it is linear and positive,
taking the value $1$ on the sure function $I$.
The dual space ${\cal A}^d$ is the set of all linear functionals, so the
states form a subset of ${\cal A}^d$; it is denoted $\Sigma(\Omega)$.
Given two states $p_1$ and $p_2$ and $0<\lambda<1$, their mixture
with probabilities $\lambda$ and $1-\lambda$, $p=\lambda p_1+(1-
\lambda)p_2$, is again a state. So the states form a convex set.

Whether $\Omega$ is countable or not, for a random variable $f$ on
$(\Omega,p)$
the probability of the occurrence of a single value $f_0$ might be zero,
even when there is an $\omega_0\in\Omega$ with $f(\omega_0)=f_0$;
for, $p(\omega_0)$ might be zero. This often happens when $\Omega$ is
not countable, and $f$ takes continuous values. Then, more information
about the probability measure is provided by the ``cumulative''
distribution function
\begin{equation}
P_f(x)=p\{\omega:f(\omega)<x\}.
\end{equation}
This is an increasing function of $x$, going from $0$ at $x=-\infty$ to $1$
at $x=\infty$.
We say that $f$ possesses a density $\rho_f$ if $P_f(x)$ is
differentiable, and we write
\begin{equation}
\rho_f(x)=\frac{dP_f(x)}{dx}.
\end{equation}
It is clear that we cannot cope with this subject without a certain
amount of real analysis.

A cumulative probability distribution $P_f(x)$ is determined by its
characteristic function
\begin{equation}
C_f(\lambda):=\int e^{i\lambda x}dP_f(x)
\end{equation}
Here we use the Stieltjes integral. Any characteristic function satisfies
\begin{enumerate}
\item $C(\lambda)$ is continuous;
\item $C(0)=1$;
\item $C$ is of positive type:
\[\sum_{ij}\overline{z}_iz_jC(\lambda_j-\lambda_i)\geq0.\]
\end{enumerate}
Conversely, any function $C$ obeying $1,2$ and $3$ is the characteristic
function of a probability distribution; this is Bochner's theorem.
In terms of the original $(\Omega,p)$ and random variable $f$, the
characteristic function is
\begin{equation}
C_f(\lambda):=E_p[e^{i\lambda f}].
\end{equation}
If $C_f$ is analytic in $\lambda$ around $\lambda=0$, we can easily
justify the expansion
\[C_f(\lambda)=\sum_n(i\lambda)^nE_p[f^n]/n!=\sum_n(i\lambda)^nM_n/n!.\]
here, $M_n$ are the $n^{\rm th}$ moments of $f$; for this reason,
$C_f$ acts as a {\em moment generating function} for the r. v. $f$.
An important variant of this is the cumulant generating function
\[ \log C_f(\lambda)=\sum_n(i\lambda)^n\kappa_n/n!.\]
We prefer to keep the imaginary unit in these formulas, since if we
drop it the mean $C_f(\lambda)$ might not be finite.
The cumulants $\kappa_n$ are determined by induction from the system
\begin{equation}
M_n=\sum_k\sum_{{\cal I}_k}\kappa_{n_1}\ldots\kappa_{n_k}.
\label{cumulants}
\end{equation}
Here, ${\cal I}_k={\cal I}_1\cup{\cal I}_2\cup\ldots\cup{\cal I}_k$
is an arbitrary partition of $\{1,2,\ldots,n\}$ into $k$ parts,
including the identity partition, and $n_j=|{\cal I}_j|, j=1,\ldots,k$.
The condition for independence, $p_{f,g}(i,j)=p_f(i)p_g(j)$ for all $i,j$ is equivalent to
$C_{f+g}=C_fC_g$; it follows that then the cumulants of $f+g$ are the sums
of those of $f$ and $g$.

For a Gaussian distribution, all the cumulants beyond the second are zero.
There are results of the following kind: if all the cumulants
$\kappa_n$ of a distribution are zero for $n\geq N$, then they are zero
beyond $n=2$, and so the distribution is Gaussian.
These results use the positivity of the mean of a positive polynomial in $f$:
\begin{equation}
\sum_{ij}\overline{z}_iz_jM_{i+j}=E[\sum_{ij}\overline{z}_iz_jf^{i+j}]=
E[|\sum_jz_if^j|^2]\geq0.
\label{moment}
\end{equation}
Given a set of real numbers $\{M_n\}$ satisfying the positivity condition in
(\ref{moment}), it is
not obvious that $M_n$ is the $n^{\rm th}$ moment of a random variable $f$,
or that if so, $f$ is unique.
This has led to a body of work called the moment problem.

The distribution of a random variable $f$ determines that of any
differentiable function $g(f)$ of $f$; this is also a random variable;
the density of the distribution of $g$ is determined by the usual rule:
if $g$ is bijective, so that $f$ is a function of $g$,
the probability that $g$ lies between $y$ and $y+dy$
is $\rho_g(y)dy$, and this occurs if and only if $f$ lies between $x$
and $x+dx$, where $y=g(x)$. Therefore $\rho_gdy=\rho_fdx$, giving the
relation
\begin{equation}
\rho_g=|(df/dg)|\rho_f.
\label{change}
\end{equation}
If $g$ is not bijective, but has a local inverse with various branches,
$f_i$, then we have to sum over the contribution $|(df_i/dg)|\rho_{f_i}$
of each branch.


The remarkable thing is that the methods of probability theory give good
results in many cases that are not governed by chance, such as the
distribution of digits in $\pi$. Another example is the
configuration of a chaotic system at the time $t$, where $t$ is large,
given the initial configuration at time zero. If the initial
state is not specified sufficiently accurately, then the configuration at
time $t$ seems to be governed by chance, although it is not.
It was suggested by Krylov \cite{Krylov} that statistical physics is a
successful method exactly in the cases when the underlying
dynamics is chaotic. This will occur when nearby initial points
become exponentially far apart as time progresses, and this is signalled by
a positive
real part to the dominant eigenvalue of the linearised dynamics.
This largest real part is called the Lyapunov index.
We are talking here about a chaotic {\em theory};
actual experimental measurements will always have further uncertainty,
influenced by small
effects omitted from the theory. In a non-chaotic system small
forces can be omitted in the first few approximations. However, in a
chaotic system, the inclusion of one such small force can change the
outcome of the calculation at the large time $t$, making it appear to
be random. This is well modelled by omitting any attempt to include all
the actual forces, replacing those omitted by a ``noise'', that is, a
random term. Thus, we expect chaos to be well-modelled by a system with
increasing uncertainty, as measured by entropy. Kolmogorov, and then
Sinai took up Krylov's cause, and were able to relate the rate of
``entropy'' production to the Lyapunov exponent of the dynamics.
However, Ruelle interprets this 
\cite{Ruelle} as an increase in information, available as time goes by.



Laplace's problem, of whether to assign equal probabilities to each
energy-level of a system, arises in quantum theory.
Krylov takes von
Neumann to task for assuming that the density matrix for the state of a
particle with spin produced by a quantum process should, in the absence
of any theory or experiment, be taken to be totally unpolarised. Krylov
says that this is not true for most known processes, as the polarisation
is found to be nonzero, small for some and large for others.
Krylov's view is that it should be assigned
a general density matrix; we can then estimate this matrix in the light of
experiments. This leads to the subject of {\em quantum estimation},
for which there is a body of theory. Krylov
believed that physics is not in the gambling business; we do not second
guess the state of the system and follow a strategy of hedging against
wrong guesses; rather, in physics we predict what will happen (with various
probabilities) at a later time, when the initial state is known.

Estimation theory has received an impetus from a modern development,
information theory.
Shannon introduced the entropy of the random variable $f$ taking values
$x_i$ as
\begin{equation}
S_f:=-\sum_i p_f(x_i)\log p_f(x_i).
\end{equation}
Note that
$S_f$ does not depend on the actual values that $f$ takes. The distribution
with the maximum possible entropy is easily proved to be the uniform
distribution. The school of probability known as Bayesian therefore argues
that if we know nothing whatever about $f$ it must be assigned the uniform
distribution, called the {\em prior}. Thus, Laplace's
intuition gets very respectable support. There is one big problem with this:
the uniform distribution for $f$ is not in general consistent with
the uniform distribution for say $g=f^3$, as we see from eq.~(\ref{change});
so the prior depends on the
random variable we choose to name as the one we know nothing about. This
echoes Bertrand's paradox.

A quantum version of entropy was earlier given by von Neumann.
For the classical case $(\Omega,p)$ with $\Omega$ countable it reduces to
\begin{equation}
S(p):=-\sum_\omega p(\omega)\log p(\omega).
\end{equation}
It does not make any reference to a random variable. We may obtain
Shannon's entropy of a random variable $f$ as the von Neumann entropy of
$p_f$ regarded as a probability on the space of values that $f$ takes.
Note that $S_f=S(p)$ if $f$ takes different values at
different points of $\Omega$, that is, if $f$ separates the points of
$\Omega$. We then say that $f$ is a {\em sufficient statistic}. $S_f$ is in
general less than $S(p)$, and it reduces to zero when
$f$ takes only one value. The entropies of Shannon and von Neumann are not
the same concepts, and this difference reflects their different
interpretations; the point $\omega$ is the {\em message},
and $S_f$ is the information about the message that is
on average conveyed by measuring $f$; it cannot exceed $S(p)$, which is the
entropy (missing information) in the original probability space. Naturally,
if $f$ is sure it conveys no info at all. Since 
$S_f$ depends on the random variable $f$ only through its distribution, it
has a meaning in the frequentist approach.
To compute $S(p)$, the model $(\Omega,p)$ must be given, and it
does not depend on $f$. More generally, we can define the Shannon entropy
of a set $(f_1,\ldots,f_n)$ as the von Neumann entropy of their joint
distribution on the sample space of their values. Some authors regard the
Shannon entropy as the physical entropy of a reduced
description of a physical model. The trouble with this idea
is that the introduction of noise in the measurement of $f$
causes the Shannon entropy to decrease, instead of to increase as we would
want.
 
A simple example of noise is that caused by a mapping $T:\Omega
\rightarrow\Omega$. This defines a co-action on the set of random variables:
$f\mapsto T^*f:=f\circ T$, which in fact in an endomorphism of ${\cal A}$.
If $T$ is not bijective, there might be points that can be distinguished
by measuring $f$, but not by measuring $T^*f$;
thus $S_{T^*f}\leq S_f$. This also holds more generally, when $T^*$ is a
convex linear sum of such maps, thus: $T^*=\sum\lambda_iT_i^*$. It can be
shown that this is the most general {\em stochastic map} on ${\cal A}$,
that is, linear map,
taking $I$ to $I$ and non-negative functions to non-negative functions.
The reduction in the information carried by $f$ in the presence of noise
is natural in telephony. The von Neumann entropy, on the other hand,
increases if we add noise. This is achieved by a bistochastic map $T$,
(a stochastic map whose adjoint is also stochastic). We write it as
a right action, thus: $p\mapsto pT$.
By the deep theorem
of Birkhoff \cite{Birkhoff,Ando}, a bistochastic matrix is a mixture of
permutations. Since a permutation $\Omega$ does not alter $S(p)$, and
$-p\log p$ is concave, we see that $S(pT)\geq S(p)$. Moreover,
the von Neumann entropy is not decreased by a reduced description, unlike
the Shannon version. Thus $S(p)$ is the
correct concept to represent physical entropy \cite{Streater}.

If we are given information about which $\omega$ has occurred, the
probability on $\Omega$, called the prior, changes. Suppose that $p$ is
the prior. If the information is that the sample lies in a known subset
(event) $\Omega_0\subseteq\Omega$, then Bayes's theorem on conditional
probability is used; the conditional probability is
\begin{equation}
p(E|\Omega_0):=\frac{p(E\cap\Omega_0)}{p(\Omega_0)}.
\end{equation}
This is called the posterior probability, and correctly describes the
probability among outcomes all of which lie in $\Omega_0$. A conditional
probability satisfies the axioms of probability, def.~ (\ref{probability}).

This use of information to modify the probability $p$ should not
be confused with estimation theory. There, we do not change $p$, since
after the measurement of independent samples, we continue to assume that
new samples are governed by the original $p$.
The method of estimation using the principle of maximum entropy proceeds
as follows. Suppose that we
know $\Omega$, and $f$, with $|\Omega|<\infty$ and we are also told the
average of $f$ over a number of independent trials.
We can vary $p$ over the simplex $\Sigma(\Omega)$ to find the
point that maximises the entropy of the probability, given the observed
mean value, $\eta$ say. Thus we use the method of Lagrange multipliers to
maximise
\[-\sum_\omega p(\omega)\log(p(\omega))\hspace{.4in}
\mbox{ subject to }E_p[f]=\eta.\]
Gibbs knew that the solution to this is
\begin{equation}
p(\omega)=Z^{-1}\exp(-\beta f(\omega)),
\label{Gibbs}
\end{equation}
where $Z=\sum_\omega e^{-\beta f(\omega)}$, is the Lagrange multiplier for
the normalisation condition $\sum p(\omega)=1$, and is called the
partition function. The parameter $\beta$ is the Lagrange multiplier for
the condition $p.f:=E_p[f]=\eta$, and is determined by it. Then (\ref{Gibbs})
is the least prejudiced estimate for the probability, given the mean
\cite{Ingarden,Jaynes}.

The method of maximum entropy solves an important problem in the theory of
estimation. Let $X$ be a variate of which the distribution is known
to be one of a family, ${\cal M}=\{p_\eta(i)\}_{\eta\in{\bf R}}$;
we hope to estimate
$\eta$ by measuring $X$ independently $m$ times. An {\em estimator}
$f$ is a function of the data $x_1,x_2,\ldots,x_m$
that is used for this estimate.
Thus $f$ is a function of $X$, and so is a random variable.
Since we do not know $\eta$, to be useful,
the estimator must be independent of $\eta$. We say an estimator is {\em
unbiased} if its mean is the desired parameter, thus:
\begin{equation}
p_\eta.f:=\sum_ip_\eta(i)f(x_i)=\eta.
\label{unbiased}
\end{equation}
Apart from being unbiased, a good estimator should have a small chance
of being far from the mean; so we are interested in estimators of
minimum variance, $V=p_\eta.[(f-\eta)^2]$.
To any probability $p\in{\cal M}$ define the
{\em Fisher information} as \cite{Rao,Fisher}
\begin{equation}
G=p_\eta.\left(\frac{\partial\log p_\eta}{\partial\eta}\right)^2
\label{Fisher}
\end{equation}
We recognise this as the variance of the random variable $Y=\partial\
p/\partial\eta$.
The {\em Cramer-Rao} theorem puts limits on the smallness
of the variance $V$ of an estimator $f$: 
\begin{theorem}
\begin{equation}
V\geq G^{-1}.
\label{CramerRao}
\end{equation}
\end{theorem}
For the proof, differentiate (\ref{unbiased}) with respect to $\eta$,
to get
\[\sum_i\frac{\partial p_\eta(i)}{\partial \eta}f(x_i)=1.\]
Now use $\partial/\partial\eta[\sum_ip_\eta(i)]=0,$ and rearrange, to get
\begin{equation}
\sum_ip_\eta(i)\left(\frac{\partial\log p_\eta(i)}{\partial\eta}
(f(x_i)-\eta)\right)=1.
\label{Schwarz}
\end{equation}
This is the correlation between the random variables $Y$ and $f$;
the (positive-semidefinite) covariance matrix is therefore
\begin{equation}
\left(\begin{array}{cc}
      G&1\\
      1&V
\end{array}\right)
\end{equation}
Schwarz's inequality then gives (\ref{CramerRao}).

The minimum variance allowed by (\ref{CramerRao}) occurs when the Cauchy
inequality is equality, which occurs when the factors in (\ref{Schwarz})
are proportional (with ratio dependent on $\eta$). Calling this factor
$-\partial\xi/\partial\eta$, we see that the distribution of minimum
variance must satisfy
\begin{equation}
\log p_\eta(i)=-\int^\eta \partial\xi/\partial\eta(f(x_i)-\eta)d\eta=
-\xi f(x_i)-\psi,
\end{equation}
showing that a necessary and sufficient condition is that $\{p_\eta\}$ be the
exponential family.




In the case of several parameters $\eta_1,\ldots\eta_n$, we have
estimators $f_1,\ldots,f_n$, which can
be taken to be linearly independent, but need not be functionally
independent. The state of maximum entropy, given the means $E_p[f_i]=\eta_i,
\hspace{.2in}i=1,\ldots,n$ is easily shown to be of the form
\begin{equation}
p(\omega)=Z^{-1}\exp-\{\xi_1f_1(\omega)+\ldots\xi_nf_n(\omega)\}=Z^{-1}\exp
(-f)\mbox{ say},
\label{infomanifold}
\end{equation}
where the Lagrange multipliers $\xi_i$ are determined by the given conditions
on the means. The set of probabilities of the form
eq.~(\ref{infomanifold}) form the set ${\cal M}$
called the info manifold, or the exponential family, determined by
Span$\{f_i\}$. We can regard $\{\xi_i\}$ or indeed $f$ as coordinates,
called canonical; or we can regard $\{\eta_i\}$ or indeed $p$ as coordinates,
called the expectation coordinates. In this case the Fisher {\em information
matrix} is defined to be
\begin{equation}
G^{ij}:=p.\left(\frac{\partial \log p}{\partial\eta_i}\frac{\partial\log p}{
\partial\eta_j}\right).
\end{equation}
Then the Cramer-Rao inequality (\ref{CramerRao}) becomes a matrix inequality,
where $V$ is the covariance matrix $V_{ij}:=p.[(f_i-\eta_i)(f_j-\eta_j)]$.
Equality holds only if $G=V^{-1}$, which leads to the exponential family. 

Rao showed that $G$  defines a Riemannian metric on
the tangent spaces of ${\cal M}$ \cite{Ingarden2}; as such, its components
depend on the coordinates chosen for the tangent space and it transforms
as a tensor under changes in variables.
At the point $p\in{\cal M}$, a vector in the tangent space is given in
in canonical coordinates by a random
variable $f$ in the span of the ``score variables'' $\hat{f}_j:=f_i-\eta_j$.
Writing $f=\sum_k\xi^k\hat{f}_k$ introduces contravariant components $\xi^k$.
These are dual to the $\eta_j$, which are covariant components.
The covariant metric is the covariance matrix
\begin{equation}
G_{ij}=G(\hat{f}_i,\hat{f}_j)=E_p[\hat{f}_i\hat{f}_j].
\end{equation}
It is the inverse of the contravariant $G^{ij}$, which explains why we get
equality in (\ref{CramerRao}).

The Massieu function $\psi:=\log Z$, where $Z$ is the partition function,
is related to the free energy; it is the generating function for
the cumulants; so we have
\begin{eqnarray}
\eta_j&=&-\frac{\partial \psi}{\partial \xi^j}\\
G_{ij}:=V_{ij}&=&\frac{\partial^2\psi}{\partial\xi^i\partial\xi^j}.
\end{eqnarray}
The entropy is the Legendre transform of $\psi$, and its second variation
is the Fisher information matrix, $G^{ij}$, the metric in the coordinates
$\eta$.

Amari showed that ${\cal M}$ is furnished with a pair of affine flat
connections, for which the global affine coordinates are $\xi^i$
and $\eta_i$ \cite{Amari}. These connections are not metric connections, but
are dual
relative to $G$. An important role in information geometry is played
by the {\em relative information} $S(p|p^\prime):=\sum_\omega p(\omega)
(\log p(\omega)-\log p^\prime(\omega))$. This distinguishes between the
points $p$ and $p^\prime$ in ${\cal M}$, in that $S(p|p^\prime)\geq 0$ and
vanishes only when $p=p^\prime$. For a modern version, see \cite{Pistone}.

The observables form the algebra ${\cal A}(\Omega)$ in which multiplication
is pointwise: $(fg)(\omega):=f(\omega)g(\omega)$; the states lie in
its dual. Thus, states and observables are not the same kind of thing,
and they transform as duals under stochastic maps.
However, states like observables are functions  of $\omega$; to distinguish
them we can write $p(\omega)$ for a state and
$(\omega)f$ for an
observable.  If $|\Omega|<\infty$, either can be identified with an element
of the formal vector space spanned by $\Omega$, thus: $\sum_\omega
\alpha(\omega)\omega\leftrightarrow\alpha$, whether $\alpha$ is regarded
as an observable or
a state. Then ${\cal M}$ is the interior of the convex hull of $\Omega$.
The permutation group of $\Omega$ acts by right action
$\omega\mapsto\omega T$. Its inverse $\omega\mapsto\omega T^{-1}$
is a co-action of the group (its product law is the opposite of that
of the group) and so can be written as a left action: $\omega\mapsto
T\omega:=\omega T^{-1}$. These induce a right action on probabilities,
and a left action on observables, by $pT(\omega):=p(T\omega)$ and
$(\omega)Tf:=(\omega T)f$, the latter written without the dual symbol $^*$.
These express associativity, as does the dual relation $pT.f=p.Tf$.

These definitions can be extended to any map $T:\Omega\rightarrow\Omega$,
whether invertible or not: we define the action on probabilities
using $T(\omega):=(\{\omega\}T^{-1})$, the inverse image of the point-set
$\{\omega\}$. Every algebraic
endomorphism of ${\cal A}$ is of the form $f\mapsto Tf$ for some map
$T:\Omega\rightarrow\Omega$, and these make up exactly
the extreme points of the convex set of stochastic maps.

In infinite dimensions, there is more than one useful topology
on the states and observables.
The modern view \cite{Pistone} is that the state $p$ and the observable
$-\log p$ are merely alternative coordinates for a point
in the info manifold. The natural class of charts are
related by monotone, convex functions, of which the stochastic maps,
\cite{Chentsov}, as well as the non-linear maps $p\mapsto-\log p$ and
$p\mapsto p^\alpha$, $0<\alpha<1$ are examples.

An active field of research is to set up quantum analogues of all this
\cite{Petz,Ray4,Ray5,Ray6} 
\input new20001


