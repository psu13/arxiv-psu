\section{From Bachelier to Wiener}
In 1900, Bachelier proposed a random model of the stock market
\cite{Bachelier};
the idea was that the decision to buy or sell a stock is randomly
taken by independent investors. Let us suppose that the chance $\lambda$
that the price goes up one unit $dx$ is the same as that for going down,
during any unit trading period $dt$. Let $X\in{\bf Z}$ be the random price,
and $p(x,t)$
be the probability that the price is $x$ at time $t$; then the
new probability $p(x,t+dt)$ can be unchanged, or can change due to
a movement down from $x+dx$ or a movement up from $x-dx$. The probabilities
of these are, respectively, $1-2\lambda$, $\lambda$ and $\lambda$. Thus we
get the relation
\begin{equation}
p(x,t+dt)=(1-2\lambda)p(x,t)+\lambda p(x+dx,t)+\lambda p(x-dx,t).
\label{Bachelier}
\end{equation}
Let $T$ be the tridiagonal infinite matrix $\{\lambda,1-2\lambda,\lambda\}$.
Then the row $T_{xy},y\in{\bf Z}$ is the conditional probability, that the
price will be $y$ at time $(N+1)dt$, given that it is $x$ at time $Ndt$.
In fact, $T$ is a stochastic matrix, which happens to be symmetric.

Suppose that at $t=0$ the price is $x_0$; then in time $Ndt$, the
price will follow the path $\gamma:=x_0\mapsto x_1\mapsto\ldots\mapsto x_N$
with probability 
\begin{equation}
p(\gamma)=T(x_0,x_1)T(x_1,x_2)\ldots T(x_{N-1},x_N).
\label{path}
\end{equation}
This is
called the random walk on ${\bf Z}$ determined by $T$, starting at $x_0$.
The set of allowed paths starting at $x_0$ is a finite subset of
$\Omega={\bf Z}^N$. $p(\gamma)$ is a probability on $\Omega$, and
the structure is called a Markov chain. An alternative point of view is to
start with $p_0\in\Sigma({\bf Z})$,
and to follow the path in $\Sigma({\bf Z})$ given by the time evolution.
By Bayes's law, the probability that at time $t=1$ the particle is at $x_1$
whatever its initial position, is
$p(x_1,1)=\sum_{x_0}p(x_0,0)T(x_0,x_1)$; this can be written as the matrix
product $p_0T$, where $p_t$ is a row vector made from the components of
$p(x_t,t)$.
By induction, the probability that at time $N$ the particle is at $x$
is $p_0T^N$. In this way, a Markov chain is described by a semi-group
of stochastic maps $T(t):=T^t$ acting on $\Sigma(\Lambda)$. Obviously
\begin{eqnarray}
T(0)&=&1\\
T(s)T(t)&=&T(s+t),\hspace{.4in} s,\;t\in {\bf N}.
\end{eqnarray}
One of the themes of probability theory is the relationship between a
semi-group of stochastic maps and a probabiliy on the corresponding
path space. The latter is called a {\em dilation} of the former.
Since $T$ is independent of time, the chain is said to be {\em
stationary}; if we limit the allowed space to be a finite set $\Lambda
\subseteq{\bf Z}$, we get a finite Markov chain, in which case
there is at least one stationary distribution $p^*$; this means that
$p^*T=p^*$, so that 1 is a left eigenvalue of $T$. If some power of $T$
has all its matrix elements positive, then the Perron-Frobenius theorem
tells us that $1$ is a simple eigen-value, and all the others have
modulus less than $1$. One can then show that $pT^n\rightarrow p^*$
as $n\rightarrow\infty$; the system converges exponentially
to equilibrium. We then say that the dynamics is mixing.
There are similar results in infinite dimensions, but
to get exponential convergence we need to show that there is a {\em
spectral gap}. This means that $1$ is simple and lies a finite distance
from the next eigenvalue of $TT^*$. To prove this in the case at hand is
usually the key to the study of the long-time behaviour. 
The Markov property is
that the probability of getting to $x$ at time $t+1$ depends only on
where the particle was at time $t$, and not on the previous path.
The study of Markov chains was started in the $19^{\rm th}$ century,
and is a huge subject.

Fick obtained an equation similar to (\ref{Bachelier}) for the
diffusion of particles in one dimension.
If $dx$ and $dt$ become small such that $(dx)^2/dt\rightarrow a$, a finite
limit, we say the system is following the diffusion limit. Rearranging,
and taking the diffusion limit,
Fick obtained the heat equation for the probability density, which we call
$\rho$:
\begin{equation}
\frac{\partial \rho}{\partial t}=\kappa\frac{\partial^2\rho}{\partial x^2}.
\label{heat}
\end{equation}
Here, $\kappa=a\lambda$. This is not a very good model of the market; apart
from the omission of drift, the gains in price should grow with the
overall price. As it is, negative prices are possible.

The heat equation (\ref{heat}) can be written
in the form of a conservation law:
\begin{equation}
\frac{\partial \rho}{\partial x}+\mbox{div}\,j(x,t)=0
\end{equation}
where $j(x,t)=-\kappa\nabla \rho$. At this stage,
mathematicians did not have the continuous version of the sample space
$\Omega$; this was to be Wiener's great construction.


In his celebrated work of 1905 \cite{Einstein1}, Einstein also used
(\ref{heat}) to describe the Brownian motion of small particles in a warm
liquid. He was mindful of Stokes's law of diffusion; this says that in a
viscous liquid a small particle under a constant force, such as gravity,
will increase its speed towards a terminal velocity $v$ say, which is
proportional to the force. Einstein required that in equilibrium
the current $v\rho$ due to this flow should balance the diffusion due to the
density gradient, so that steady state should obey
\begin{equation}
-\kappa\nabla \rho+v\rho=0.
\end{equation}
The solution to this in the case of gravity, where $v=-|v|$ in the
$z$-direction, is
\begin{equation}
\rho(x,y,z)=\mbox{const.}e^{-|v|z/\kappa},
\end{equation}
and this should be the 
Maxwell-Boltzmann law at the temperature $\Theta$ of the liquid,
\begin{equation}
\rho(x)=Z^{-1}e^{-mgz/(k_B\Theta)}.
\end{equation}
Einstein thus obtained the famous Einstein relation
\begin{equation}
F=k_B\Theta v/\kappa.
\end{equation}
His treatment is not
complete, since he omitted
the drift term in the diffusion equation! See \S 4, (1) in \cite{Einstein1}.
In a detailed
study, Smoluchowski \cite{Smol} wrote down the diffusion equation with drift
\begin{equation}
\frac{\partial \rho}{\partial t}=\kappa\frac{\partial \rho}{\partial x^2}-
v\frac{\partial \rho}{\partial x}.
\label{heat2}
\end{equation}
now known as the Smoluchowski equation, and is a special case of the
Fokker-Planck or backward Kolmogorov equation. He solved this by
using the method of images for several systems with
boundaries, such as the mass of air above the ground, and obtained
the approach to the stationary state expected by Einstein. 

It was known that one can solve eq.~(\ref{heat2}) exactly,
to fit a more or less arbitrary initial function $\rho(x,0)=f(x)$, by using
the Green function (in one dimension)
\begin{equation}
G(x,t):=[4\pi\kappa t]^{-(1/2)}e^{-(x-vt)^2/(4\kappa t)}.
\end{equation}
This satisfies eq.~(\ref{heat2}), and converges in the sense of
distributions to the Dirac $\delta$-function as $t\rightarrow0$. Then
\begin{equation}
\rho(x,t)=\int_{-\infty}^\infty G(x-y,t)f(y)\,dy
\label{solution}
\end{equation}
satisfies eq.~(\ref{heat2}) and the boundary condition.
The operator whose kernel is $G$ is the continuum analogue of the matrix
$T^n$ of the Markov chain, 

When the force and
temperature are slowly varying, we get the coupled system
\begin{eqnarray}
\frac{\partial \rho}{\partial t}&+&\mbox{div}\,J=0;\\
\frac{\partial\Theta}{\partial t}&=&\kappa^\prime \mbox{div}\,\nabla\Theta+\kappa
{\bf F}.{\bf F}/k_B\Theta.
\end{eqnarray}
Here, $J(x,t)=-\kappa\nabla(\rho+V\rho/\Theta)$, where $V$ is the potential
giving rise to the force $F$. The source term in the heat equation is $F.J$,
the power of the external force supplied to the particle, all of which is
converted into heat. This system obeys
the first and second laws of thermodynamics \cite{RFS1}.

Consider now the solution (\ref{solution}) to (\ref{heat2}). Because $G$ is positive,
the density remains positive for all time, and the conservation law shows
that the integral of $\rho$ over space is constant. So we get a flow
through the space of probabilities. The question arises, is there a
process in continuous time associated with the Smoluchowski equation?
The answer is yes, and this was the result of the work of Wiener,
and later, Ito. An alternative idea was introduced by Langevin, who
considered Newton's laws, in which a part of the external
force, denoted $F$, is random; friction enters as a damping
force proportional to the velocity, parametrised by $\gamma>0$.
Thus his equation is
\begin{equation}
\frac{d^2x}{dt^2}=-\frac{\partial V}{\partial x}-\gamma\dot{x}+F(t).
\label{Langevin}
\end{equation}
This is the equation for a single particle, but as $F$ is random, the
position
$x(t)$ becomes random as time goes by, even if its initial condition is
given. Statistical properties of $x$ are determined by those of $F$; the
relation of these to the Smoluchowski equation were studied by Fokker and
Planck, but were fully understood only in terms of stochastic calculus.
One might assume that $F$ is Gaussian distributed, and is of mean zero,
with independent values at different times. This would now be described as
{\em white noise}. Langevin's work started the enormous field of
stochastic differential equations.



In 1904 Lebesgue tried to set up a general theory in which every subset
of $[0,1]$ is assigned a measure.
\cite{Legesgue}. 
The very next year, G. Vitale showed that the scheme was inconsistent
\cite{Vitale}. Hausdorff \cite{Hausdorff} and Banach and Tarski,
showed that the measure could not be additive \cite{Williams}. The point
is that some
sets are so bad they cannot be assigned a measure, even a finitely
additive one. This led to the concept of measurable set. Let us start with
the Borel measurable sets on $[0,1]$.

Let $\Omega=[0,1]$; let us say that a collection ${\cal B}$ of subsets
of $\Omega$ form a {\em tribe} if
\begin{enumerate}
\item $\Omega\in{\cal B}$
\item whenever $B\in{\cal B}$, we have $B^c:=\Omega-B\in{\cal B}$;
\item whenever $A\in{\cal B}$ and $B\in{\cal B}$, we have $A\cup B\in
{\cal B}$.
\end{enumerate}
Such a collection of subsets is also called a Boolean ring, or a Boolean
algebra. The collection ${\cal B}$ is
actually a ring, with multiplication given by intersection, and addition
given by symmetric difference, that is $A+B:=A\cup B-A\cap B$. It is
also an algebra in the technical modern sense, but trivially in that any
ring is an algebra over the field
consisting of two numbers, $0$ and $1$. Since this ring structure plays no
role in the theory, we prefer not to furnish ${\cal B}$ with the extra
structure `+', and will use the word `tribe' instead.

We define a
$\sigma$-tribe to be a collection ${\cal B}$ of sets $B_i\subseteq\Omega$
such that 3. above is replaced by\\
$\;\;3_\infty.\mbox{ if }B_i\in{\cal B} \mbox{ is a countable family of disjoint
sets, then }\cup B_i\in{\cal B}.$\\
The set of all subsets of a set $\Omega=[0,1]$ is obviously a $\sigma$-tribe,
and indeed satisfies uncountable additivity as well. This $\sigma$-tribe is
called the {\em power set} of $\Omega$. But, as we saw, there are no
useful definitions of measure on the power set. Another easy case is the
collection of all countable subsets of $\Omega$: the union of
a countable collection of countable subsets is countable. However, any
countable set has length zero, since it can be covered by a sequence
of intervals of length $\leq\epsilon/2,\;\epsilon/4\;\epsilon/8,\ldots$, of total
length $\epsilon$. Since $\epsilon$ can be anything, the set has length zero.
To get some sets of non-zero length, let us consider the tribe ${\cal B}_0$
of all finite disjoint unions of open, closed and half-open intervals. We
could add to ${\cal B}_0$ all countable unions of sets in ${\cal B}_0$, and
all complements in $\Omega$ of sets in the tribe so obtained. Call this
${\cal B}_1$. Then we would need to consider the
collection of countable unions of sets in ${\cal B}_1$, and their
complements, to get a new tribe ${\cal B}_2$, and so on. Does this end
up with a well-defined $\sigma$-tribe?
The following argument does the trick. Let ${\cal G}$ be any $\sigma$-tribe
containing all sets in ${\cal B}_0$, and let $C$ be the set of all
such $\sigma$-tribes. Then $C$ is non-empty, as it
contains the power set at least. Then form
\begin{equation}
{\cal B}=\bigcap_{{\cal G}\in C}{\cal G}.
\end{equation}
That is, ${\cal B}$ contains those subsets of $\Omega$ that lie in all
$\sigma$-tribes ${\cal G}$, and no other subsets. In particular, ${\cal B}$
contains all subsets in ${\cal B}_0$, ${\cal B}_1$ etc. In fact, by using
the techniques of set theory, one can prove that ${\cal B}$ is
smallest $\sigma$-tribe containing all the open
intervals in $\Omega=[0,1]$; it is called the {\em Borel tribe}. One can ask
whether we have arrived at the power set after all, or have something
without the pathological sets. That ${\cal B}$ contains only nice sets
follows the construction of a countable measure on its sets, namely
the Lebesgue measure.


A {\em finitely additive measure} on a tribe ${\cal B}$
is a map $\mu:{\cal B}\rightarrow {\bf R}_+\cup\{+\infty\}$ such that
\[\mu(A\cup B)=\mu(A)+\mu(B)\mbox{ for all disjoint }A, B\in{\cal B}.\]
If $\mu(\Omega)=1$, it is a finitely additive probability measure.
To do analysis, we must be able to take some limits, and so we
now assume that ${\cal B}$ is a $\sigma$-tribe.

A probability measure on $(\Omega,{\cal B})$ is a map $\mu:{\cal B}
\rightarrow{\bf R}^+$ such that
\begin{enumerate}
\item $\mu(B)\geq0$ for all $B\in{\cal B}$;
\item $\mu(\Omega)=1$;
\item if $B_i$ is a countable collection of disjoint sets in ${\cal B}$,
then
\[\mu(\cup B_i)=\sum_i\mu(B_i).\]
\end{enumerate}
Considering the tribe ${\cal B}_0$ of finite unions of disjoint open,
closed
and half-open intervals, we can define the {\em Lebesgue measure} of
$B\in{\cal B}_0$ to be the sum of the usual lengths of the intervals
involved. It is then proved that there is a countably additive measure
on the Borel $\sigma$-tribe, which agrees with the length on the intervals.
This measure is called the {\em Lebesgue} measure.

It is sometimes useful to extend the concept of measure to unbounded sets
such as ${\bf R}$, whose total length is infinite. For this, we just
drop axiom 2. above.

So much for the measure; integration theory needs a remark as well.
Suppose that we have a function $y=f(x)$, where $x\in[0,1]$ and
$y$ is real-valued and bounded,
and we seek a way of finding the area under the graph of $y$ against
$x$. In Riemann's {\em method of integration} we divide the $x$-axis
into a large number of small intervals, $[0,x_1],\,(x_1,x_2],\ldots,
(x_N,1]$, and define $y_i$ to be the smallest value of $y$ in the
interval $(x_i,x_{i+1}]$ and $Y_i$ to be the largest value.
Now define the two approximations
to the area, known as the upper sum and the lower sum,
$R^+=\sum_i Y_i(x_{i+1}-x_i)$ and $R^-=\sum_i y_i(x_{i+1}-x_i)$.
As we refine the subdivision, $R^+$ decreases
and $R^-$ increases. If the limits of these are equal, we say that the
function is Riemann-integrable, and take
their common value as the area under the curve $y=f(x),\;\;0\leq x\leq1$.
One shows that continuous functions are
integrable, and can establish the fundamental theorem of the
calculus; a generalisation, called the Riemann-Stieltjes integral,
can be defined, if we replace $x_{i+1}-x_i$ by $P(x_{i+1})-P(x_i)$, where
$P$ is an increasing function of bounded variation, continuous from the left.
We write the integral as $\int y(x)dP(x)$.
To define the integral of unbounded functions, various limiting methods were
invented. The theory is not really satisfactory.

Lebesgue introduced a new form of integration: compared
with Riemann's method, it is done
the other way round. As the first step, only positive functions are
considered. Then, we divide the $y$-axis into intervals $([0,y_1],
\,(y_1,y_2],\ldots,(y_N,\infty))$, and for each interval, look for the
inverse image of each interval under the map $f$. That is, we
consider the subset of the $x$-axis consisting of $x$ such that $f(x)\in
(y_i,y_{i+1}]$. This set, denoted by $f^{-1}(y_i,y_{i+1}]:=B_i$, may
consist of many pieces, and so will not
always be an interval. We require, however that it should be a set in
the Borel $\sigma$-tribe ${\cal B}$; if this holds for every
subdivision of the $y$-axis into intervals, we say that the function $f$
is ${\cal B}$-{\em measurable}. The set $B_i$ will have a `length',
namely, its Lebesgue measure, $\mu(B_i)$.
We approximate the area under the graph of $f$ 
by the sum
\[ L(f):=\sum_i y_i\mu(B_i).\]
This is positive and increases as we refine the partition
of the $y$-axis. If its supremum over all partitions is finite, we say
that $f$ is Lebesgue-integrable, and write
\begin{equation}
\int_0^1 f(x)dx=\sup L(f).
\end{equation}
We can integrate functions that are not positive, provided that the
positive and negative parts are separately integrable, and we integrate
complex functions by treating the real and imaginary parts separately.
This generalises the Riemann integral in that any Riemann-integrable
function is Lebesgue integrable, and then both versions give the same
answer. 

Lebesgue integration has the following easy generalisation, which is
important for
probability. Suppose that $\Omega$ is any set, provided with a
$\sigma$-tribe ${\cal B}$; the pair $(\Omega,{\cal B})$ is called
a {\em measurable space}.
A real-valued function is said to be ${\cal B}$-measurable
if the inverse image of every open interval lies in ${\cal B}$:
\[f^{-1}(y_1,y_2):=\{\omega\in\Omega:y_1<f(\omega)<y_2\}\in{\cal B}.\]

A random variable is then simply a real-valued ${\cal B}$-measurable
function on $\Omega$. Given a measure $\mu$ on $(\Omega,{\cal
B})$, not necessarily of finite total measure, we can
regard as the same random variable $f$ two that differ only on
a set of $\mu$-measure zero; they are called {\em versions} of $f$.
The set of all bounded random variables forms a commutative {\em
algebra} ${\cal A}(\Omega)$ with norm $\|f\|_\infty:=\inf\,\sup_\omega
|f(\omega)|$; here, $\inf$ is taken over all versions of $f$.
The sets in ${\cal B}$ are called events. The integral
of a positive measurable function (with respect to the measure $\mu$) is
defined
similarly to the case when $\Omega={\bf R}$. If $\mu$ is a probability
measure, this integral is called the mean $\mu.f$ of $f$ in the state $\mu$.
and if $\mu.|f|<\infty$ we write $f\in L^1(\Omega,{\cal B},\mu)$.
More generally, we write $f\in L^p(\Omega,{\cal B},\mu)$, $1\leq p<\infty$
if $f$ is ${\cal B}$-measurable and $|f|^p$ is integrable. These are Banach
spaces with norm $\|f\|_p:=(\int |f(\omega)|^p\,d\mu)^{1/p}$.
The probability of an event $B$ is taken to be $\mu(B)$.
Each measure $\mu$ defines an element
of the dual space of ${\cal A}$ by the linear form $f\mapsto \int f\,d\mu$.

We have remarked that the original motivation for introducing the
$\sigma$-tribe was to avoid pathology. However, the concept has been very
useful in a heuristic way, to describe the {\em information}
carried by events and observables
in a random theory based on a measure space $(\Omega,{\cal B},\mu)$;
in particular, it is useful to consider a sub-tribe
or sub-$\sigma$-tribe, of ${\cal B}$. Suppose that $B\in{\cal B}$ is an
event; it is determined by its {\em indicator function}, $\chi_{_B}(\omega)$
which is $1$ if $\omega\in B$ and zero outside $B$. If $\mu(B)\neq 0,1$
and $f$ is measurable, we can define the conditional expectation
\begin{equation}
E[f|B]=\sum_\omega f(\omega)\mu(\omega|B).
\end{equation}
We may also find the conditional probability of $A\in{\cal B}$, given
that $B$ did not happen: $\mu(A|B^c)=\mu(A\cap B^c)/\mu(B^c)$, and the
corresponding conditional expectation
\begin{equation}
E[f|B^c]=\sum_\omega f(\omega)\mu(\omega|B^c).
\end{equation}
We may regard the pair of numbers, $\{E[f|B],E[f|B^c]\}$ as defining
a simple measurable function on $\Omega$, equal to $E[f|B]$ if
$\omega\in B$ and to $E[f|B^c]$ if $\omega\notin B$. Let us now
generalise this idea. Let $B_1,\ldots,B_n\in{\cal B}$ be
disjoint measurable sets such that $\mu(B_j)\neq 0$ for all $j$, and
$\mu(\cup_j B_j)=1$. These sets generate a tribe, say ${\cal B}_0$
(by various unions; there are $2^n$ such unions). If $f$ is measurable,
the functions on $\Omega$ defined by
\begin{equation}
F_f(\omega)=E[f|B_j]\;\mbox{ if }\omega\in B_j
\end{equation}
are measurable relative to ${\cal B}_0$. They take constant values,
$E[f|B_j]$ on each $B_j$ and so can be written
\begin{equation}
F_f(\omega)=\sum_j\chi_j(\omega)c_j\;\mbox{ where }c_j=E[f|B_j].
\end{equation}
Conversely, every function $F$, measurable relative to ${\cal B}_0$,
has this form for some $\{c_j\}$. The map, $f\mapsto F_f$, is linear
and is called the {\em conditional expectation} of $f$ given
${\cal B}_0$. This map leaves invariant the
vector space of ${\cal B}_0$-measurable functions, and indeed
is the orthogonal projection of $L^2(\Omega,{\cal B},\mu)$ onto
$L^2(\Omega,{\cal B}_0,\mu)$.

The tribe ${\cal B}_0$ tells how
fine was the division into the sets $B_j$, and determines how
much detail can be obtained from the functions that are
${\cal B}_0$-measurable. From the fact the $F$ is the orthogonal projection,
we see that $E[f|{\cal B}_0]$ is the best approximation (in the $L^2$-sense)
to $f$ by functions that are ${\cal B}_0$-measurable.

Consider for example the price
of a stock at time $t$, where $t$ is a non-negative
integer; $S(t)_{t\geq0}$ are then a family of
random variables on $\Omega$, and while we can find out the prices
up to the present time, we cannot know the future. Suppose that $t=N$ is
the present. The {\em information}
contained in the knowledge of the prices at $N+1$ previous times, namely
$S(t=0)=s_0,\,S(t=1)=s_1,\,\ldots,S(t=N)=s_N$, selects in $\Omega$ a
particular level set of these functions: this is the event
\[\{\omega\in\Omega:S(0)(\omega)=s_0\ldots S(N)(\omega)=s_N\}\]
Since we assume that $S(t)$ are ${\cal B}$-measurable, this set lies in
${\cal B}$, by the intersection property. The same for any other possible
set of values of these observations. There is a smallest $\sigma$-tribe
with respect to which all these functions are measurable, and in fact,
this $\sigma$-tribe is generated by all the level sets described above.
Call this ${\cal B}_{\leq N}$. The set of all random variables that
are ${\cal B}_{\leq N}$-measurable is exactly the set of functions of the
data $S(0),\ldots,S(N)$, measurable in the Lebesgue sense; they can
therefore be computed from the data we have access to.

The increasing family $\{{\cal B}_{\leq n}\}$ is called the filtration
generated by the process.
It provides a neat formulation of the Markov condition for a process $X_n$;
let ${\cal B}_n$ be the $\sigma$-tribe generated by the r. v. $X_n$.
Then a process is called Markovian if
\begin{equation}
E[X_n|{\cal B}_{\leq m}]=E[X_n|{\cal B}_m]\mbox{ if }n\geq m.
\end{equation}
The idea is that the information contained in $X_m$, the present value,
tells us as much about the future as the whole previous history.
Consider again the semigroup $\{T^n\}$ of stochastic maps, acting on
$\Sigma({\bf Z})$ in one time-step as in eq.~(\ref{Bachelier}).
One can check that if $p_0$ is the initial probability distribution of
the initial point of the path, then
\begin{equation}
p_0T^n=E_{p_n}[x_n|{\cal B}_0].
\label{KBE}
\end{equation}
Here, $\gamma=(x_0,\ldots,x_n)$ and $p_n(\gamma)=p_0(x_0)p(\gamma)$
where $p(\gamma)$ is as in (\ref{path}).

Wiener \cite{Wiener} was able to put the Bachelier-Einstein diffusion
theory on a rigorous footing. He has to define, first, the sample space
$\Omega$; then
he needs a $\sigma$-tribe ${\cal B}$ and a measure on it; he also
needs a family of ${\cal B}$-measurable functions $X_t(\omega)$ whose
distribution has density of probability equal to $\rho(x,t)$ obeying the
diffusion equation. Finally, he needs to get the continuum version of
eq.~(\ref{KBE}).

Let $\Omega$ be the set of all continuous functions $\omega$ of $t\geq 0$
with $\omega(0)=0$;
these are called `Brownian paths'. Let $(x_1,y_1)$ be an interval of the
real line, which we call a {\em gate}; we now
consider the subset of paths which pass through the gate at time $t_1$. This set is called
the {\em cylinder set based on} $(x_1,y_1)$.
In symbols, it is
\[\{\omega\in\Omega:x_1<\omega(t_1)<y_1\}\]
The $\omega(t)$ for various $t$ are
coordinates of the point $\omega$; we have a condition
on only one of the coordinates; the rest run over the real line. Consider
another cylinder set, similarly constructed at time $t_2>t_1$, based on
another open interval $(x_2,y_2)$. The intersection of these sets is a
cylinder set based on rectangle $(x_1,y_1)\times(x_2,y_2)$ in the plane
made by the coordinates $\omega(t_1),\omega(t_2)$. The path $\omega(t)$
passes through the first gate at time $t_1$ and the
second at time $t_2$; it is a slalom. Consider the collection of subsets
of $\Omega$ consisting of all these cylinder sets defined by slaloms with
any finite number of gates, at any selection of different positive times.
The finite unions of these form a tribe. The smallest $\sigma$-tribe
${\cal B}$ containing all these is the one we choose, so obtaining
the measurable space $(\Omega,{\cal B})$.

We first define a finitely additive measure on the tribe of cylinder sets.
It is enough
to give the measure of a general cylinder set, and to use the finite
additivity. Starting at $x=0$, the probability density
that a diffusing particle reaches $x_1$ at time $t_1$ is taken to be the
Gaussian given by the Green function; thus the probability of lying in the
interval $x_1,y_1$ is
\begin{eqnarray}
\mbox{Prob}\{\omega(t_1)\in(x_1,y_1)|\omega(0)=0\}&=&\frac{1}{(4\pi\kappa t_1)
^{1/2}}\int_{x_1}^{y_1}e^{-x^2/(4\kappa t_1)}dx\nonumber\\
&=&\int_{x_1}^{y_1}G(x,t_1)dx.
\label{gate1}
\end{eqnarray}
The probability that the path goes through two gates, $(x_1,y_1)$ at $t_1$
and $(x_2,y_2)$ at $t_2$ is defined to be
\begin{eqnarray}
\mbox{Prob}\{\omega(t_1)&\in&(x_1,y_1)\mbox{ and }\omega(t_2)\in(x_2,y_2)|
\omega(0)=0\}\nonumber\\
&=&\int_{x_1}^{y_1}dx\int_{x_2}^{y_2}dx^\prime G(x,t_1)G(x^
\prime-x,t_2-t_1).
\label{gate2}
\end{eqnarray}
This can be interpreted as Bayes's theorem, in which $G$
is the conditional probability density.
Similarly, the probability of any cylinder set, based on a finite set of
gates, can be given. The probability is the same, whether the gates are
open, closed or half-open. We would like the measure we are constructing
to be at least finitely additive. Thus we take the measure of the union
of two disjoint cylinder sets to be the sum of the measures we have
just given them individually. A possible problem arises if we add together
infinitely many gates at time $t_1$ to make up the whole line; for, we
would like our measure to be countably additive, and we need the {\em
consistency} condition between the two ways to define the probability
of reaching the gate $(x_2,y_2)$: from $0$ directly, with no gate at $t_1$,
as given by eq.~(\ref{gate1}), 
or as the sum over all paths going through any complete set of disjoint
gates at $t_1$, as got by summing eq.~(\ref{gate2}). Indeed, we do
get the same answer, because of the propagating property of $G$:
\begin{equation}
\int_{-\infty}^{\infty}dx^\prime\;G(x-x^\prime,t_1)G(x^\prime-y,t_2-t_1)=
G(x-y,t_2).
\end{equation}
This is a continuous version of the obvious property of the stochastic
matrices $T^n$ of a Markov chain, namely $T^mT^n=T^{m+n}$, in which the
matrix product, expressed as the sum over an intermediate index, is
replaced by the integral over the point $x^\prime$. Thus our equation
just expresses the semi-group property of the time-evolution of a
first-order equation, here the heat equation.
It is seen here as the main point which establishes the additivity
of the finitely additive measure we have constructed on the tribe of
cylinder sets.

Let us define ${\cal B}_{[s,t]}$ as the $\sigma$-tribe generated by the
cylinder sets labelled by times in the interval $[s,t]$, and ${\cal B}$
that generated by all of these. Then Wiener proved that there exists
a unique measure on the measurable space $(\Omega,{\cal B})$ that coincides
with the measure above on the tribe of unions of such cylinder sets.
This measure is now called {\em Wiener measure}.
The Wiener process starting at $0$ is then the family of random variables
defines by $W_t(\omega):=\omega(t),\hspace{.1in}t\geq0$.
The process has the following properties:
\begin{enumerate}
\item $W_t-W_s$ is Gaussian with mean zero and variance $t-s$, for $t>s$.
\item $W_t-W_s$ is independent of $W_v-W_u$ if $0\leq u\leq v\leq s\leq t$.
\item $W_0=0$.
\end{enumerate}
These properties characterise the process.
By requiring that $W^x_0=x$ we get the Wiener process $W_t^x$ 
starting at $x\in{\bf R}$.


We now need the concept of the {\em symmetric Fock space} $\Gamma({\cal H})$
of a Hilbert space ${\cal H}$. The $n$-fold tensor product $\otimes^n{\cal
 H}$ is the completed span of the symbols $\otimes_{i=1}^n\psi_i=\psi_1
\otimes\ldots\psi_n$ with the scalar product
\[ \langle\otimes\psi_i,\otimes\phi_i\rangle:=\prod_{i=1}^n
\langle\psi_i,\phi_i\rangle.\]
The symmetric tensor product ${\cal H}^n:=\otimes_S^n{\cal H}$
is the subset of symmetric
tensors, called the $n$-particle space; the zeroth tensor power is taken to
be ${\bf C}$. The Fock space
$\Gamma({\cal H})$ is the direct sum $\oplus_{n=0}^\infty{\cal H}^n$.
This has the functorial property
\[ \Gamma({\cal H}_1)\otimes\Gamma({\cal H}_2)=\Gamma({\cal H}_1)
\oplus\Gamma({\cal H}_2).\]
As a special case, $\Gamma({\bf C})={\bf C}\oplus{\bf C}\oplus\ldots$.

There is a unitary map $L^2(\Omega,{\cal B},\mu)\rightarrow\Gamma
(L^2([0,\infty),dt)$, in such a way that $\oplus_{j=0}^n{\cal H}^j$
is identified with the $L^2$-completed span of the polynomials in $W_t$
of degree $\leq n$ \cite{Segal}. The $n$-particle space is then identified successively
by Gram-Schmidt orthogonalisation with the part orthogonal to the
$k$-particle spaces, $k<n$. This is Wiener's {\em chaos expansion}
\cite{Wiener2}.
In particular, the one-particle space is spanned by $\{W_t\}_{t\geq0}$.

For each fixed $t$, the space $L^2(\Omega,{\cal B},\mu)$ contains the
random variables $I$, $W_t$, ...$W^n_t$...They can act as multiplication
operators successively on the vector $1$, to get $n$ vectors.
Suppose we orthogonalise them by
the Gram-Schmidt procedure. Since $W_t$ is Gaussian, we get the Hermite
polynomials in successive spaces, and any $L^2$ function of $W_t$
has a convergent expansion as a sum of its components in these spaces.
The subspace we get can
be identified as the Fock space over the one-dimensional space spanned by
$W_t$. We shall see that these  polynomials are Wick-ordered powers
\cite{Wick} of $W_t$, and that they are martingales.

Now suppose that $u>s$; since $W_u$ is independent of $W_u-W_s$, and they
are Gaussian, they are orthogonal in the one-particle space, which
can thus be written as the direct sum $L^2([0,\infty),dt)=L^2([0,s),dt)
\oplus L^2([s,\infty),dt)$. By the functorial property of Fock space, we therefore
can write 
\begin{equation}
L^2(\Omega,{\cal B},\mu)=L^2(\Omega,{\cal B}_{[0,s]},\mu)\otimes
L^2(\Omega,{\cal B}_{\geq s},\mu).
\end{equation}
We can similarly split Fock space into arbitrarily many factors,
corresponding to any partition of the time axis into intervals:
it has the property of a continuous tensor product.

The continuous analogue of the semigroup $(\gamma T^n)_m:= \gamma_{m+n}
,\;m,n=0,1,2\ldots$ of the random walk is the {\em left-shift} of the paths:
$(\omega T_s)(t)=\omega(s+t)$. This induces the dual action on the
observables:
\begin{equation}
T^*_s:L^2(\Omega,{\cal B},\mu)
\rightarrow L^2(\Omega,{\cal B}_{\geq s},\mu),\;s\geq 0.
\label{shift}
\end{equation}
\noindent This operator is isometric but not invertible. We can also embed
$L^2(\Omega,{\cal B},\mu)$ in the two-sided space
$\Gamma(L^2(-\infty,\infty))$, on which the left shift is unitary, and
induces the action of the group ${\bf R}$ rather than the semigroup
${\bf R}^+$.
In that case the paths are not conditioned to pass through the origin, and
only the differences, $W_t-W_s$ make sense as vectors or operators.



\input new20002
