\section{Hallucinations}
\label{sec:hallucination}

\varun{still formalizing this bit; worst case it can be removed}

Hallucinations are phenomena demonstrated by LLMs where the model has the propensity to generate realistic data which is inaccurate. Formally speaking, we define hallucinations as generated content that are {\em incorrect} w.r.t some external source of information. In the context of text summarization (where the objective is to summarize an input document), hallucinated statements are those that are not present (or can be reasonably derived from statements) in the input document. Note that even if the generated content is {\em factually accurate}, it would be a hallucination in this setting. Similarly, in open-world question answering (Q+A) systems which are powered by parametric knowledge (which aims to replicate normative knowledge) or external corpora (such as those available on the web), hallucinations are statements that are unsubstantiated in this knowledge bank.

In this section, we wish to understand how much \DV hallucinates compared to prior models in tasks related to summarization and open-world Q+A. We then discuss how this can be detected and/or minimized, and aim to shed some light on the origin of this behavior.

\input{contents/9.1_hal_comparison}
\input{contents/9.2_hal_minimization}