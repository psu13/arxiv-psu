\begin{Verbatim}[commandchars=\\\{\}]
I want to write a customized optimizer in pytorch, the optimizer should do:
1. the parameters for the optimizer is learning rate, momentum ,weigth decay, d\PYGZus{}dim, k and factor alpha
2. the optimizer goes through each parameter in the network, and then
\PYGZhy{}\PYGZhy{} Get the gradient of this parameter, reshape it to 2D by looping through the dimensions, and keeping the dimension whose actual dimension (shape) is equal to d\PYGZus{}dim, do an SVD decomposition of the parameter so W = U\PYGZbs{}Sigma V.
\PYGZhy{}\PYGZhy{} Keep the top k singular vectors of W, so we get a new matrix W\PYGZus{}k = U \PYGZbs{}Sigma\PYGZus{}k V.
\PYGZhy{}\PYGZhy{} Keep the top 2k singular vectors of W, so we get a new matrix W\PYGZus{}\PYGZob{}2k\PYGZcb{} = U \PYGZbs{}Sigma\PYGZus{}\PYGZob{}2k\PYGZcb{} V.
\PYGZhy{}\PYGZhy{} Normalize W\PYGZus{}k according to the momentum of the F\PYGZhy{}norm of W\PYGZus{}\PYGZob{}2k\PYGZcb{}.
\PYGZhy{}\PYGZhy{} Truncate each coordinate of W\PYGZus{}k to the top alpha  percentile of all the coordinates, in absolute value.
\PYGZhy{}\PYGZhy{} Apply momentum on W\PYGZus{}k, and then update the network parameters using this momentum.
\PYGZhy{}\PYGZhy{} Apply weight decay.
\end{Verbatim}
