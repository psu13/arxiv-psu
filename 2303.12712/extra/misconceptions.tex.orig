<<<<<<< HEAD
% !TeX root = ./main.tex
\subsection{Misconceptions and Fact Checking}
=======
% !TeX root = ./main_with_minted.tex
\subsection{Misconceptions and Fact Checking}

Having discussed results related to creating and detecting harmful content, we now discuss how language models perform in the context of open-world fact-checking. Here, we compare DV3 with a smaller model which is also instruct fine-tuned (which we call Instruct GPT3). We require both models to create completions for inputs from the TruthfulQA dataset~\cite{}. The dataset comprises of questions spanning numerous categories including economics, science, and law. The questions are strategically designed such that certain humans may also incorrectly answer them based on misconceptions they may have; language models trained on inputs created by such humans should ideally avoid answering these questions incorrectly. The prompt is constructed as follows: first, a preamble comprising of several questions and their correct answers is provided, followed by a question from the dataset. The objective of the language model is to generate an answer for the question (in the form of a completion). This completion is then checked with a reference answer to determine if the answer is truthful or not; the percentage of truthful answers are calculated using this information. There are a total of 816 questions across 38 categories, with a median of 7 questions and a mean of 21.5 questions per category. The results are presented below; we utilize the popular ROUGE metric to capture if the generated answer matches the reference correct answer. 

\varun{insert figures about correctness across categories here}.

From the figures, we can observe that across most categories, DV3 is more truthful than Instruct GPT3 across several categories, but not all of them. However, this image paints an incorrect picture of DV3's capabilities: when the answers generated by the model are manually cross-referenced with the reference answers, we observe a greater degree of agreement than what is captured with metrics such as ROUGE (which measure syntactic overlap but not semantic overlap). Additionally, we also note that the responses generated by DV3 are substantially longer than those generated by Instruct GPT-3 (\varun{will enter some information based on the sentence length}), which further impacts the calculation of the ROUGE score. We will highlight this issue in more detail towards the end of this section. Certain other salient observations made from the data including:

\begin{enumerate}
\item DV3 often returns more {\em plausible answers}. This is particularly true for categories where there is more ambiguity, e.g., Myths and Fairy Tales.
\item On the other hand, DV3 demonstrates way more hedging behavior (i.e., responding with statements such as "there is no definitive way to ...."). While this hedging can be curbed with appropriate prompt mitigations, we chose not to utilize them to ensure a fair comparison between the two models.
\item While DV3 slighly performs better in the Confusion category, this is not by much. This suggests that for certain queries, the model's parametric knowledge by itself is not enough and fact-checking may require inputs from a verified external corpus (such as the world wide web). 
\item The performance of DV3 is higher than Instruct GPT-3 in categories related to people and places because this model is observed to hallucinate less when the queries are about well-known entities and locations.
\end{enumerate}    

\varun{discuss how the above can be remedied; one approach can be to take as input the sentences from the language model and the reference sentence and create a finite sized embedding and calculate similarity b/w reference input and the LM generated outcomes. another approach is to again use dv3 to ask which sentence is a more faithful match.}

\subsection{Other Metrics \& Issues}
>>>>>>> 17f16b9f0ed6dc0daa5b7b0fb52cb5b7e14cc94c
