\addvspace {10\p@ }
\contentsline {figure}{\numberline {1}{\ignorespaces Halley's life table}}{10}{chapter*.2}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2}{\ignorespaces Illustration of shifted gaussians}}{24}{subsection*.12}%
\contentsline {figure}{\numberline {3}{\ignorespaces The ROC curves for various signal to noise ratios in the needle in the haystack problem.}}{31}{subsection*.21}%
\contentsline {figure}{\numberline {4}{\ignorespaces Two ROC curves with the same AUC. Note that if we constrain \(\mathrm {FPR}\) to be less than 10\%, for the blue curve, \(\mathrm {TPR}\) can be as high as 80\% whereas it can only reach 50\% for the red.}}{33}{subsection*.24}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5}{\ignorespaces Illustration of the perceptron update. Left: One misclassified example \(x.\) Right: After update.}}{46}{section*.34}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6}{\ignorespaces Sample questions from the COMPAS questionnaire.}}{57}{subsection*.42}%
\contentsline {figure}{\numberline {7}{\ignorespaces A cartoon classification problem for polynomial classification. Here, the blue dot denotes the center of the displayed circle.}}{61}{subsection*.53}%
\contentsline {figure}{\numberline {8}{\ignorespaces Radial Basis Function approximation of sin(x). We plot the four Gaussian bumps that sum to approximate the function.}}{64}{subsection*.55}%
\contentsline {figure}{\numberline {9}{\ignorespaces Creating a step function from ReLUs. Here, c=1/4.}}{68}{subsection*.57}%
\contentsline {figure}{\numberline {10}{\ignorespaces Comparison of the Gaussian and Arccosine Kernels. Plotting the kernel value as a function of the angle between two unit norm vectors.}}{71}{subsection*.58}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {11}{\ignorespaces Examples of minima of functions. In the first illustration, there is a unique minimizer. In the second, there are an infinite number of minimizers, but all local minimizers are global minimizers. In the third example, there are many local minimizers that are not global minimizers.}}{77}{Definition.3}%
\contentsline {figure}{\numberline {12}{\ignorespaces Convex vs nonconvex functions.}}{77}{Definition.4}%
\contentsline {figure}{\numberline {13}{\ignorespaces Examples of stationary points.}}{78}{Proposition.4}%
\contentsline {figure}{\numberline {14}{\ignorespaces Tangent planes to graphs of functions are defined by the gradient. These hyperplanes always fall below the graphs of convex functions.}}{79}{Proposition.6}%
\contentsline {figure}{\numberline {15}{\ignorespaces Example loss functions for classification. The zero-one loss is plotted in red for comparison.}}{82}{subsection*.67}%
\contentsline {figure}{\numberline {16}{\ignorespaces Plot of the different increments of \(\genfrac {}{}{}1{1}{2n} \DOTSB \tsum \slimits@ _{i=1}^n (w-y_i)^2\). The red star denotes the optimal solution.}}{87}{subsection*.71}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {17}{\ignorespaces The so-called bias-variance trade-off represents a traditional view of generalization consistent with uniform convergence bounds}}{107}{subsection*.87}%
\contentsline {figure}{\numberline {18}{\ignorespaces An extended picture with a ``double descent'' shape accounting for the overparameterized regime.}}{107}{subsection*.87}%
\contentsline {figure}{\numberline {19}{\ignorespaces Randomization test on CIFAR-10. Left: How randomization affects training loss. Training still converges to zero loss even on fully randomized labels. Right: How increasing the fraction of corrupted training labels affects classification error on the test set. At full randomization, the test error degrades to \(90\%\), as good as guessing one of the \(10\) classes.}}{109}{subsection*.88}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {20}{\ignorespaces Parameter divergence on AlexNet trained on Imagenet. The two models differ only in a single example.}}{142}{subsection*.118}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {21}{\ignorespaces A sample of MNIST digits}}{154}{subsection*.128}%
\contentsline {figure}{\numberline {22}{\ignorespaces The adaptive analyst model. The risk estimate need not equal the empirical risk.}}{159}{subsection*.131}%
\contentsline {figure}{\numberline {23}{\ignorespaces Constructing a tree of depth \(k\) and degree \(n+1\) given an adaptive analyst. Each node corresponds to the classifier the analyst chooses based on the responses seen so far.}}{160}{Proposition.9}%
\contentsline {figure}{\numberline {24}{\ignorespaces Model accuracy on the original test sets vs.\nobreakspace {}new test sets for CIFAR-10 and ImageNet. Each data point corresponds to one model in a test bed of representative models (shown with 95\% Clopper-Pearson confidence intervals). The plots reveal two main phenomena: (i) There is generally a significant drop in accuracy from the original to the new test sets. (ii) The model accuracies closely follow a linear function with slope greater than 1 (1.7 for CIFAR-10 and 1.1 for ImageNet). This means that every percentage point of progress on the original test set translates into more than one percentage point on the new test set. The two plots are drawn so that their aspect ratio is the same, i.e., the slopes of the lines are visually comparable. The narrow shaded region is a 95\% confidence region for the linear fit from 100,000 bootstrap samples.}}{161}{subsection*.132}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {25}{\ignorespaces Causal diagrams for the heart disease examples.}}{185}{section*.151}%
\contentsline {figure}{\numberline {26}{\ignorespaces Example of a fork.}}{185}{subsection*.152}%
\contentsline {figure}{\numberline {27}{\ignorespaces Example of a mediator.}}{186}{subsection*.153}%
\contentsline {figure}{\numberline {28}{\ignorespaces Example of a collider.}}{186}{subsection*.154}%
\contentsline {figure}{\numberline {29}{\ignorespaces Graph before and after substitution.}}{187}{subsection*.156}%
\contentsline {figure}{\numberline {30}{\ignorespaces Two cases of unobserved confounding.}}{190}{subsection*.160}%
\contentsline {figure}{\numberline {31}{\ignorespaces Causal diagram for our traffic scenario.}}{192}{subsection*.163}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {32}{\ignorespaces Illustration of an idealized regression discontinuity. Real examples are rarely this clear cut.}}{207}{subsection*.177}%
\contentsline {figure}{\numberline {33}{\ignorespaces Typical graphical model for an instrumental variable setup}}{208}{subsection*.178}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsfinish 
